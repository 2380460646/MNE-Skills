# Mne-Core - Examples

**Pages:** 106

---

## Analysing continuous features with binning and regression in sensor space#

**URL:** https://mne.tools/stable/auto_examples/stats/sensor_regression.html

**Contents:**
- Analysing continuous features with binning and regression in sensor space#
- References#

Go to the end to download the full example code.

Predict single trial activity from a continuous variable. A single-trial regression is performed in each sensor and timepoint individually, resulting in an mne.Evoked object which contains the regression coefficient (beta value) for each combination of sensor and timepoint. This example shows the regression coefficient; the t and p values are also calculated automatically.

Here, we repeat a few of the analyses from [1]. This can be easily performed by accessing the metadata object, which contains word-level information about various psycholinguistically relevant features of the words for which we have EEG activity.

For the general methodology, see e.g. [2].

Psycholinguistically relevant word characteristics are continuous. I.e., concreteness or imaginability is a graded property. In the metadata, we have concreteness ratings on a 5-point scale. We can show the dependence of the EEG on concreteness by dividing the data into bins and plotting the mean activity per bin, color coded.

We observe that there appears to be a monotonic dependence of EEG on concreteness. We can also conduct a continuous analysis: single-trial level regression with concreteness as a continuous (although here, binned) feature. We can plot the resulting regression coefficient just like an Event-related Potential.

Because the linear_regression() function also estimates p values, we can – after applying FDR correction for multiple comparisons – also visualise the statistical significance of the regression of word concreteness. The mne.viz.plot_evoked_image() function takes a mask parameter. If we supply it with a boolean mask of the positions where we can reject the null hypothesis, points that are not significant will be shown transparently, and if desired, in a different colour palette and surrounded by dark contour lines.

Stéphane Dufau, Jonathan Grainger, Katherine J. Midgley, and Phillip J. Holcomb. A thousand words are worth a picture: snapshots of printed-word processing in an event-related potential megastudy. Psychological Science, 26(12):1887–1897, 2015. doi:10.1177/0956797615603934.

Olaf Hauk, Matt H. Davis, Michael A. Ford, Friedmann Pulvermüller, and William D. Marslen-Wilson. The time course of visual word recognition as revealed by linear regression analysis of ERP data. NeuroImage, 30(4):1383–1400, 2006. doi:10.1016/j.neuroimage.2005.11.048.

Total running time of the script: (0 minutes 3.918 seconds)

Download Jupyter notebook: sensor_regression.ipynb

Download Python source code: sensor_regression.py

Download zipped: sensor_regression.zip

Gallery generated by Sphinx-Gallery

Permutation T-test on sensor data

Machine Learning (Decoding, Encoding, and MVPA)

---

## Brainstorm raw (median nerve) dataset#

**URL:** https://mne.tools/stable/auto_examples/datasets/brainstorm_data.html

**Contents:**
- Brainstorm raw (median nerve) dataset#
- References#

Go to the end to download the full example code.

Here we compute the evoked from raw for the Brainstorm tutorial dataset. For comparison, see [1] and https://neuroimage.usc.edu/brainstorm/Tutorials/MedianNerveCtf.

François Tadel, Sylvain Baillet, John C. Mosher, Dimitrios Pantazis, and Richard M. Leahy. Brainstorm: a user-friendly application for MEG/EEG analysis. Computational Intelligence and Neuroscience, 2011:1–13, 2011. doi:10.1155/2011/879716.

Total running time of the script: (0 minutes 8.952 seconds)

Download Jupyter notebook: brainstorm_data.ipynb

Download Python source code: brainstorm_data.py

Download zipped: brainstorm_data.zip

Gallery generated by Sphinx-Gallery

Examples on open datasets

---

## Compare simulated and estimated source activity#

**URL:** https://mne.tools/stable/auto_examples/simulation/plot_stc_metrics.html

**Contents:**
- Compare simulated and estimated source activity#
- Define main parameters of sources#
- Create simulated source activity#
- Simulate raw data#
- Compute evoked from raw data#
- Create true stcs corresponding to evoked#
- Reconstruct simulated sources#
- Compute performance scores for different source amplitude thresholds#
  - For region#
  - For Dipoles#

Go to the end to download the full example code.

This example illustrates how to compare the simulated and estimated source time courses (STC) by computing different metrics. Simulated source is a cortical region or dipole. It is meant to be a brief introduction and only highlights the simplest use case.

In this example we simulate two types of cortical sources: a region and a dipole sources. We will test corresponding performance metrics.

First we define both region and dipole sources in terms of Where?, What? and When?.

Here, SourceSimulator is used.

Project the source time series to sensor space with multivariate Gaussian noise obtained from the noise covariance from the sample data.

Averaging epochs corresponding to events.

Before we computed stcs corresponding to raw data. To be able to compare it with the reconstruction, based on the evoked, true stc should have the same number of time samples.

Compute inverse solution using sLORETA.

Total running time of the script: (0 minutes 7.609 seconds)

Download Jupyter notebook: plot_stc_metrics.ipynb

Download Python source code: plot_stc_metrics.py

Download zipped: plot_stc_metrics.zip

Gallery generated by Sphinx-Gallery

Generate simulated evoked data

---

## Compute and visualize ERDS maps#

**URL:** https://mne.tools/stable/auto_examples/time_frequency/time_frequency_erds.html

**Contents:**
- Compute and visualize ERDS maps#
- References#

Go to the end to download the full example code.

This example calculates and displays ERDS maps of event-related EEG data. ERDS (sometimes also written as ERD/ERS) is short for event-related desynchronization (ERD) and event-related synchronization (ERS) [1]. Conceptually, ERD corresponds to a decrease in power in a specific frequency band relative to a baseline. Similarly, ERS corresponds to an increase in power. An ERDS map is a time/frequency representation of ERD/ERS over a range of frequencies [2]. ERDS maps are also known as ERSP (event-related spectral perturbation) [3].

In this example, we use an EEG BCI data set containing two different motor imagery tasks (imagined hand and feet movement). Our goal is to generate ERDS maps for each of the two tasks.

First, we load the data and create epochs of 5s length. The data set contains multiple channels, but we will only consider C3, Cz, and C4. We compute maps containing frequencies ranging from 2 to 35Hz. We map ERD to red color and ERS to blue color, which is customary in many ERDS publications. Finally, we perform cluster-based permutation tests to estimate significant ERDS values (corrected for multiple comparisons within channels).

As usual, we import everything we need.

First, we load and preprocess the data. We use runs 6, 10, and 14 from subject 1 (these runs contains hand and feet motor imagery).

Now we can create 5-second epochs around events of interest.

Here we set suitable values for computing ERDS maps. Note especially the cnorm variable, which sets up an asymmetric colormap where the middle color is mapped to zero, even though zero is not the middle value of the colormap range. This does two things: it ensures that zero values will be plotted in white (given that below we select the RdBu colormap), and it makes synchronization and desynchronization look equally prominent in the plots, even though their extreme values are of different magnitudes.

Finally, we perform time/frequency decomposition over all epochs.

Similar to Epochs objects, we can also export data from EpochsTFR and AverageTFR objects to a Pandas DataFrame. By default, the time column of the exported data frame is in milliseconds. Here, to be consistent with the time-frequency plots, we want to keep it in seconds, which we can achieve by setting time_format=None:

This allows us to use additional plotting functions like seaborn.lineplot() to plot confidence bands:

Having the data as a DataFrame also facilitates subsetting, grouping, and other transforms. Here, we use seaborn to plot the average ERDS in the motor imagery interval as a function of frequency band and imagery condition:

Gert Pfurtscheller and Fernando H. Lopes da Silva. Event-related EEG/MEG synchronization and desynchronization: basic principles. Clinical Neurophysiology, 110(11):1842–1857, 1999. doi:10.1016/S1388-2457(99)00141-8.

Bernhard Graimann, Jane E. Huggins, Simon P. Levine, and Gert Pfurtscheller. Visualization of significant ERD/ERS patterns in multichannel EEG and ECoG data. Clinical Neurophysiology, 113(1):43–47, 2002. doi:10.1016/S1388-2457(01)00697-6.

Scott Makeig. Auditory event-related dynamics of the EEG spectrum and effects of exposure to tones. Electroencephalography and Clinical Neurophysiology, 86(4):283–293, 1993. doi:10.1016/0013-4694(93)90110-H.

Total running time of the script: (0 minutes 28.694 seconds)

Download Jupyter notebook: time_frequency_erds.ipynb

Download Python source code: time_frequency_erds.py

Download zipped: time_frequency_erds.zip

Gallery generated by Sphinx-Gallery

Temporal whitening with AR model

Explore event-related dynamics for specific frequency bands

---

## Compute a cross-spectral density (CSD) matrix#

**URL:** https://mne.tools/stable/auto_examples/time_frequency/compute_csd.html

**Contents:**
- Compute a cross-spectral density (CSD) matrix#

Go to the end to download the full example code.

A cross-spectral density (CSD) matrix is similar to a covariance matrix, but in the time-frequency domain. It is the first step towards computing sensor-to-sensor coherence or a DICS beamformer.

This script demonstrates the three methods that MNE-Python provides to compute the CSD:

Using short-term Fourier transform: mne.time_frequency.csd_fourier()

Using a multitaper approach: mne.time_frequency.csd_multitaper()

Using Morlet wavelets: mne.time_frequency.csd_morlet()

In the following example, the computation of the CSD matrices can be performed using multiple cores. Set n_jobs to a value >1 to select the number of cores to use.

Loading the sample dataset.

By default, CSD matrices are computed using all MEG/EEG channels. When interpreting a CSD matrix with mixed sensor types, be aware that the measurement units, and thus the scalings, differ across sensors. In this example, for speed and clarity, we select a single channel type: gradiometers.

Computing CSD matrices using short-term Fourier transform and (adaptive) multitapers is straightforward:

When computing the CSD with Morlet wavelets, you specify the exact frequencies at which to compute it. For each frequency, a corresponding wavelet will be constructed and convolved with the signal, resulting in a time-frequency decomposition.

The CSD is constructed by computing the correlation between the time-frequency representations between all sensor-to-sensor pairs. The time-frequency decomposition originally has the same sampling rate as the signal, in our case ~600Hz. This means the decomposition is over-specified in time and we may not need to use all samples during our CSD computation, just enough to get a reliable correlation statistic. By specifying decim=10, we use every 10th sample, which will greatly speed up the computation and will have a minimal effect on the CSD.

The resulting mne.time_frequency.CrossSpectralDensity objects have a plotting function we can use to compare the results of the different methods. We’re plotting the mean CSD across frequencies. mne.time_frequency.CrossSpectralDensity.plot() returns a list of created figures; in this case, each returned list has only one figure so we use a Python trick of including a comma after our variable name to assign the figure (not the list) to our fig variable:

Total running time of the script: (0 minutes 19.587 seconds)

Download Jupyter notebook: compute_csd.ipynb

Download Python source code: compute_csd.py

Download zipped: compute_csd.zip

Gallery generated by Sphinx-Gallery

Time-Frequency Examples

Compute Power Spectral Density of inverse solution from single epochs

---

## Compute a sparse inverse solution using the Gamma-MAP empirical Bayesian method#

**URL:** https://mne.tools/stable/auto_examples/inverse/gamma_map_inverse.html

**Contents:**
- Compute a sparse inverse solution using the Gamma-MAP empirical Bayesian method#
- References#

Go to the end to download the full example code.

Plot dipole activations

Show the evoked response and the residual for gradiometers

Generate stc from dipoles

View in 2D and 3D (“glass” brain like 3D plot) Show the sources as spheres scaled by their strength

David Wipf and Srikantan Nagarajan. A unified Bayesian framework for MEG/EEG source imaging. NeuroImage, 44(3):947–966, 2009. doi:10.1016/j.neuroimage.2008.02.059.

Total running time of the script: (0 minutes 29.962 seconds)

Download Jupyter notebook: gamma_map_inverse.ipynb

Download Python source code: gamma_map_inverse.py

Download zipped: gamma_map_inverse.zip

Gallery generated by Sphinx-Gallery

Compute evoked ERS source power using DICS, LCMV beamformer, and dSPM

Extracting time course from source_estimate object

---

## Compute cross-talk functions for LCMV beamformers#

**URL:** https://mne.tools/stable/auto_examples/inverse/psf_ctf_vertices_lcmv.html

**Contents:**
- Compute cross-talk functions for LCMV beamformers#
- Compute LCMV filters with different data covariance matrices#
- Compute resolution matrices for the two LCMV beamformers#
- Visualize#

Go to the end to download the full example code.

Visualise cross-talk functions at one vertex for LCMV beamformers computed with different data covariance matrices, which affects their cross-talk functions.

The pre-stimulus beamformer’s CTF has lower values in parietal regions suppressed alpha activity?) but larger values in occipital regions (less suppression of visual activity?).

Total running time of the script: (0 minutes 12.897 seconds)

Download Jupyter notebook: psf_ctf_vertices_lcmv.ipynb

Download Python source code: psf_ctf_vertices_lcmv.py

Download zipped: psf_ctf_vertices_lcmv.zip

Gallery generated by Sphinx-Gallery

Plot point-spread functions (PSFs) and cross-talk functions (CTFs)

Plot point-spread functions (PSFs) for a volume

---

## Compute effect-matched-spatial filtering (EMS)#

**URL:** https://mne.tools/stable/auto_examples/decoding/ems_filtering.html

**Contents:**
- Compute effect-matched-spatial filtering (EMS)#
- References#

Go to the end to download the full example code.

This example computes the EMS to reconstruct the time course of the experimental effect as described in [1].

This technique is used to create spatial filters based on the difference between two conditions. By projecting the trial onto the corresponding spatial filters, surrogate single trials are created in which multi-sensor activity is reduced to one time series which exposes experimental effects, if present.

We will first plot a trials × times image of the single trials and order the trials by condition. A second plot shows the average time series for each condition. Finally a topographic plot is created which exhibits the temporal evolution of the spatial filters.

Note that a similar transformation can be applied with compute_ems However, this function replicates Schurger et al’s original paper, and thus applies the normalization outside a leave-one-out cross-validation, which we recommend not to do.

Aaron Schurger, Sebastien Marti, and Stanislas Dehaene. Reducing multi-sensor data to a single time course that reveals experimental effects. BMC Neuroscience, 2013. doi:10.1186/1471-2202-14-122.

Total running time of the script: (0 minutes 3.928 seconds)

Download Jupyter notebook: ems_filtering.ipynb

Download Python source code: ems_filtering.py

Download zipped: ems_filtering.zip

Gallery generated by Sphinx-Gallery

XDAWN Decoding From EEG data

Linear classifier on sensor data with plot patterns and filters

---

## Compute induced power in the source space with dSPM#

**URL:** https://mne.tools/stable/auto_examples/time_frequency/source_space_time_frequency.html

**Contents:**
- Compute induced power in the source space with dSPM#

Go to the end to download the full example code.

Returns STC files ie source estimates of induced power for different bands in the source space. The inverse method is linear based on dSPM inverse operator.

Total running time of the script: (0 minutes 3.891 seconds)

Download Jupyter notebook: source_space_time_frequency.ipynb

Download Python source code: source_space_time_frequency.py

Download zipped: source_space_time_frequency.zip

Gallery generated by Sphinx-Gallery

Compute source power spectral density (PSD) of VectorView and OPM data

Temporal whitening with AR model

---

## Compute iterative reweighted TF-MxNE with multiscale time-frequency dictionary#

**URL:** https://mne.tools/stable/auto_examples/inverse/multidict_reweighted_tfmxne.html

**Contents:**
- Compute iterative reweighted TF-MxNE with multiscale time-frequency dictionary#
- References#

Go to the end to download the full example code.

The iterative reweighted TF-MxNE solver is a distributed inverse method based on the TF-MxNE solver, which promotes focal (sparse) sources [1]. The benefits of this approach are that:

it is spatio-temporal without assuming stationarity (source properties can vary over time),

activations are localized in space, time, and frequency in one step,

the solver uses non-convex penalties in the TF domain, which results in a solution less biased towards zero than when simple TF-MxNE is used,

using a multiscale dictionary allows to capture short transient activations along with slower brain waves [2].

Load somatosensory MEG data

Run iterative reweighted multidict TF-MxNE solver

Generate stc from dipoles

Show the evoked response and the residual for gradiometers

Daniel Strohmeier, Alexandre Gramfort, and Jens Haueisen. MEG/EEG Source Imaging with a Non-Convex Penalty in the Time-Frequency Domain. In 2015 International Workshop on Pattern Recognition in NeuroImaging, 21–24. 2015. doi:10.1109/PRNI.2015.14.

Yousra Bekhti, Daniel Strohmeier, Mainak Jas, Roland Badeau, and Alexandre Gramfort. M/EEG source localization with multi-scale time-frequency dictionaries. In Proceedings of PRNI-2016, 1–4. Trento, 2016. IEEE. doi:10.1109/PRNI.2016.7552337.

Total running time of the script: (0 minutes 23.944 seconds)

Download Jupyter notebook: multidict_reweighted_tfmxne.ipynb

Download Python source code: multidict_reweighted_tfmxne.py

Download zipped: multidict_reweighted_tfmxne.zip

Gallery generated by Sphinx-Gallery

Computing source timecourses with an XFit-like multi-dipole model

Visualize source leakage among labels using a circular graph

---

## Compute MxNE with time-frequency sparse prior#

**URL:** https://mne.tools/stable/auto_examples/inverse/time_frequency_mixed_norm_inverse.html

**Contents:**
- Compute MxNE with time-frequency sparse prior#
- References#

Go to the end to download the full example code.

The TF-MxNE solver is a distributed inverse method (like dSPM or sLORETA) that promotes focal (sparse) sources (such as dipole fitting techniques) [1][2]. The benefit of this approach is that:

it is spatio-temporal without assuming stationarity (sources properties can vary over time)

activations are localized in space, time and frequency in one step.

with a built-in filtering process based on a short time Fourier transform (STFT), data does not need to be low passed (just high pass to make the signals zero mean).

the solver solves a convex optimization problem, hence cannot be trapped in local minima.

Plot dipole activations

Plot location of the strongest dipole with MRI slices

Show the evoked response and the residual for gradiometers

Generate stc from dipoles

View in 2D and 3D (“glass” brain like 3D plot)

Alexandre Gramfort, Daniel T. Strohmeier, Jens Haueisen, Matti S. Hämäläinen, and Matthieu Kowalski. Time-frequency mixed-norm estimates: sparse M/EEG imaging with non-stationary source activations. NeuroImage, 70:410–422, 2013. doi:10.1016/j.neuroimage.2012.12.051.

Alexandre Gramfort, Daniel Strohmeier, Jens Haueisen, Matti S. Hämäläinen, and Matthieu Kowalski. Functional brain imaging with M/EEG using structured sparsity in time-frequency dictionaries. In Gábor Székely and Horst K. Hahn, editors, Information Processing in Medical Imaging, volume 6801, pages 600–611. Springer, Berlin; Heidelberg, 2011. doi:10.1007/978-3-642-22092-0_49.

Total running time of the script: (0 minutes 17.538 seconds)

Download Jupyter notebook: time_frequency_mixed_norm_inverse.ipynb

Download Python source code: time_frequency_mixed_norm_inverse.py

Download zipped: time_frequency_mixed_norm_inverse.zip

Gallery generated by Sphinx-Gallery

Computing source space SNR

Compute Trap-Music on evoked data

---

## Compute power and phase lock in label of the source space#

**URL:** https://mne.tools/stable/auto_examples/time_frequency/source_label_time_frequency.html

**Contents:**
- Compute power and phase lock in label of the source space#

Go to the end to download the full example code.

Compute time-frequency maps of power and phase lock in the source space. The inverse method is linear based on dSPM inverse operator.

The example also shows the difference in the time-frequency maps when they are computed with and without subtracting the evoked response from each epoch. The former results in induced activity only while the latter also includes evoked (stimulus-locked) activity.

In the example above, we averaged power across vertices after calculating power because we provided a single label for power calculation and therefore power of all sources within the single label were returned separately. When we provide a list of labels, power is averaged across sources within each label automatically. With a list of labels, averaging is performed before rescaling, so choose a baseline method appropriately.

Total running time of the script: (0 minutes 27.134 seconds)

Download Jupyter notebook: source_label_time_frequency.ipynb

Download Python source code: source_label_time_frequency.py

Download zipped: source_label_time_frequency.zip

Gallery generated by Sphinx-Gallery

Compute Power Spectral Density of inverse solution from single epochs

Compute source power spectral density (PSD) in a label

---

## Compute sLORETA inverse solution on raw data#

**URL:** https://mne.tools/stable/auto_examples/inverse/compute_mne_inverse_raw_in_label.html

**Contents:**
- Compute sLORETA inverse solution on raw data#

Go to the end to download the full example code.

Compute sLORETA inverse solution on raw dataset restricted to a brain label and stores the solution in stc files for visualisation.

View activation time-series

Download Jupyter notebook: compute_mne_inverse_raw_in_label.ipynb

Download Python source code: compute_mne_inverse_raw_in_label.py

Download zipped: compute_mne_inverse_raw_in_label.zip

Gallery generated by Sphinx-Gallery

Compute MNE-dSPM inverse solution on single epochs

Compute MNE-dSPM inverse solution on evoked data in volume source space

---

## Compute source power estimate by projecting the covariance with MNE#

**URL:** https://mne.tools/stable/auto_examples/inverse/mne_cov_power.html

**Contents:**
- Compute source power estimate by projecting the covariance with MNE#
- References#
- Compute empty-room covariance#
- Epoch the data#
- Compute and plot covariances#
- Apply inverse operator to covariance#

Go to the end to download the full example code.

We can apply the MNE inverse operator to a covariance matrix to obtain an estimate of source power. This is computationally more efficient than first estimating the source timecourses and then computing their power. This code is based on the code from [1] and has been useful to correct for individual field spread using source localization in the context of predictive modeling.

David Sabbagh, Pierre Ablin, Gaël Varoquaux, Alexandre Gramfort, and Denis A. Engemann. Predictive regression modeling with meg/eeg: from source power to signals and cognitive states. NeuroImage, 2020. doi:10.1016/j.neuroimage.2020.116893.

First we compute an empty-room covariance, which captures noise from the sensors and environment.

In addition to the empty-room covariance above, we compute two additional covariances:

Baseline covariance, which captures signals not of interest in our analysis (e.g., sensor noise, environmental noise, physiological artifacts, and also resting-state-like brain activity / “noise”).

Data covariance, which captures our activation of interest (in addition to noise sources).

We can also look at the covariances using topomaps, here we just show the baseline and data covariances, followed by the data covariance whitened by the baseline covariance:

Finally, we can construct an inverse using the empty-room noise covariance:

Project our data and baseline covariance to source space:

And visualize power is relative to the baseline:

Total running time of the script: (0 minutes 32.080 seconds)

Download Jupyter notebook: mne_cov_power.ipynb

Download Python source code: mne_cov_power.py

Download zipped: mne_cov_power.zip

Gallery generated by Sphinx-Gallery

Compute MNE inverse solution on evoked data with a mixed source space

Morph surface source estimate

---

## Compute source power spectral density (PSD) in a label#

**URL:** https://mne.tools/stable/auto_examples/time_frequency/source_power_spectrum.html

**Contents:**
- Compute source power spectral density (PSD) in a label#

Go to the end to download the full example code.

Returns an STC file containing the PSD (in dB) of each of the sources within a label.

View PSD of sources in label

Total running time of the script: (0 minutes 4.052 seconds)

Download Jupyter notebook: source_power_spectrum.ipynb

Download Python source code: source_power_spectrum.py

Download zipped: source_power_spectrum.zip

Gallery generated by Sphinx-Gallery

Compute power and phase lock in label of the source space

Compute source power spectral density (PSD) of VectorView and OPM data

---

## Compute source power spectral density (PSD) of VectorView and OPM data#

**URL:** https://mne.tools/stable/auto_examples/time_frequency/source_power_spectrum_opm.html

**Contents:**
- Compute source power spectral density (PSD) of VectorView and OPM data#
- Preprocessing#
- Alignment and forward#
- Alpha#
- Beta#
- References#

Go to the end to download the full example code.

Here we compute the resting state from raw for data recorded using a Neuromag VectorView system and a custom OPM system. The pipeline is meant to mostly follow the Brainstorm [1] OMEGA resting tutorial pipeline. The steps we use are:

Filtering: downsample heavily.

Artifact detection: use SSP for EOG and ECG.

Source localization: dSPM, depth weighting, cortically constrained.

Frequency: power spectral density (Welch), 4 s window, 50% overlap.

Standardize: normalize by relative power for each source.

Load data, resample. We will store the raw objects in dicts with entries “vv” and “opm” to simplify housekeeping and simplify looping later.

Compute and apply inverse to PSD estimated using multitaper + Welch. Group into frequency bands, then normalize each source point and sensor independently. This makes the value of each sensor point and source location in each frequency band the percentage of the PSD accounted for by that band.

Now we can make some plots of each frequency band. Note that the OPM head coverage is only over right motor cortex, so only localization of beta is likely to be worthwhile.

Here we also show OPM data, which shows a profile similar to the VectorView data beneath the sensors. VectorView first:

François Tadel, Sylvain Baillet, John C. Mosher, Dimitrios Pantazis, and Richard M. Leahy. Brainstorm: a user-friendly application for MEG/EEG analysis. Computational Intelligence and Neuroscience, 2011:1–13, 2011. doi:10.1155/2011/879716.

Total running time of the script: (0 minutes 45.322 seconds)

Download Jupyter notebook: source_power_spectrum_opm.ipynb

Download Python source code: source_power_spectrum_opm.py

Download zipped: source_power_spectrum_opm.zip

Gallery generated by Sphinx-Gallery

Compute source power spectral density (PSD) in a label

Compute induced power in the source space with dSPM

---

## Compute source power using DICS beamformer#

**URL:** https://mne.tools/stable/auto_examples/inverse/dics_source_power.html

**Contents:**
- Compute source power using DICS beamformer#
- References#

Go to the end to download the full example code.

Compute a Dynamic Imaging of Coherent Sources (DICS) [1] filter from single-trial activity to estimate source power across a frequency band. This example demonstrates how to source localize the event-related synchronization (ERS) of beta band activity in the somato dataset.

Reading the raw data and creating epochs:

We are interested in the beta band. Define a range of frequencies, using a log scale, from 12 to 30 Hz.

Computing the cross-spectral density matrix for the beta frequency band, for different time intervals. We use a decim value of 20 to speed up the computation in this example at the loss of accuracy.

To compute the source power for a frequency band, rather than each frequency separately, we average the CSD objects across frequencies.

Computing DICS spatial filters using the CSD that was computed on the entire timecourse.

Applying DICS spatial filters separately to the CSD computed using the baseline and the CSD computed during the ERS activity.

Visualizing source power during ERS activity relative to the baseline power.

Joachim Groß, Jan Kujala, Matti S. Hämäläinen, Lars Timmermann, Alfons Schnitzler, and Riitta Salmelin. Dynamic imaging of coherent sources: studying neural interactions in the human brain. Proceedings of the National Academy of Sciences, 98(2):694–699, 2001. doi:10.1073/pnas.98.2.694.

Total running time of the script: (0 minutes 18.187 seconds)

Download Jupyter notebook: dics_source_power.ipynb

Download Python source code: dics_source_power.py

Download zipped: dics_source_power.zip

Gallery generated by Sphinx-Gallery

Compute source level time-frequency timecourses using a DICS beamformer

Compute evoked ERS source power using DICS, LCMV beamformer, and dSPM

---

## Compute sparse inverse solution with mixed norm: MxNE and irMxNE#

**URL:** https://mne.tools/stable/auto_examples/inverse/mixed_norm_inverse.html

**Contents:**
- Compute sparse inverse solution with mixed norm: MxNE and irMxNE#
- References#

Go to the end to download the full example code.

Runs an (ir)MxNE (L1/L2 [1] or L0.5/L2 [2] mixed norm) inverse solver. L0.5/L2 is done with irMxNE which allows for sparser source estimates with less amplitude bias due to the non-convexity of the L0.5/L2 mixed norm penalty.

Run solver with SURE criterion [3]

Plot dipole activations

Generate stc from dipoles

View in 2D and 3D (“glass” brain like 3D plot)

Morph onto fsaverage brain and view

Alexandre Gramfort, Matthieu Kowalski, and Matti S. Hämäläinen. Mixed-norm estimates for the M/EEG inverse problem using accelerated gradient methods. Physics in Medicine and Biology, 57(7):1937–1961, 2012. doi:10.1088/0031-9155/57/7/1937.

Daniel Strohmeier, Jens Haueisen, and Alexandre Gramfort. Improved MEG/EEG source localization with reweighted mixed-norms. In Proceedings of PRNI-2014, 1–4. Tübingen, 2014. IEEE. doi:10.1109/PRNI.2014.6858545.

Charles-Alban Deledalle, Samuel Vaiter, Jalal Fadili, and Gabriel Peyré. Stein unbiased gradient estimator of the risk (sugar) for multiple parameter selection. SIAM Journal on Imaging Sciences, 7(4):2448–2487, 2014. doi:10.1137/140968045.

Total running time of the script: (0 minutes 22.437 seconds)

Download Jupyter notebook: mixed_norm_inverse.ipynb

Download Python source code: mixed_norm_inverse.py

Download zipped: mixed_norm_inverse.zip

Gallery generated by Sphinx-Gallery

Extracting the time series of activations in a label

Compute MNE inverse solution on evoked data with a mixed source space

---

## Compute spatial filters with Spatio-Spectral Decomposition (SSD)#

**URL:** https://mne.tools/stable/auto_examples/decoding/ssd_spatial_filters.html

**Contents:**
- Compute spatial filters with Spatio-Spectral Decomposition (SSD)#
- Epoched data#
- References#

Go to the end to download the full example code.

In this example, we will compute spatial filters for retaining oscillatory brain activity and down-weighting 1/f background signals as proposed by [1]. The idea is to learn spatial filters that separate oscillatory dynamics from surrounding non-oscillatory noise based on the covariance in the frequency band of interest and the noise covariance based on surrounding frequencies.

Let’s investigate spatial filter with the max power ratio. We will first inspect the topographies. According to Nikulin et al. (2011), this is done by either inverting the filters (W^{-1}) or by multiplying the noise cov with the filters Eq. (22) (C_n W)^t. We rely on the inversion approach here.

Let’s also look at the power spectrum of that source and compare it to the power spectrum of the source with lowest SNR.

Although we suggest using this method before epoching, there might be some situations in which data can only be treated by chunks.

Vadim V Nikulin, Guido Nolte, and Gabriel Curio. A novel method for reliable and fast extraction of neuronal EEG/MEG oscillations on the basis of spatio-spectral decomposition. NeuroImage, 55(4):1528–1535, 2011. doi:10.1016/j.neuroimage.2011.01.057.

Total running time of the script: (0 minutes 3.849 seconds)

Download Jupyter notebook: ssd_spatial_filters.ipynb

Download Python source code: ssd_spatial_filters.py

Download zipped: ssd_spatial_filters.zip

Gallery generated by Sphinx-Gallery

Receptive Field Estimation and Prediction

Connectivity Analysis Examples

---

## Compute spatial resolution metrics in source space#

**URL:** https://mne.tools/stable/auto_examples/inverse/resolution_metrics.html

**Contents:**
- Compute spatial resolution metrics in source space#
- MNE#
- dSPM#
- Visualize results#
- References#

Go to the end to download the full example code.

Compute peak localisation error and spatial deviation for the point-spread functions of dSPM and MNE. Plot their distributions and difference of distributions. This example mimics some results from [1], namely Figure 3 (peak localisation error for PSFs, L2-MNE vs dSPM) and Figure 4 (spatial deviation for PSFs, L2-MNE vs dSPM).

Compute resolution matrices, peak localisation error (PLE) for point spread functions (PSFs), spatial deviation (SD) for PSFs:

Do the same for dSPM:

Visualise peak localisation error (PLE) across the whole cortex for MNE PSF:

Subtract the two distributions and plot this difference

These plots show that dSPM has generally lower peak localization error (red color) than MNE in deeper brain areas, but higher error (blue color) in more superficial areas.

Next we’ll visualise spatial deviation (SD) across the whole cortex for MNE PSF:

Subtract the two distributions and plot this difference:

These plots show that dSPM has generally higher spatial deviation than MNE (blue color), i.e. worse performance to distinguish different sources.

Olaf Hauk, Matti Stenroos, and Matthias Treder. Towards an objective evaluation of EEG/MEG source estimation methods: the linear tool kit. bioRxiv, 2019. doi:10.1101/672956.

Total running time of the script: (0 minutes 30.311 seconds)

Download Jupyter notebook: resolution_metrics.ipynb

Download Python source code: resolution_metrics.py

Download zipped: resolution_metrics.zip

Gallery generated by Sphinx-Gallery

Compute spatial resolution metrics to compare MEG with EEG+MEG

---

## Compute spatial resolution metrics to compare MEG with EEG+MEG#

**URL:** https://mne.tools/stable/auto_examples/inverse/resolution_metrics_eegmeg.html

**Contents:**
- Compute spatial resolution metrics to compare MEG with EEG+MEG#
- EEGMEG#
- MEG#
- Visualization#
- References#

Go to the end to download the full example code.

Compute peak localisation error and spatial deviation for the point-spread functions of dSPM and MNE. Plot their distributions and difference of distributions. This example mimics some results from [1], namely Figure 3 (peak localisation error for PSFs, L2-MNE vs dSPM) and Figure 4 (spatial deviation for PSFs, L2-MNE vs dSPM). It shows that combining MEG with EEG reduces the point-spread function and increases the spatial resolution of source imaging, especially for deeper sources.

Compute resolution matrices, localization error, and spatial deviations for MNE:

Look at peak localisation error (PLE) across the whole cortex for PSF:

Subtract the two distributions and plot this difference:

These plots show that with respect to peak localization error, adding EEG to MEG does not bring much benefit. Next let’s visualise spatial deviation (SD) across the whole cortex for PSF:

Subtract the two distributions and plot this difference:

Adding EEG to MEG decreases the spatial extent of point-spread functions (lower spatial deviation, blue colors), thus increasing resolution, especially for deeper source locations.

Olaf Hauk, Matti Stenroos, and Matthias Treder. Towards an objective evaluation of EEG/MEG source estimation methods: the linear tool kit. bioRxiv, 2019. doi:10.1101/672956.

Total running time of the script: (0 minutes 34.019 seconds)

Download Jupyter notebook: resolution_metrics_eegmeg.ipynb

Download Python source code: resolution_metrics_eegmeg.py

Download zipped: resolution_metrics_eegmeg.zip

Gallery generated by Sphinx-Gallery

Compute spatial resolution metrics in source space

Estimate data SNR using an inverse

---

## Computing source space SNR#

**URL:** https://mne.tools/stable/auto_examples/inverse/source_space_snr.html

**Contents:**
- Computing source space SNR#
- EEG#
- References#

Go to the end to download the full example code.

This example shows how to compute and plot source space SNR as in [1].

Next we do the same for EEG and plot the result on the cortex:

The same can be done for MEG, which looks more similar to the MEG-EEG case than the EEG case does.

Daniel M. Goldenholz, Seppo P. Ahlfors, Matti S. Hämäläinen, Dahlia Sharon, Mamiko Ishitobi, Lucia M. Vaina, and Steven M. Stufflebeam. Mapping the signal-to-noise-ratios of cortical sources in magnetoencephalography and electroencephalography. Human Brain Mapping, 30(4):1077–1086, 2009. doi:10.1002/hbm.20571.

Total running time of the script: (0 minutes 14.577 seconds)

Download Jupyter notebook: source_space_snr.ipynb

Download Python source code: source_space_snr.py

Download zipped: source_space_snr.zip

Gallery generated by Sphinx-Gallery

Estimate data SNR using an inverse

Compute MxNE with time-frequency sparse prior

---

## Computing source timecourses with an XFit-like multi-dipole model#

**URL:** https://mne.tools/stable/auto_examples/inverse/multi_dipole_model.html

**Contents:**
- Computing source timecourses with an XFit-like multi-dipole model#

Go to the end to download the full example code.

MEGIN’s XFit program offers a “guided ECD modeling” interface, where multiple dipoles can be fitted interactively. By manually selecting subsets of sensors and time ranges, dipoles can be fitted to specific signal components. Then, source timecourses can be computed using a multi-dipole model. The advantage of using a multi-dipole model over fitting each dipole in isolation, is that when multiple dipoles contribute to the same signal component, the model can make sure that activity assigned to one dipole is not also assigned to another. This example shows how to build a multi-dipole model for estimating source timecourses for evokeds or single epochs.

The XFit program is the recommended approach for guided ECD modeling, because it offers a convenient graphical user interface for it. These dipoles can then be imported into MNE-Python by using the mne.read_dipole() function for building and applying the multi-dipole model. In addition, this example will also demonstrate how to perform guided ECD modeling using only MNE-Python functionality, which is less convenient than using XFit, but has the benefit of being reproducible.

Importing everything and setting up the data paths for the MNE-Sample dataset.

Read the MEG data from the audvis experiment. Make epochs and evokeds for the left and right auditory conditions.

Guided dipole modeling, meaning fitting dipoles to a manually selected subset of sensors as a manually chosen time, can now be performed in MEGINs XFit on the evokeds we computed above. However, it is possible to do it completely in MNE-Python.

Now that we have the location and orientations of the dipoles, compute the full timecourses using MNE, assigning activity to both dipoles at the same time while preventing leakage between the two. We use a very low lambda value to ensure both dipoles are fully used.

We can also fit the timecourses to single epochs. Here, we do it for each experimental condition separately.

To summarize and visualize the single-epoch dipole amplitudes, we will create a detailed plot of the mean amplitude of the dipoles during different experimental conditions.

Total running time of the script: (0 minutes 36.446 seconds)

Download Jupyter notebook: multi_dipole_model.ipynb

Download Python source code: multi_dipole_model.py

Download zipped: multi_dipole_model.zip

Gallery generated by Sphinx-Gallery

Morph volumetric source estimate

Compute iterative reweighted TF-MxNE with multiscale time-frequency dictionary

---

## Connectivity Analysis Examples#

**URL:** https://mne.tools/stable/auto_examples/connectivity/index.html

**Contents:**
- Connectivity Analysis Examples#

Examples demonstrating connectivity analysis in sensor and source space.

Connectivity functionality has moved into the mne_connectivity package. Examples can be found at Examples.

Compute spatial filters with Spatio-Spectral Decomposition (SSD)

---

## Continuous Target Decoding with SPoC#

**URL:** https://mne.tools/stable/auto_examples/decoding/decoding_spoc_CMC.html

**Contents:**
- Continuous Target Decoding with SPoC#
- References#

Go to the end to download the full example code.

Source Power Comodulation (SPoC) [1] allows to identify the composition of orthogonal spatial filters that maximally correlate with a continuous target.

SPoC can be seen as an extension of the CSP for continuous variables.

Here, SPoC is applied to decode the (continuous) fluctuation of an electromyogram from MEG beta activity using data from Cortico-Muscular Coherence example of FieldTrip

Plot the contributions to the detected components (i.e., the forward model)

Sven Dähne, Frank C. Meinecke, Stefan Haufe, Johannes Höhne, Michael Tangermann, Klaus-Robert Müller, and Vadim V. Nikulin. SPoC: a novel framework for relating the amplitude of neuronal oscillations to behaviorally relevant parameters. NeuroImage, 86:111–122, 2014. doi:10.1016/j.neuroimage.2013.07.079.

Total running time of the script: (0 minutes 20.013 seconds)

Download Jupyter notebook: decoding_spoc_CMC.ipynb

Download Python source code: decoding_spoc_CMC.py

Download zipped: decoding_spoc_CMC.zip

Gallery generated by Sphinx-Gallery

Decoding source space data

Decoding sensor space data with generalization across time and conditions

---

## Cross-hemisphere comparison#

**URL:** https://mne.tools/stable/auto_examples/visualization/xhemi.html

**Contents:**
- Cross-hemisphere comparison#

Go to the end to download the full example code.

This example illustrates how to visualize the difference between activity in the left and the right hemisphere. The data from the right hemisphere is mapped to the left hemisphere, and then the difference is plotted. For more information see mne.compute_source_morph().

Total running time of the script: (0 minutes 15.221 seconds)

Download Jupyter notebook: xhemi.ipynb

Download Python source code: xhemi.py

Download zipped: xhemi.zip

Gallery generated by Sphinx-Gallery

Plot custom topographies for MEG sensors

Time-Frequency Examples

---

## Data Simulation#

**URL:** https://mne.tools/stable/auto_examples/simulation/index.html

**Contents:**
- Data Simulation#

Tools to generate simulation data.

Compare simulated and estimated source activity

Generate simulated evoked data

Generate simulated raw data

Simulate raw data using subject anatomy

Generate simulated source data

Compare simulated and estimated source activity

---

## Decoding#

**URL:** https://mne.tools/stable/api/decoding.html

**Contents:**
- Decoding#

Decoding and encoding, including machine learning and receptive fields.

CSP([n_components, reg, log, cov_est, ...])

M/EEG signal decomposition using the Common Spatial Patterns (CSP).

Transformer to compute event-matched spatial filters.

FilterEstimator(info, l_freq, h_freq[, ...])

Estimator to filter RtEpochs.

Compute and store patterns from linear models.

PSDEstimator([sfreq, fmin, fmax, bandwidth, ...])

Compute power spectral density (PSD) using a multi-taper method.

Scaler([info, scalings, with_mean, with_std])

Standardize channel data.

TemporalFilter([l_freq, h_freq, sfreq, ...])

Estimator to filter data array along the last dimension.

TimeFrequency(freqs[, sfreq, method, ...])

Time frequency transformer.

UnsupervisedSpatialFilter(estimator[, average])

Use unsupervised spatial filtering across time and samples.

Transform n-dimensional array into 2D array of n_samples by n_features.

ReceptiveField(tmin, tmax, sfreq[, ...])

Fit a receptive field model.

TimeDelayingRidge(tmin, tmax, sfreq[, ...])

Ridge regression of data with time delays.

SlidingEstimator(base_estimator[, scoring, ...])

GeneralizingEstimator(base_estimator[, ...])

Generalization Light.

SPoC([n_components, reg, log, ...])

Implementation of the SPoC spatial filtering.

SSD(info, filt_params_signal, filt_params_noise)

Signal decomposition using the Spatio-Spectral Decomposition (SSD).

XdawnTransformer([n_components, reg, ...])

Implementation of the Xdawn Algorithm compatible with scikit-learn.

SpatialFilter(info, filters, *[, evals, ...])

Container for spatial filter weights (evecs) and patterns.

Functions that assist with decoding and model fitting:

compute_ems(epochs[, conditions, picks, ...])

Compute event-matched spatial filter on epochs.

cross_val_multiscore(estimator, X[, y, ...])

Evaluate a score by cross-validation.

get_coef(estimator[, attr, ...])

Retrieve the coefficients of an estimator ending with a Linear Model.

get_spatial_filter_from_estimator(estimator, ...)

Instantiate a mne.decoding.SpatialFilter object.

mne.simulation.metrics.peak_position_error

---

## Decoding in time-frequency space using Common Spatial Patterns (CSP)#

**URL:** https://mne.tools/stable/auto_examples/decoding/decoding_csp_timefreq.html

**Contents:**
- Decoding in time-frequency space using Common Spatial Patterns (CSP)#

Go to the end to download the full example code.

The time-frequency decomposition is estimated by iterating over raw data that has been band-passed at different frequencies. This is used to compute a covariance matrix over each epoch or a rolling time-window and extract the CSP filtered signals. A linear discriminant classifier is then applied to these signals.

Set parameters and read data

Loop through frequencies, apply classifier and save scores

Plot frequency results

Loop through frequencies and time, apply classifier and save scores

Plot time-frequency results

Total running time of the script: (0 minutes 14.142 seconds)

Download Jupyter notebook: decoding_csp_timefreq.ipynb

Download Python source code: decoding_csp_timefreq.py

Download zipped: decoding_csp_timefreq.zip

Gallery generated by Sphinx-Gallery

Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)

Representational Similarity Analysis

---

## Decoding sensor space data with generalization across time and conditions#

**URL:** https://mne.tools/stable/auto_examples/decoding/decoding_time_generalization_conditions.html

**Contents:**
- Decoding sensor space data with generalization across time and conditions#
- References#

Go to the end to download the full example code.

This example runs the analysis described in [1]. It illustrates how one can fit a linear classifier to identify a discriminatory topography at a given time instant and subsequently assess whether this linear model can accurately predict all of the time samples of a second set of conditions.

We will train the classifier on all left visual vs auditory trials and test on all right visual vs auditory trials.

Score on the epochs where the stimulus was presented to the right.

Jean-Rémi King and Stanislas Dehaene. Characterizing the dynamics of mental representations: the temporal generalization method. Trends in Cognitive Sciences, 18(4):203–210, 2014. doi:10.1016/j.tics.2014.01.002.

Total running time of the script: (0 minutes 5.419 seconds)

Download Jupyter notebook: decoding_time_generalization_conditions.ipynb

Download Python source code: decoding_time_generalization_conditions.py

Download zipped: decoding_time_generalization_conditions.zip

Gallery generated by Sphinx-Gallery

Continuous Target Decoding with SPoC

Analysis of evoked response using ICA and PCA reduction techniques

---

## Decoding source space data#

**URL:** https://mne.tools/stable/auto_examples/decoding/decoding_spatio_temporal_source.html

**Contents:**
- Decoding source space data#

Go to the end to download the full example code.

Decoding to MEG data in source space on the left cortical surface. Here univariate feature selection is employed for speed purposes to confine the classification to a small number of potentially relevant features. The classifier then is trained to selected features of epochs in source space.

Compute inverse solution

Decoding in sensor space using a logistic regression

To investigate weights, we need to retrieve the patterns of a fitted model

Total running time of the script: (0 minutes 16.916 seconds)

Download Jupyter notebook: decoding_spatio_temporal_source.ipynb

Download Python source code: decoding_spatio_temporal_source.py

Download zipped: decoding_spatio_temporal_source.zip

Gallery generated by Sphinx-Gallery

Representational Similarity Analysis

Continuous Target Decoding with SPoC

---

## Display sensitivity maps for EEG and MEG sensors#

**URL:** https://mne.tools/stable/auto_examples/forward/forward_sensitivity_maps.html

**Contents:**
- Display sensitivity maps for EEG and MEG sensors#

Go to the end to download the full example code.

Sensitivity maps can be produced from forward operators that indicate how well different sensor types will be able to detect neural currents from different regions of the brain.

To get started with forward modeling see Head model and forward computation.

Compute sensitivity maps

Show gain matrix a.k.a. leadfield matrix with sensitivity map

Compare sensitivity map with distribution of source depths

Sensitivity is likely to co-vary with the distance between sources to sensors. To determine the strength of this relationship, we can compute the correlation between source depth and sensitivity values.

Gradiometer sensitiviy is highest close to the sensors, and decreases rapidly with inreasing source depth. This is confirmed by the high negative correlation between the two.

Total running time of the script: (0 minutes 11.311 seconds)

Download Jupyter notebook: forward_sensitivity_maps.ipynb

Download Python source code: forward_sensitivity_maps.py

Download zipped: forward_sensitivity_maps.zip

Gallery generated by Sphinx-Gallery

Generate a left cerebellum volume source space

---

## Estimate data SNR using an inverse#

**URL:** https://mne.tools/stable/auto_examples/inverse/snr_estimate.html

**Contents:**
- Estimate data SNR using an inverse#

Go to the end to download the full example code.

This estimates the SNR as a function of time for a set of data using a minimum-norm inverse operator.

Total running time of the script: (0 minutes 1.700 seconds)

Download Jupyter notebook: snr_estimate.ipynb

Download Python source code: snr_estimate.py

Download zipped: snr_estimate.zip

Gallery generated by Sphinx-Gallery

Compute spatial resolution metrics to compare MEG with EEG+MEG

Computing source space SNR

---

## Examples Gallery#

**URL:** https://mne.tools/stable/auto_examples/index.html

**Contents:**
- Examples Gallery#
- Input/Output#
- Data Simulation#
- Preprocessing#
- Visualization#
- Time-Frequency Examples#
- Statistics Examples#
- Machine Learning (Decoding, Encoding, and MVPA)#
- Connectivity Analysis Examples#
- Forward modeling#

The examples gallery provides working code samples demonstrating various analysis and visualization techniques. These examples often lack the narrative explanations seen in the tutorials, and do not follow any specific order. These examples are a useful way to discover new analysis or plotting ideas, or to see how a particular technique you’ve read about can be applied using MNE-Python.

If example-scripts contain plots and are run locally, using the interactive interactive flag with python -i tutorial_script.py keeps them open.

These examples sometimes use simulations or shortcuts (such as intentionally adding noise to recordings) to illustrate a point. Use caution when copy-pasting code samples.

Recipes for reading and writing files. See also our tutorials on reading data from various recording systems and our tutorial on manipulating MNE-Python data structures.

Getting averaging info from .fif files

Getting impedances from raw files

How to use data in neural ensemble (NEO) format

Reading/Writing a noise covariance matrix

Tools to generate simulation data.

Compare simulated and estimated source activity

Generate simulated evoked data

Generate simulated raw data

Simulate raw data using subject anatomy

Generate simulated source data

Examples related to data preprocessing (artifact detection / rejection etc.)

Locating micro-scale intracranial electrode contacts

Using contralateral referencing for EEG

Cortical Signal Suppression (CSS) for removal of cortical signals

Define target events based on time lag, plot evoked response

Identify EEG Electrodes Bridged by too much Gel

Transform EEG data using current source density (CSD)

Show EOG artifact timing

Reduce EOG artifacts through regression

Automated epochs metadata generation with variable time windows

Principal Component Analysis - Optimal Basis Sets (PCA-OBS) removing cardiac artefact

Find MEG reference channel artifacts

Visualise NIRS artifact correction methods

Compare the different ICA algorithms in MNE

Interpolate bad channels for MEG/EEG channels

Interpolate EEG data to any montage

Maxwell filter data with movement compensation

Annotate movement artifacts and reestimate dev_head_t

Annotate muscle artifacts

Removing muscle ICA components

Plot sensor denoising using oversampled temporal projection

Shifting time-scale in evoked data

Remap MEG channel types

Looking at data and processing output.

How to convert 3D electrode positions to a 2D image

Plotting with mne.viz.Brain

Visualize channel over epochs as an image

Plotting EEG sensors on the scalp

Plotting topographic arrowmaps of evoked data

Plotting topographic maps of evoked data

Whitening evoked data with a noise covariance

Plotting eye-tracking heatmaps in MNE-Python

Plotting sensor layouts of MEG systems

Plot the MNE brain and helmet

Plotting sensor layouts of EEG systems

Plot a cortical parcellation

Plot single trial activity, grouped by ROI and sorted by RT

Sensitivity map of SSP projections

Compare evoked responses for different conditions

Plot custom topographies for MEG sensors

Cross-hemisphere comparison

Some examples of how to explore time-frequency content of M/EEG data with MNE.

Compute a cross-spectral density (CSD) matrix

Compute Power Spectral Density of inverse solution from single epochs

Compute power and phase lock in label of the source space

Compute source power spectral density (PSD) in a label

Compute source power spectral density (PSD) of VectorView and OPM data

Compute induced power in the source space with dSPM

Temporal whitening with AR model

Compute and visualize ERDS maps

Explore event-related dynamics for specific frequency bands

Time-frequency on simulated data (Multitaper vs. Morlet vs. Stockwell vs. Hilbert)

Some examples of how to compute statistics on M/EEG data with MNE.

Permutation F-test on sensor data with 1D cluster level

FDR correction on T-test on sensor data

Regression on continuous data (rER[P/F])

Permutation T-test on sensor data

Analysing continuous features with binning and regression in sensor space

Decoding, encoding, and general machine learning examples.

Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)

Decoding in time-frequency space using Common Spatial Patterns (CSP)

Representational Similarity Analysis

Decoding source space data

Continuous Target Decoding with SPoC

Decoding sensor space data with generalization across time and conditions

Analysis of evoked response using ICA and PCA reduction techniques

XDAWN Decoding From EEG data

Compute effect-matched-spatial filtering (EMS)

Linear classifier on sensor data with plot patterns and filters

Receptive Field Estimation and Prediction

Compute spatial filters with Spatio-Spectral Decomposition (SSD)

Examples demonstrating connectivity analysis in sensor and source space.

Connectivity functionality has moved into the mne_connectivity package. Examples can be found at Examples.

From BEM segmentation, coregistration, setting up source spaces to actual computation of forward solution.

Display sensitivity maps for EEG and MEG sensors

Generate a left cerebellum volume source space

Use source space morphing

Estimate source activations, extract activations in labels, morph data between subjects etc.

Compute MNE-dSPM inverse solution on single epochs

Compute sLORETA inverse solution on raw data

Compute MNE-dSPM inverse solution on evoked data in volume source space

Source localization with a custom inverse solver

Compute source level time-frequency timecourses using a DICS beamformer

Compute source power using DICS beamformer

Compute evoked ERS source power using DICS, LCMV beamformer, and dSPM

Compute a sparse inverse solution using the Gamma-MAP empirical Bayesian method

Extracting time course from source_estimate object

Generate a functional label from source estimates

Extracting the time series of activations in a label

Compute sparse inverse solution with mixed norm: MxNE and irMxNE

Compute MNE inverse solution on evoked data with a mixed source space

Compute source power estimate by projecting the covariance with MNE

Morph surface source estimate

Morph volumetric source estimate

Computing source timecourses with an XFit-like multi-dipole model

Compute iterative reweighted TF-MxNE with multiscale time-frequency dictionary

Visualize source leakage among labels using a circular graph

Plot point-spread functions (PSFs) and cross-talk functions (CTFs)

Compute cross-talk functions for LCMV beamformers

Plot point-spread functions (PSFs) for a volume

Compute Rap-Music on evoked data

Reading an inverse operator

Compute spatial resolution metrics in source space

Compute spatial resolution metrics to compare MEG with EEG+MEG

Estimate data SNR using an inverse

Computing source space SNR

Compute MxNE with time-frequency sparse prior

Compute Trap-Music on evoked data

Plotting the full vector-valued MNE solution

Some demos on common/public datasets using MNE.

Brainstorm raw (median nerve) dataset

Kernel OPM phantom data

Single trial linear regression analysis with the LIMO dataset

Optically pumped magnetometer (OPM) data

From raw data to dSPM on SPM Faces dataset

Download all examples in Python source code: auto_examples_python.zip

Download all examples in Jupyter notebooks: auto_examples_jupyter.zip

Gallery generated by Sphinx-Gallery

Using the event system to link figures

---

## Examples on open datasets#

**URL:** https://mne.tools/stable/auto_examples/datasets/index.html

**Contents:**
- Examples on open datasets#

Some demos on common/public datasets using MNE.

Brainstorm raw (median nerve) dataset

Kernel OPM phantom data

Single trial linear regression analysis with the LIMO dataset

Optically pumped magnetometer (OPM) data

From raw data to dSPM on SPM Faces dataset

Plotting the full vector-valued MNE solution

Brainstorm raw (median nerve) dataset

---

## Explore event-related dynamics for specific frequency bands#

**URL:** https://mne.tools/stable/auto_examples/time_frequency/time_frequency_global_field_power.html

**Contents:**
- Explore event-related dynamics for specific frequency bands#
- References#

Go to the end to download the full example code.

The objective is to show you how to explore spectrally localized effects. For this purpose we adapt the method described in [1] and use it on the somato dataset. The idea is to track the band-limited temporal evolution of spatial patterns by using the global field power (GFP).

We first bandpass filter the signals and then apply a Hilbert transform. To reveal oscillatory activity the evoked response is then subtracted from every single trial. Finally, we rectify the signals prior to averaging across trials by taking the magnitude of the Hilbert. Then the GFP is computed as described in [2], using the sum of the squares but without normalization by the rank. Baselining is subsequently applied to make the GFP comparable between frequencies. The procedure is then repeated for each frequency band of interest and all GFPs are visualized. To estimate uncertainty, non-parametric confidence intervals are computed as described in [3] across channels.

The advantage of this method over summarizing the Space × Time × Frequency output of a Morlet Wavelet in frequency bands is relative speed and, more importantly, the clear-cut comparability of the spectral decomposition (the same type of filter is used across all bands).

We will use this dataset: Somatosensory

Riitta Hari and Riitta Salmelin. Human cortical oscillations: a neuromagnetic view through the skull. Trends in Neurosciences, 20(1):44–49, 1997. doi:10.1016/S0166-2236(96)10065-5.

Denis A. Engemann and Alexandre Gramfort. Automated model selection in covariance estimation and spatial whitening of MEG and EEG signals. NeuroImage, 108:328–342, 2015. doi:10.1016/j.neuroimage.2014.12.040.

Bradley Efron and Trevor Hastie. Computer Age Statistical Inference: Algorithms, Evidence, and Data Science. Number 5 in Institute of Mathematical Statistics Monographs. Cambridge University Press, New York, 2016. ISBN 978-1-107-14989-2. URL: https://hastie.su.domains/CASI/.

We create average power time courses for each frequency band

Now we can compute the Global Field Power We can track the emergence of spatial patterns compared to baseline for each frequency band, with a bootstrapped confidence interval.

We see dominant responses in the Alpha and Beta bands.

Total running time of the script: (0 minutes 21.898 seconds)

Download Jupyter notebook: time_frequency_global_field_power.ipynb

Download Python source code: time_frequency_global_field_power.py

Download zipped: time_frequency_global_field_power.zip

Gallery generated by Sphinx-Gallery

Compute and visualize ERDS maps

Time-frequency on simulated data (Multitaper vs. Morlet vs. Stockwell vs. Hilbert)

---

## Extracting the time series of activations in a label#

**URL:** https://mne.tools/stable/auto_examples/inverse/label_source_activations.html

**Contents:**
- Extracting the time series of activations in a label#
- Compute inverse solution#
- View source activations#
- Using vector solutions#

Go to the end to download the full example code.

We first apply a dSPM inverse operator to get signed activations in a label (with positive and negative values) and we then compare different strategies to average the times series in a label. We compare a simple average, with an averaging using the dipoles normal (flip mode) and then a PCA, also using a sign flip.

It’s also possible to compute label time courses for a mne.VectorSourceEstimate, but only with mode='mean'.

Total running time of the script: (0 minutes 2.430 seconds)

Download Jupyter notebook: label_source_activations.ipynb

Download Python source code: label_source_activations.py

Download zipped: label_source_activations.zip

Gallery generated by Sphinx-Gallery

Generate a functional label from source estimates

Compute sparse inverse solution with mixed norm: MxNE and irMxNE

---

## Extracting time course from source_estimate object#

**URL:** https://mne.tools/stable/auto_examples/inverse/label_activation_from_stc.html

**Contents:**
- Extracting time course from source_estimate object#

Go to the end to download the full example code.

Load a SourceEstimate object from stc files and extract the time course of activation in individual labels, as well as in a complex label formed through merging two labels.

Download Jupyter notebook: label_activation_from_stc.ipynb

Download Python source code: label_activation_from_stc.py

Download zipped: label_activation_from_stc.zip

Gallery generated by Sphinx-Gallery

Compute a sparse inverse solution using the Gamma-MAP empirical Bayesian method

Generate a functional label from source estimates

---

## Forward modeling#

**URL:** https://mne.tools/stable/auto_examples/forward/index.html

**Contents:**
- Forward modeling#

From BEM segmentation, coregistration, setting up source spaces to actual computation of forward solution.

Display sensitivity maps for EEG and MEG sensors

Generate a left cerebellum volume source space

Use source space morphing

Connectivity Analysis Examples

Display sensitivity maps for EEG and MEG sensors

---

## From raw data to dSPM on SPM Faces dataset#

**URL:** https://mne.tools/stable/auto_examples/datasets/spm_faces_dataset.html

**Contents:**
- From raw data to dSPM on SPM Faces dataset#

Go to the end to download the full example code.

Runs a full pipeline using MNE-Python. This example does quite a bit of processing, so even on a fast machine it can take several minutes to complete.

Load data, filter it, and fit ICA.

Epoch data and apply ICA.

Estimate noise covariance and look at the whitened evoked data

Compute forward model

Compute inverse solution and plot

Total running time of the script: (0 minutes 48.725 seconds)

Download Jupyter notebook: spm_faces_dataset.ipynb

Download Python source code: spm_faces_dataset.py

Download zipped: spm_faces_dataset.zip

Gallery generated by Sphinx-Gallery

Optically pumped magnetometer (OPM) data

---

## Generate a functional label from source estimates#

**URL:** https://mne.tools/stable/auto_examples/inverse/label_from_stc.html

**Contents:**
- Generate a functional label from source estimates#

Go to the end to download the full example code.

Threshold source estimates and produce a functional label. The label is typically the region of interest that contains high values. Here we compare the average time course in the anatomical label obtained by FreeSurfer segmentation and the average time course from the functional label. As expected the time course in the functional label yields higher values.

plot the time courses….

plot brain in 3D with mne.viz.Brain if available

Total running time of the script: (0 minutes 4.447 seconds)

Download Jupyter notebook: label_from_stc.ipynb

Download Python source code: label_from_stc.py

Download zipped: label_from_stc.zip

Gallery generated by Sphinx-Gallery

Extracting time course from source_estimate object

Extracting the time series of activations in a label

---

## Generate a left cerebellum volume source space#

**URL:** https://mne.tools/stable/auto_examples/forward/left_cerebellum_volume_source.html

**Contents:**
- Generate a left cerebellum volume source space#

Go to the end to download the full example code.

Generate a volume source space of the left cerebellum and plot its vertices relative to the left cortical surface source space and the FreeSurfer segmentation file.

Setup the source spaces

Plot the positions of each source space

You can export source positions to a NIfTI file:

And display source positions in freeview:

Total running time of the script: (0 minutes 9.309 seconds)

Download Jupyter notebook: left_cerebellum_volume_source.ipynb

Download Python source code: left_cerebellum_volume_source.py

Download zipped: left_cerebellum_volume_source.zip

Gallery generated by Sphinx-Gallery

Display sensitivity maps for EEG and MEG sensors

Use source space morphing

---

## Generate simulated raw data#

**URL:** https://mne.tools/stable/auto_examples/simulation/simulate_raw_data.html

**Contents:**
- Generate simulated raw data#

Go to the end to download the full example code.

This example generates raw data by repeating a desired source activation multiple times.

Generate dipole time series

Total running time of the script: (0 minutes 12.469 seconds)

Download Jupyter notebook: simulate_raw_data.ipynb

Download Python source code: simulate_raw_data.py

Download zipped: simulate_raw_data.zip

Gallery generated by Sphinx-Gallery

Generate simulated evoked data

Simulate raw data using subject anatomy

---

## Generate simulated source data#

**URL:** https://mne.tools/stable/auto_examples/simulation/source_simulator.html

**Contents:**
- Generate simulated source data#

Go to the end to download the full example code.

This example illustrates how to use the mne.simulation.SourceSimulator class to generate source estimates and raw data. It is meant to be a brief introduction and only highlights the simplest use case.

For this example, we will be using the information of the sample subject. This will download the data if it not already on your machine. We also set the subjects directory so we don’t need to give it to functions.

First, we get an info structure from the test subject.

To simulate sources, we also need a source space. It can be obtained from the forward solution of the sample subject.

To select a region to activate, we use the caudal middle frontal to grow a region of interest.

Define the time course of the activity for each source of the region to activate. Here we use a sine wave at 18 Hz with a peak amplitude of 10 nAm.

Define when the activity occurs using events. The first column is the sample of the event, the second is not used, and the third is the event id. Here the events occur every 200 samples.

Create simulated source activity. Here we use a SourceSimulator whose add_data method is key. It specified where (label), what (source_time_series), and when (events) an event type will occur.

Project the source time series to sensor space and add some noise. The source simulator can be given directly to the simulate_raw function.

Plot evoked data to get another view of the simulated raw data.

Total running time of the script: (0 minutes 4.422 seconds)

Download Jupyter notebook: source_simulator.ipynb

Download Python source code: source_simulator.py

Download zipped: source_simulator.zip

Gallery generated by Sphinx-Gallery

Simulate raw data using subject anatomy

---

## Getting impedances from raw files#

**URL:** https://mne.tools/stable/auto_examples/io/read_impedances.html

**Contents:**
- Getting impedances from raw files#
- ANT Neuro#

Go to the end to download the full example code.

Many EEG systems provide impedance measurements for each channel within their file format. MNE does not parse this information and does not store it in the Raw object. However, it is possible to extract this information from the raw data and store it in a separate data structure.

The .cnt file format from ANT Neuro stores impedance information in the form of triggers. The function mne.io.read_raw_ant() reads this information and marks the time-segment during which an impedance measurement was performed as Annotations with the description set in the argument impedance_annotation. However, it doesn’t extract the impedance values themselves. To do so, use the function antio.parser.read_triggers.

Note that the impedance measurement contains all channels, including the bipolar ones. We can visualize the impedances on a topographic map; below we show a topography of impedances before and after the recording for the EEG channels only.

In this very short test file, the impedances are stable over time.

Total running time of the script: (0 minutes 1.938 seconds)

Download Jupyter notebook: read_impedances.ipynb

Download Python source code: read_impedances.py

Download zipped: read_impedances.zip

Gallery generated by Sphinx-Gallery

Getting averaging info from .fif files

How to use data in neural ensemble (NEO) format

---

## HF-SEF dataset#

**URL:** https://mne.tools/stable/auto_examples/datasets/hf_sef_data.html

**Contents:**
- HF-SEF dataset#

Go to the end to download the full example code.

This example looks at high-frequency SEF responses.

Create a highpass filtered version

Compare high-pass filtered and unfiltered data on a single channel

Download Jupyter notebook: hf_sef_data.ipynb

Download Python source code: hf_sef_data.py

Download zipped: hf_sef_data.zip

Gallery generated by Sphinx-Gallery

Brainstorm raw (median nerve) dataset

Kernel OPM phantom data

---

## How to convert 3D electrode positions to a 2D image#

**URL:** https://mne.tools/stable/auto_examples/visualization/3d_to_2d.html

**Contents:**
- How to convert 3D electrode positions to a 2D image#
- Load data#
- Project 3D electrodes to a 2D snapshot#
- Manually creating 2D electrode positions#

Go to the end to download the full example code.

Sometimes we want to convert a 3D representation of electrodes into a 2D image. For example, if we are using electrocorticography it is common to create scatterplots on top of a brain, with each point representing an electrode.

In this example, we’ll show two ways of doing this in MNE-Python. First, if we have the 3D locations of each electrode then we can use PyVista to take a snapshot of a view of the brain. If we do not have these 3D locations, and only have a 2D image of the electrodes on the brain, we can use the mne.viz.ClickableImage class to choose our own electrode positions on the image.

First we will load a sample ECoG dataset which we’ll use for generating a 2D snapshot.

Because we have the 3D location of each electrode, we can use the mne.viz.snapshot_brain_montage() function to return a 2D image along with the electrode positions on that image. We use this in conjunction with mne.viz.plot_alignment(), which visualizes electrode positions.

If we don’t have the 3D electrode positions then we can still create a 2D representation of the electrodes. Assuming that you can see the electrodes on the 2D image, we can use mne.viz.ClickableImage to open the image interactively. You can click points on the image and the x/y coordinate will be stored.

We’ll open an image file, then use ClickableImage to return 2D locations of mouse clicks (or load a file already created). Then, we’ll return these xy positions as a layout for use with plotting topo maps.

Total running time of the script: (0 minutes 6.654 seconds)

Download Jupyter notebook: 3d_to_2d.ipynb

Download Python source code: 3d_to_2d.py

Download zipped: 3d_to_2d.zip

Gallery generated by Sphinx-Gallery

Plotting with mne.viz.Brain

---

## How to use data in neural ensemble (NEO) format#

**URL:** https://mne.tools/stable/auto_examples/io/read_neo_format.html

**Contents:**
- How to use data in neural ensemble (NEO) format#

Go to the end to download the full example code.

This example shows how to create an MNE-Python Raw object from data in the neural ensemble format. For general information on creating MNE-Python’s data objects from NumPy arrays, see Creating MNE-Python data structures from scratch.

This example uses NEO’s ExampleIO object for creating fake data. The data will be all zeros, so the plot won’t be very interesting, but it should demonstrate the steps to using NEO data. For actual data and different file formats, consult the NEO documentation.

Total running time of the script: (0 minutes 1.522 seconds)

Download Jupyter notebook: read_neo_format.ipynb

Download Python source code: read_neo_format.py

Download zipped: read_neo_format.zip

Gallery generated by Sphinx-Gallery

Getting impedances from raw files

Reading/Writing a noise covariance matrix

---

## Input/Output#

**URL:** https://mne.tools/stable/auto_examples/io/index.html

**Contents:**
- Input/Output#

Recipes for reading and writing files. See also our tutorials on reading data from various recording systems and our tutorial on manipulating MNE-Python data structures.

Getting averaging info from .fif files

Getting impedances from raw files

How to use data in neural ensemble (NEO) format

Reading/Writing a noise covariance matrix

Getting averaging info from .fif files

---

## Kernel OPM phantom data#

**URL:** https://mne.tools/stable/auto_examples/datasets/kernel_phantom.html

**Contents:**
- Kernel OPM phantom data#

Go to the end to download the full example code.

In this dataset, a Neuromag phantom was placed inside the Kernel OPM helmet and stimulated with 7 modules active (121 channels). Here we show some example traces.

The data covariance has an interesting structure because of densely packed sensors:

So let’s be careful and compute rank ahead of time and regularize:

Look at our alignment:

Let’s do dipole fits, which are not great because the dev_head_t is approximate and the sensor coverage is sparse:

Total running time of the script: (0 minutes 39.982 seconds)

Download Jupyter notebook: kernel_phantom.ipynb

Download Python source code: kernel_phantom.py

Download zipped: kernel_phantom.zip

Gallery generated by Sphinx-Gallery

Single trial linear regression analysis with the LIMO dataset

---

## Linear classifier on sensor data with plot patterns and filters#

**URL:** https://mne.tools/stable/auto_examples/decoding/linear_model_patterns.html

**Contents:**
- Linear classifier on sensor data with plot patterns and filters#
- Decoding in sensor space using a LogisticRegression classifier#
- References#

Go to the end to download the full example code.

Here decoding, a.k.a MVPA or supervised machine learning, is applied to M/EEG data in sensor space. Fit a linear classifier with the LinearModel object providing topographical patterns which are more neurophysiologically interpretable [1] than the classifier filters (weight vectors). The patterns explain how the MEG and EEG data were generated from the discriminant neural sources which are extracted by the filters. Note patterns/filters in MEG data are more similar than EEG data because the noise is less spatially correlated in MEG than EEG.

Let’s do the same on EEG data using a scikit-learn pipeline

Stefan Haufe, Frank Meinecke, Kai Görgen, Sven Dähne, John-Dylan Haynes, Benjamin Blankertz, and Felix Bießmann. On the interpretation of weight vectors of linear models in multivariate neuroimaging. NeuroImage, 87:96–110, 2014. doi:10.1016/j.neuroimage.2013.10.067.

Total running time of the script: (0 minutes 5.773 seconds)

Download Jupyter notebook: linear_model_patterns.ipynb

Download Python source code: linear_model_patterns.py

Download zipped: linear_model_patterns.zip

Gallery generated by Sphinx-Gallery

Compute effect-matched-spatial filtering (EMS)

Receptive Field Estimation and Prediction

---

## Machine Learning (Decoding, Encoding, and MVPA)#

**URL:** https://mne.tools/stable/auto_examples/decoding/index.html

**Contents:**
- Machine Learning (Decoding, Encoding, and MVPA)#

Decoding, encoding, and general machine learning examples.

Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)

Decoding in time-frequency space using Common Spatial Patterns (CSP)

Representational Similarity Analysis

Decoding source space data

Continuous Target Decoding with SPoC

Decoding sensor space data with generalization across time and conditions

Analysis of evoked response using ICA and PCA reduction techniques

XDAWN Decoding From EEG data

Compute effect-matched-spatial filtering (EMS)

Linear classifier on sensor data with plot patterns and filters

Receptive Field Estimation and Prediction

Compute spatial filters with Spatio-Spectral Decomposition (SSD)

Analysing continuous features with binning and regression in sensor space

Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)

---

## mne.decoding.compute_ems#

**URL:** https://mne.tools/stable/generated/mne.decoding.compute_ems.html

**Contents:**
- mne.decoding.compute_ems#
- Examples using mne.decoding.compute_ems#

Compute event-matched spatial filter on epochs.

This version of EMS [1] operates on the entire time course. No time window needs to be specified. The result is a spatial filter at each time point and a corresponding time course. Intuitively, the result gives the similarity between the filter at each time point and the data vector (sensors) at that time point.

If a list of strings, strings must match the epochs.event_id’s key as well as the number of conditions supported by the objective_function. If None keys in epochs.event_id are used.

Channels to include. Slices and lists of integers will be interpreted as channel indices. In lists, channel type strings (e.g., ['meg', 'eeg']) will pick channels of those types, channel name strings (e.g., ['MEG0111', 'MEG2623'] will pick the given channels. Can also be the string values 'all' to pick all channels, or 'data' to pick data channels. None (default) will pick good data channels. Note that channels in info['bads'] will be included if their names or indices are explicitly provided.

The number of jobs to run in parallel. If -1, it is set to the number of CPU cores. Requires the joblib package. None (default) is a marker for ‘unset’ that will be interpreted as n_jobs=1 (sequential execution) unless the call is performed under a joblib.parallel_config context manager that sets another value for n_jobs.

The cross-validation scheme.

Control verbosity of the logging output. If None, use the default verbosity level. See the logging documentation and mne.verbose() for details. Should only be passed as a keyword argument.

The trial surrogates.

The set of spatial filters.

The conditions used. Values correspond to original event ids.

Aaron Schurger, Sebastien Marti, and Stanislas Dehaene. Reducing multi-sensor data to a single time course that reveals experimental effects. BMC Neuroscience, 2013. doi:10.1186/1471-2202-14-122.

Compute effect-matched-spatial filtering (EMS)

mne.decoding.SpatialFilter

mne.decoding.cross_val_multiscore

---

## mne.decoding.cross_val_multiscore#

**URL:** https://mne.tools/stable/generated/mne.decoding.cross_val_multiscore.html

**Contents:**
- mne.decoding.cross_val_multiscore#
- Examples using mne.decoding.cross_val_multiscore#

Evaluate a score by cross-validation.

The object to use to fit the data. Must implement the ‘fit’ method.

The data to fit. Can be, for example a list, or an array at least 2d.

The target variable to try to predict in the case of supervised learning.

Group labels for the samples used while splitting the dataset into train/test set.

A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator, X, y). Note that when using an estimator which inherently returns multidimensional output - in particular, SlidingEstimator or GeneralizingEstimator - you should set the scorer there, not here.

Determines the cross-validation splitting strategy. Possible inputs for cv are:

None, to use the default 5-fold cross validation,

integer, to specify the number of folds in a (Stratified)KFold,

An object to be used as a cross-validation generator.

An iterable yielding train, test splits.

For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, sklearn.model_selection.StratifiedKFold is used. In all other cases, sklearn.model_selection.KFold is used.

The number of jobs to run in parallel. If -1, it is set to the number of CPU cores. Requires the joblib package. None (default) is a marker for ‘unset’ that will be interpreted as n_jobs=1 (sequential execution) unless the call is performed under a joblib.parallel_config context manager that sets another value for n_jobs.

Control verbosity of the logging output. If None, use the default verbosity level. See the logging documentation and mne.verbose() for details. Should only be passed as a keyword argument.

Parameters to pass to the fit method of the estimator.

Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:

None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs

An int, giving the exact number of total jobs that are spawned

A string, giving an expression as a function of n_jobs, as in ‘2*n_jobs’

Array of scores of the estimator for each run of the cross validation.

Decoding source space data

mne.decoding.compute_ems

mne.decoding.get_coef

---

## mne.decoding.CSP#

**URL:** https://mne.tools/stable/generated/mne.decoding.CSP.html

**Contents:**
- mne.decoding.CSP#
- Examples using mne.decoding.CSP#

M/EEG signal decomposition using the Common Spatial Patterns (CSP).

This class can be used as a supervised decomposition to estimate spatial filters for feature extraction. CSP in the context of EEG was first described in [1]; a comprehensive tutorial on CSP can be found in [2]. Multi-class solving is implemented from [3].

The number of components to decompose M/EEG signals. This number should be set by cross-validation.

If not None (same as 'empirical', default), allow regularization for covariance estimation. If float (between 0 and 1), shrinkage is used. For str values, reg will be passed as method to mne.compute_covariance().

If transform_into equals 'average_power' and log is None or True, then apply a log transform to standardize features, else features are z-scored. If transform_into is 'csp_space', log must be None.

If 'concat', covariance matrices are estimated on concatenated epochs for each class. If 'epoch', covariance matrices are estimated on each epoch separately and then averaged over each class.

If ‘average_power’ then self.transform will return the average power of each spatial filter. If 'csp_space', self.transform will return the data in CSP space.

Normalize class covariance by its trace. Trace normalization is a step of the original CSP algorithm [1] to eliminate magnitude variations in the EEG between individuals. It is not applied in more recent work [2], [3] and can have a negative impact on pattern order.

Parameters to pass to mne.compute_covariance().

Restricting transformation for covariance matrices before performing generalized eigendecomposition. If “restricting” only restriction to the principal subspace of signal_cov will be performed. If “whitening”, covariance matrices will be additionally rescaled according to the whitening for the signal_cov. If None, no restriction will be applied. Defaults to “restricting”.

The mne.Info object with information about the sensors and methods of measurement used for covariance estimation and generalized eigendecomposition. If None, one channel type and no projections will be assumed and if rank is dict, it will be sum of ranks per channel type. Defaults to None.

This controls the rank computation that can be read from the measurement info or estimated from the data. When a noise covariance is used for whitening, this should reflect the rank of that covariance, otherwise amplification of noise components can occur in whitening (e.g., often during source localization).

The rank will be estimated from the data after proper scaling of different channel types.

The rank is inferred from info. If data have been processed with Maxwell filtering, the Maxwell filtering header is used. Otherwise, the channel counts themselves are used. In both cases, the number of projectors is subtracted from the (effective) number of channels in the data. For example, if Maxwell filtering reduces the rank to 68, with two projectors the returned value will be 66.

The rank is assumed to be full, i.e. equal to the number of good channels. If a Covariance is passed, this can make sense if it has been (possibly improperly) regularized without taking into account the true data rank.

Calculate the rank only for a subset of channel types, and explicitly specify the rank for the remaining channel types. This can be extremely useful if you already know the rank of (part of) your data, for instance in case you have calculated it earlier.

This parameter must be a dictionary whose keys correspond to channel types in the data (e.g. 'meg', 'mag', 'grad', 'eeg'), and whose values are integers representing the respective ranks. For example, {'mag': 90, 'eeg': 45} will assume a rank of 90 and 45 for magnetometer data and EEG data, respectively.

The ranks for all channel types present in the data, but not specified in the dictionary will be estimated empirically. That is, if you passed a dataset containing magnetometer, gradiometer, and EEG data together with the dictionary from the previous example, only the gradiometer rank would be determined, while the specified magnetometer and EEG ranks would be taken for granted.

If 'mutual_info' order components by decreasing mutual information (in the two-class case this uses a simplification which orders components by decreasing absolute deviation of the eigenvalues from 0.5 [4]). For the two-class case, 'alternate' orders components by starting with the largest eigenvalue, followed by the smallest, the second-to-largest, the second-to-smallest, and so on [2].

If fit, the CSP components used to decompose the data, else None.

If fit, the CSP patterns used to restore M/EEG signals, else None.

If fit, the mean squared power for each component.

If fit, the std squared power for each component.

Estimate the CSP decomposition on epochs.

fit_transform(X[, y])

Fit CSP to data, then transform it.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

Project CSP features back to sensor space.

plot_filters(info[, components, ch_type, ...])

plot_patterns(info[, components, ch_type, ...])

set_output(*[, transform])

Set output container.

Set the parameters of this estimator.

Estimate epochs sources given the CSP filters.

Zoltan J. Koles, Michael S. Lazar, and Steven Z. Zhou. Spatial patterns underlying population differences in the background EEG. Brain Topography, 2(4):275–284, 1990. doi:10.1007/BF01129656.

Benjamin Blankertz, Ryota Tomioka, Steven Lemm, Motoaki Kawanabe, and Klaus-Robert Müller. Optimizing spatial filters for robust EEG single-trial analysis. IEEE Signal Processing Magazine, 25(1):41–56, 2008. doi:10.1109/MSP.2008.4408441.

Moritz Grosse-Wentrup and Martin Buss. Multiclass common spatial patterns and information theoretic feature extraction. IEEE Transactions on Biomedical Engineering, 55(8):1991–2000, 2008. doi:10.1109/TBME.2008.921154.

Alexandre Barachant, Stephane Bonnet, Marco Congedo, and Christian Jutten. Common spatial pattern revisited by Riemannian geometry. In 2010 IEEE International Workshop on Multimedia Signal Processing, 472–476. 2010. doi:10.1109/MMSP.2010.5662067.

Estimate the CSP decomposition on epochs.

The data on which to estimate the CSP.

The class for each epoch.

Returns the modified instance.

Continuous Target Decoding with SPoC

Fit CSP to data, then transform it.

Fits transformer to X and y with optional parameters fit_params, and returns a transformed version of X.

The data on which to estimate the CSP.

The class for each epoch.

Additional fitting parameters passed to the mne.decoding.CSP.fit() method. Not used for this class.

If self.transform_into == 'average_power' then returns the power of CSP features averaged over time and shape is (n_epochs, n_components). If self.transform_into == 'csp_space' then returns the data in CSP space and shape is (n_epochs, n_components, n_times).

Examples using fit_transform:

Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

Project CSP features back to sensor space.

The data in CSP power space.

The data in sensor space and shape (n_epochs, n_channels, n_components).

LEGACY: New code should use get_spatial_filter_from_estimator(clf, info=info).plot_filters().

Plot topographic filters of components.

The filters are used to extract discriminant neural sources from the measured data (a.k.a. the backward model).

The mne.Info object with information about the sensors and methods of measurement. Used for fitting. If not available, consider using mne.create_info().

The patterns to plot. If None, all components will be shown.

The channel type to plot. For 'grad', the gradiometers are collected in pairs and the RMS for each pair is plotted. If None the first available channel type from order shown above is used. Defaults to None.

The scalings of the channel types to be applied for plotting. If None, defaults to dict(eeg=1e6, grad=1e13, mag=1e15).

Whether to add markers for sensor locations. If str, should be a valid matplotlib format string (e.g., 'r+' for red plusses, see the Notes section of plot()). If True (the default), black circles will be used.

If True, show channel names next to each sensor marker. If callable, channel names will be formatted using the callable; e.g., to delete the prefix ‘MEG ‘ from all channel names, pass the function lambda x: x.replace('MEG ', ''). If mask is not None, only non-masked sensor names will be shown.

Array indicating channel-pattern combinations to highlight with a distinct plotting style. Array elements set to True will be plotted with the parameters given in mask_params. Defaults to None, equivalent to an array of all False elements.

Additional plotting parameters for plotting significant sensors. Default (None) equals:

The number of contour lines to draw. If 0, no contours will be drawn. If a positive integer, that number of contour levels are chosen using the matplotlib tick locator (may sometimes be inaccurate, use array for accuracy). If array-like, the array values are used as the contour levels. The values should be in µV for EEG, fT for magnetometers and fT/m for gradiometers. If colorbar=True, the colorbar will have ticks corresponding to the contour levels. Default is 6.

The outlines to be drawn. If ‘head’, the default head scheme will be drawn. If dict, each key refers to a tuple of x and y positions, the values in ‘mask_pos’ will serve as image mask. Alternatively, a matplotlib patch object can be passed for advanced masking options, either directly or as a function that returns patches (required for multi-axis plots). If None, nothing will be drawn. Defaults to ‘head’.

The sphere parameters to use for the head outline. Can be array-like of shape (4,) to give the X/Y/Z origin and radius in meters, or a single float to give just the radius (origin assumed 0, 0, 0). Can also be an instance of a spherical ConductorModel to use the origin and radius from that object. Can also be a str, in which case:

'auto': the sphere is fit to external digitization points first, and to external + EEG digitization points if the former fails.

'eeglab': the head circle is defined by EEG electrodes 'Fpz', 'Oz', 'T7', and 'T8' (if 'Fpz' is not present, it will be approximated from the coordinates of 'Oz').

'extra': the sphere is fit to external digitization points.

'eeg': the sphere is fit to EEG digitization points.

'cardinal': the sphere is fit to cardinal digitization points.

'hpi': the sphere is fit to HPI coil digitization points.

Can also be a list of str, in which case the sphere is fit to the specified digitization points, which can be any combination of 'extra', 'eeg', 'cardinal', and 'hpi', as specified above. None (the default) is equivalent to 'auto' when enough extra digitization points are available, and (0, 0, 0, 0.095) otherwise.

Changed in version 1.1: Added 'eeglab' option.

Changed in version 1.11: Added 'extra', 'eeg', 'cardinal', 'hpi' and list of str options.

The image interpolation to be used. Options are 'cubic' (default) to use scipy.interpolate.CloughTocher2DInterpolator, 'nearest' to use scipy.spatial.Voronoi or 'linear' to use scipy.interpolate.LinearNDInterpolator.

Extrapolate to four points placed to form a square encompassing all data points, where each side of the square is three times the range of the data in the respective dimension.

Extrapolate only to nearby points (approximately to points closer than median inter-electrode distance). This will also set the mask to be polygonal based on the convex hull of the sensors.

Extrapolate out to the edges of the clipping circle. This will be on the head circle when the sensors are contained within the head circle, but it can extend beyond the head when sensors are plotted outside the head circle.

Value to extrapolate to on the topomap borders. If 'mean' (default), then each extrapolated point has the average value of its neighbours.

The resolution of the topomap image (number of pixels along each side).

Side length of each subplot in inches.

Colormap to use. If tuple, the first value indicates the colormap to use and the second value is a boolean defining interactivity. In interactive mode the colors are adjustable by clicking and dragging the colorbar with left and right mouse button. Left mouse button moves the scale up and down and right mouse button adjusts the range. Hitting space bar resets the range. Up and down arrows can be used to change the colormap. If None, 'Reds' is used for data that is either all-positive or all-negative, and 'RdBu_r' is used otherwise. 'interactive' is equivalent to (None, True). Defaults to None.

Interactive mode works smoothly only for a small amount of topomaps. Interactive mode is disabled by default for more than 2 topomaps.

Lower and upper bounds of the colormap, typically a numeric value in the same units as the data. Elements of the tuple may also be callable functions which take in a NumPy array and return a scalar.

If both entries are None, the bounds are set at ± the maximum absolute value of the data (yielding a colormap with midpoint at 0), or (0, max(abs(data))) if the (possibly baselined) data are all-positive. Providing None for just one entry will set the corresponding boundary at the min/max of the data. If vlim="joint", will compute the colormap limits jointly across all topomaps of the same channel type (instead of separately for each topomap), using the min/max of the data for that channel type. Defaults to (None, None).

How to normalize the colormap. If None, standard linear normalization is performed. If not None, vmin and vmax will be ignored. See Matplotlib docs for more details on colormap normalization, and the ERDs example for an example of its use.

Plot a colorbar in the rightmost column of the figure.

Formatting string for colorbar tick labels. See Format Specification Mini-Language for details.

The units to use for the colorbar label. Ignored if colorbar=False. If None the label will be “AU” indicating arbitrary units. Default is None.

The axes to plot into. If None, a new Figure will be created with the correct number of axes. If Axes are provided (either as a single instance or a list of axes), the number of axes provided must match the number of times provided (unless times is None). Default is None.

String format for topomap values. Defaults to “CSP%01d”.

The number of rows and columns of topographies to plot. If either nrows or ncols is 'auto', the necessary number will be inferred. Defaults to nrows=1, ncols='auto'.

Show the figure if True.

LEGACY: New code should use get_spatial_filter_from_estimator(clf, info=info).plot_patterns().

Plot topographic patterns of components.

The patterns explain how the measured data was generated from the neural sources (a.k.a. the forward model).

The mne.Info object with information about the sensors and methods of measurement. Used for fitting. If not available, consider using mne.create_info().

The patterns to plot. If None, all components will be shown.

The channel type to plot. For 'grad', the gradiometers are collected in pairs and the RMS for each pair is plotted. If None the first available channel type from order shown above is used. Defaults to None.

The scalings of the channel types to be applied for plotting. If None, defaults to dict(eeg=1e6, grad=1e13, mag=1e15).

Whether to add markers for sensor locations. If str, should be a valid matplotlib format string (e.g., 'r+' for red plusses, see the Notes section of plot()). If True (the default), black circles will be used.

If True, show channel names next to each sensor marker. If callable, channel names will be formatted using the callable; e.g., to delete the prefix ‘MEG ‘ from all channel names, pass the function lambda x: x.replace('MEG ', ''). If mask is not None, only non-masked sensor names will be shown.

Array indicating channel-pattern combinations to highlight with a distinct plotting style. Array elements set to True will be plotted with the parameters given in mask_params. Defaults to None, equivalent to an array of all False elements.

Additional plotting parameters for plotting significant sensors. Default (None) equals:

The number of contour lines to draw. If 0, no contours will be drawn. If a positive integer, that number of contour levels are chosen using the matplotlib tick locator (may sometimes be inaccurate, use array for accuracy). If array-like, the array values are used as the contour levels. The values should be in µV for EEG, fT for magnetometers and fT/m for gradiometers. If colorbar=True, the colorbar will have ticks corresponding to the contour levels. Default is 6.

The outlines to be drawn. If ‘head’, the default head scheme will be drawn. If dict, each key refers to a tuple of x and y positions, the values in ‘mask_pos’ will serve as image mask. Alternatively, a matplotlib patch object can be passed for advanced masking options, either directly or as a function that returns patches (required for multi-axis plots). If None, nothing will be drawn. Defaults to ‘head’.

The sphere parameters to use for the head outline. Can be array-like of shape (4,) to give the X/Y/Z origin and radius in meters, or a single float to give just the radius (origin assumed 0, 0, 0). Can also be an instance of a spherical ConductorModel to use the origin and radius from that object. Can also be a str, in which case:

'auto': the sphere is fit to external digitization points first, and to external + EEG digitization points if the former fails.

'eeglab': the head circle is defined by EEG electrodes 'Fpz', 'Oz', 'T7', and 'T8' (if 'Fpz' is not present, it will be approximated from the coordinates of 'Oz').

'extra': the sphere is fit to external digitization points.

'eeg': the sphere is fit to EEG digitization points.

'cardinal': the sphere is fit to cardinal digitization points.

'hpi': the sphere is fit to HPI coil digitization points.

Can also be a list of str, in which case the sphere is fit to the specified digitization points, which can be any combination of 'extra', 'eeg', 'cardinal', and 'hpi', as specified above. None (the default) is equivalent to 'auto' when enough extra digitization points are available, and (0, 0, 0, 0.095) otherwise.

Changed in version 1.1: Added 'eeglab' option.

Changed in version 1.11: Added 'extra', 'eeg', 'cardinal', 'hpi' and list of str options.

The image interpolation to be used. Options are 'cubic' (default) to use scipy.interpolate.CloughTocher2DInterpolator, 'nearest' to use scipy.spatial.Voronoi or 'linear' to use scipy.interpolate.LinearNDInterpolator.

Extrapolate to four points placed to form a square encompassing all data points, where each side of the square is three times the range of the data in the respective dimension.

Extrapolate only to nearby points (approximately to points closer than median inter-electrode distance). This will also set the mask to be polygonal based on the convex hull of the sensors.

Extrapolate out to the edges of the clipping circle. This will be on the head circle when the sensors are contained within the head circle, but it can extend beyond the head when sensors are plotted outside the head circle.

Value to extrapolate to on the topomap borders. If 'mean' (default), then each extrapolated point has the average value of its neighbours.

The resolution of the topomap image (number of pixels along each side).

Side length of each subplot in inches.

Colormap to use. If tuple, the first value indicates the colormap to use and the second value is a boolean defining interactivity. In interactive mode the colors are adjustable by clicking and dragging the colorbar with left and right mouse button. Left mouse button moves the scale up and down and right mouse button adjusts the range. Hitting space bar resets the range. Up and down arrows can be used to change the colormap. If None, 'Reds' is used for data that is either all-positive or all-negative, and 'RdBu_r' is used otherwise. 'interactive' is equivalent to (None, True). Defaults to None.

Interactive mode works smoothly only for a small amount of topomaps. Interactive mode is disabled by default for more than 2 topomaps.

Lower and upper bounds of the colormap, typically a numeric value in the same units as the data. If both entries are None, the bounds are set at (min(data), max(data)). Providing None for just one entry will set the corresponding boundary at the min/max of the data. Defaults to (None, None).

How to normalize the colormap. If None, standard linear normalization is performed. If not None, vmin and vmax will be ignored. See Matplotlib docs for more details on colormap normalization, and the ERDs example for an example of its use.

Plot a colorbar in the rightmost column of the figure.

Formatting string for colorbar tick labels. See Format Specification Mini-Language for details.

The units to use for the colorbar label. Ignored if colorbar=False. If None the label will be “AU” indicating arbitrary units. Default is None.

The axes to plot into. If None, a new Figure will be created with the correct number of axes. If Axes are provided (either as a single instance or a list of axes), the number of axes provided must match the number of times provided (unless times is None). Default is None.

String format for topomap values. Defaults to “CSP%01d”.

The number of rows and columns of topographies to plot. If either nrows or ncols is 'auto', the necessary number will be inferred. Defaults to nrows=1, ncols='auto'.

Show the figure if True.

Set output container.

See Introducing the set_output API for an example on how to use the API.

Configure output of transform and fit_transform.

“default”: Default output format of a transformer

“pandas”: DataFrame output

“polars”: Polars output

None: Transform configuration is unchanged

New in v1.4: “polars” option was added.

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Estimate epochs sources given the CSP filters.

If self.transform_into == ‘average_power’ then returns the power of CSP features averaged over time and shape (n_epochs, n_components) If self.transform_into == ‘csp_space’ then returns the data in CSP space and shape is (n_epochs, n_components, n_times).

Examples using transform:

Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)

Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)

Decoding in time-frequency space using Common Spatial Patterns (CSP)

Continuous Target Decoding with SPoC

---

## mne.decoding.EMS#

**URL:** https://mne.tools/stable/generated/mne.decoding.EMS.html

**Contents:**
- mne.decoding.EMS#
- Examples using mne.decoding.EMS#

Transformer to compute event-matched spatial filters.

This version of EMS [1] operates on the entire time course. No time window needs to be specified. The result is a spatial filter at each time point and a corresponding time course. Intuitively, the result gives the similarity between the filter at each time point and the data vector (sensors) at that time point.

EMS only works for binary classification.

The set of spatial filters.

Fit the spatial filters.

fit_transform(X[, y])

Fit to data, then transform it.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

set_output(*[, transform])

Set output container.

Set the parameters of this estimator.

Transform the data by the spatial filters.

Aaron Schurger, Sebastien Marti, and Stanislas Dehaene. Reducing multi-sensor data to a single time course that reveals experimental effects. BMC Neuroscience, 2013. doi:10.1186/1471-2202-14-122.

Fit the spatial filters.

Compute effect-matched-spatial filtering (EMS)

Fit to data, then transform it.

Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.

Target values (None for unsupervised transformations).

Additional fit parameters. Pass only if the estimator accepts additional params in its fit method.

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

Set output container.

See Introducing the set_output API for an example on how to use the API.

Configure output of transform and fit_transform.

“default”: Default output format of a transformer

“pandas”: DataFrame output

“polars”: Polars output

None: Transform configuration is unchanged

New in v1.4: “polars” option was added.

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Transform the data by the spatial filters.

The input data transformed by the spatial filters.

Examples using transform:

Compute effect-matched-spatial filtering (EMS)

Compute effect-matched-spatial filtering (EMS)

mne.decoding.FilterEstimator

---

## mne.decoding.FilterEstimator#

**URL:** https://mne.tools/stable/generated/mne.decoding.FilterEstimator.html

**Contents:**
- mne.decoding.FilterEstimator#

Estimator to filter RtEpochs.

Applies a zero-phase low-pass, high-pass, band-pass, or band-stop filter to the channels selected by “picks”.

l_freq and h_freq are the frequencies below which and above which, respectively, to filter out of the data. Thus the uses are:

l_freq < h_freq: band-pass filter

l_freq > h_freq: band-stop filter

l_freq is not None, h_freq is None: low-pass filter

l_freq is None, h_freq is not None: high-pass filter

If n_jobs > 1, more memory is required as “len(picks) * n_times” additional time points need to be temporarily stored in memory.

The mne.Info object with information about the sensors and methods of measurement.

For FIR filters, the lower pass-band edge; for IIR filters, the lower cutoff frequency. If None the data are only low-passed.

For FIR filters, the upper pass-band edge; for IIR filters, the upper cutoff frequency. If None the data are only high-passed.

Channels to include. Slices and lists of integers will be interpreted as channel indices. In lists, channel type strings (e.g., ['meg', 'eeg']) will pick channels of those types, channel name strings (e.g., ['MEG0111', 'MEG2623'] will pick the given channels. Can also be the string values 'all' to pick all channels, or 'data' to pick data channels. None (default) will pick good data channels. Note that channels in info['bads'] will be included if their names or indices are explicitly provided.

Length of the FIR filter to use (if applicable):

‘auto’ (default): The filter length is chosen based on the size of the transition regions (6.6 times the reciprocal of the shortest transition band for fir_window=’hamming’ and fir_design=”firwin2”, and half that for “firwin”).

str: A human-readable time in units of “s” or “ms” (e.g., “10s” or “5500ms”) will be converted to that number of samples if phase="zero", or the shortest power-of-two length at least that duration for phase="zero-double".

int: Specified length in samples. For fir_design=”firwin”, this should not be used.

Width of the transition band at the low cut-off frequency in Hz (high pass or cutoff 1 in bandpass). Can be “auto” (default) to use a multiple of l_freq:

Only used for method='fir'.

Width of the transition band at the high cut-off frequency in Hz (low pass or cutoff 2 in bandpass). Can be “auto” (default in 0.14) to use a multiple of h_freq:

Only used for method='fir'.

Number of jobs to run in parallel. Can be ‘cuda’ if cupy is installed properly and method=’fir’.

‘fir’ will use overlap-add FIR filtering, ‘iir’ will use IIR filtering.

Dictionary of parameters to use for IIR filtering. See mne.filter.construct_iir_filter for details. If iir_params is None and method=”iir”, 4th order Butterworth will be used.

Can be “firwin” (default) to use scipy.signal.firwin(), or “firwin2” to use scipy.signal.firwin2(). “firwin” uses a time-domain design technique that generally gives improved attenuation using fewer samples than “firwin2”.

fit_transform(X[, y])

Fit to data, then transform it.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

set_fit_request(*[, epochs_data])

Configure whether metadata should be requested to be passed to the fit method.

set_output(*[, transform])

Set output container.

Set the parameters of this estimator.

set_transform_request(*[, epochs_data])

Configure whether metadata should be requested to be passed to the transform method.

transform(epochs_data)

This is primarily meant for use in realtime applications. In general it is not recommended in a normal processing pipeline as it may result in edge artifacts. Use with caution.

The label for each epoch.

The modified instance.

Fit to data, then transform it.

Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.

Target values (None for unsupervised transformations).

Additional fit parameters. Pass only if the estimator accepts additional params in its fit method.

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

Configure whether metadata should be requested to be passed to the fit method.

Note that this method is only relevant when this estimator is used as a sub-estimator within a meta-estimator and metadata routing is enabled with enable_metadata_routing=True (see sklearn.set_config()). Please check the User Guide on how the routing mechanism works.

The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.

False: metadata is not requested and the meta-estimator will not pass it to fit.

None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.

str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.

Metadata routing for epochs_data parameter in fit.

Set output container.

See Introducing the set_output API for an example on how to use the API.

Configure output of transform and fit_transform.

“default”: Default output format of a transformer

“pandas”: DataFrame output

“polars”: Polars output

None: Transform configuration is unchanged

New in v1.4: “polars” option was added.

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Configure whether metadata should be requested to be passed to the transform method.

Note that this method is only relevant when this estimator is used as a sub-estimator within a meta-estimator and metadata routing is enabled with enable_metadata_routing=True (see sklearn.set_config()). Please check the User Guide on how the routing mechanism works.

The options for each parameter are:

True: metadata is requested, and passed to transform if provided. The request is ignored if metadata is not provided.

False: metadata is not requested and the meta-estimator will not pass it to transform.

None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.

str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.

Metadata routing for epochs_data parameter in transform.

The data after filtering.

mne.decoding.LinearModel

---

## mne.decoding.GeneralizingEstimator#

**URL:** https://mne.tools/stable/generated/mne.decoding.GeneralizingEstimator.html

**Contents:**
- mne.decoding.GeneralizingEstimator#
- Examples using mne.decoding.GeneralizingEstimator#

Generalization Light.

Fit a search-light along the last dimension and use them to apply a systematic cross-tasks generalization.

The base estimator to iteratively fit on a subset of the dataset.

Score function (or loss function) with signature score_func(y, y_pred, **kwargs). Note that the “predict” method is automatically identified if scoring is a string (e.g. scoring='roc_auc' calls predict_proba), but is not automatically set if scoring is a callable (e.g. scoring=sklearn.metrics.roc_auc_score).

The number of jobs to run in parallel. If -1, it is set to the number of CPU cores. Requires the joblib package. None (default) is a marker for ‘unset’ that will be interpreted as n_jobs=1 (sequential execution) unless the call is performed under a joblib.parallel_config context manager that sets another value for n_jobs.

The position for the progress bar.

If True, allow 2D data as input (i.e. n_samples, n_features).

Control verbosity of the logging output. If None, use the default verbosity level. See the logging documentation and mne.verbose() for details. Should only be passed as a keyword argument.

Estimate distances of each data slice to all hyperplanes.

fit(X, y, **fit_params)

Fit a series of independent estimators to the dataset.

fit_transform(X, y, **fit_params)

Fit and transform a series of independent estimators to the dataset.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

Predict each data slice with all possible estimators.

Estimate probabilistic estimates of each data slice with all possible estimators.

Score each of the estimators on the tested dimensions.

set_output(*[, transform])

Set output container.

Set the parameters of this estimator.

Transform each data slice with all possible estimators.

Estimate distances of each data slice to all hyperplanes.

The training input samples. Each estimator outputs the distance to its hyperplane, e.g.: [estimators[ii].decision_function(X[..., ii]) for ii in range(n_estimators)]. The feature dimension can be multidimensional e.g. X.shape = (n_samples, n_features_1, n_features_2, n_estimators).

The predicted values for each estimator.

This requires base_estimator to have a decision_function method.

Fit a series of independent estimators to the dataset.

The training input samples. For each data slice, a clone estimator is fitted independently. The feature dimension can be multidimensional e.g. X.shape = (n_samples, n_features_1, n_features_2, n_tasks).

Parameters to pass to the fit method of the estimator.

Decoding sensor space data with generalization across time and conditions

Fit and transform a series of independent estimators to the dataset.

The training input samples. For each task, a clone estimator is fitted independently. The feature dimension can be multidimensional, e.g.:

Parameters to pass to the fit method of the estimator.

The predicted values for each estimator.

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

Predict each data slice with all possible estimators.

The training input samples. For each data slice, a fitted estimator predicts each slice of the data independently. The feature dimension can be multidimensional e.g. X.shape = (n_samples, n_features_1, n_features_2, n_estimators).

The predicted values for each estimator.

Estimate probabilistic estimates of each data slice with all possible estimators.

The training input samples. For each data slice, a fitted estimator predicts a slice of the data. The feature dimension can be multidimensional e.g. X.shape = (n_samples, n_features_1, n_features_2, n_estimators).

The predicted values for each estimator.

This requires base_estimator to have a predict_proba method.

Score each of the estimators on the tested dimensions.

The input samples. For each data slice, the corresponding estimator scores the prediction, e.g.: [estimators[ii].score(X[..., ii], y) for ii in range(n_slices)]. The feature dimension can be multidimensional e.g. X.shape = (n_samples, n_features_1, n_features_2, n_estimators).

Score for each estimator / data slice couple.

Examples using score:

Decoding sensor space data with generalization across time and conditions

Set output container.

See Introducing the set_output API for an example on how to use the API.

Configure output of transform and fit_transform.

“default”: Default output format of a transformer

“pandas”: DataFrame output

“polars”: Polars output

None: Transform configuration is unchanged

New in v1.4: “polars” option was added.

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Transform each data slice with all possible estimators.

The input samples. For estimator the corresponding data slice is used to make a transformation. The feature dimension can be multidimensional e.g. X.shape = (n_samples, n_features_1, n_features_2, n_estimators).

The transformed values generated by each estimator.

Decoding sensor space data with generalization across time and conditions

mne.decoding.SlidingEstimator

---

## mne.decoding.get_coef#

**URL:** https://mne.tools/stable/generated/mne.decoding.get_coef.html

**Contents:**
- mne.decoding.get_coef#
- Examples using mne.decoding.get_coef#

Retrieve the coefficients of an estimator ending with a Linear Model.

This is typically useful to retrieve “spatial filters” or “spatial patterns” of decoding models [1].

An estimator from scikit-learn.

The name of the coefficient attribute to retrieve, typically 'filters_' (default) or 'patterns_'.

If True, returns the coefficients after inverse transforming them with the transformer steps of the estimator.

Name of the sklearn’s pipeline step to get the coef from. If inverse_transform is True, the inverse transformations will be applied using transformers before this step. If None, the last step will be used. Defaults to None.

Control verbosity of the logging output. If None, use the default verbosity level. See the logging documentation and mne.verbose() for details. Should only be passed as a keyword argument.

Stefan Haufe, Frank Meinecke, Kai Görgen, Sven Dähne, John-Dylan Haynes, Benjamin Blankertz, and Felix Bießmann. On the interpretation of weight vectors of linear models in multivariate neuroimaging. NeuroImage, 87:96–110, 2014. doi:10.1016/j.neuroimage.2013.10.067.

Decoding source space data

mne.decoding.cross_val_multiscore

mne.decoding.get_spatial_filter_from_estimator

---

## mne.decoding.get_spatial_filter_from_estimator#

**URL:** https://mne.tools/stable/generated/mne.decoding.get_spatial_filter_from_estimator.html

**Contents:**
- mne.decoding.get_spatial_filter_from_estimator#
- Examples using mne.decoding.get_spatial_filter_from_estimator#

Instantiate a mne.decoding.SpatialFilter object.

Creates object from the fitted generalized eigendecomposition transformers or mne.decoding.LinearModel. This object can be used to visualize spatial filters, patterns, and eigenvalues.

Sklearn-based estimator or meta-estimator from which to initialize spatial filter. Use step_name to select relevant transformer from the pipeline object (works with nested names using __ syntax).

The measurement info object for plotting topomaps.

If True, returns filters and patterns after inverse transforming them with the transformer steps of the estimator. Defaults to False.

Name of the sklearn’s pipeline step to get the coefs from. If inverse_transform is True, the inverse transformations will be applied using transformers before this step. If None, the last step will be used. Defaults to None.

The names of the coefficient attributes to retrieve, can include 'filters_', 'patterns_' and 'evals_'. If step is GEDTransformer, will use all. if step is LinearModel will only use 'filters_' and 'patterns_'. Defaults to ('filters_', 'patterns_', 'evals_').

The method used to compute the patterns. Can be None, 'pinv' or 'haufe'. It will be set automatically to 'pinv' if step is GEDTransformer, or to 'haufe' if step is LinearModel. Defaults to None.

Control verbosity of the logging output. If None, use the default verbosity level. See the logging documentation and mne.verbose() for details. Should only be passed as a keyword argument.

The spatial filter object.

Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)

Continuous Target Decoding with SPoC

XDAWN Decoding From EEG data

Linear classifier on sensor data with plot patterns and filters

Compute spatial filters with Spatio-Spectral Decomposition (SSD)

mne.decoding.get_coef

---

## mne.decoding.LinearModel#

**URL:** https://mne.tools/stable/generated/mne.decoding.LinearModel.html

**Contents:**
- mne.decoding.LinearModel#
- Examples using mne.decoding.LinearModel#

Compute and store patterns from linear models.

The linear model coefficients (filters) are used to extract discriminant neural sources from the measured data. This class computes the corresponding patterns of these linear filters to make them more interpretable [1].

A linear model from scikit-learn with a fit method that updates a coef_ attribute. If None the model will be sklearn.linear_model.LogisticRegression.

If fit, the filters used to decompose the data.

If fit, the patterns used to restore M/EEG signals.

fit(X, y, **fit_params)

Estimate the coefficients of the linear model.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

Set the parameters of this estimator.

Stefan Haufe, Frank Meinecke, Kai Görgen, Sven Dähne, John-Dylan Haynes, Benjamin Blankertz, and Felix Bießmann. On the interpretation of weight vectors of linear models in multivariate neuroimaging. NeuroImage, 87:96–110, 2014. doi:10.1016/j.neuroimage.2013.10.067.

Estimate the coefficients of the linear model.

Save the coefficients in the attribute filters_ and computes the attribute patterns_.

The training input samples to estimate the linear coefficients.

Parameters to pass to the fit method of the estimator.

Returns the modified instance.

Linear classifier on sensor data with plot patterns and filters

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Decoding source space data

Linear classifier on sensor data with plot patterns and filters

mne.decoding.FilterEstimator

mne.decoding.PSDEstimator

---

## mne.decoding.PSDEstimator#

**URL:** https://mne.tools/stable/generated/mne.decoding.PSDEstimator.html

**Contents:**
- mne.decoding.PSDEstimator#

Compute power spectral density (PSD) using a multi-taper method.

The sampling frequency.

The lower frequency of interest.

The upper frequency of interest.

The bandwidth of the multi taper windowing function in Hz.

Use adaptive weights to combine the tapered spectra into PSD (slow, use n_jobs >> 1 to speed up computation).

Only use tapers with more than 90% spectral concentration within bandwidth.

Number of parallel jobs to use (only used if adaptive=True).

Normalization strategy. If “full”, the PSD will be normalized by the sampling rate as well as the length of the signal (as in Nitime). Default is 'length'.

fit(epochs_data[, y])

Compute power spectral density (PSD) using a multi-taper method.

fit_transform(X[, y])

Fit to data, then transform it.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

set_fit_request(*[, epochs_data])

Configure whether metadata should be requested to be passed to the fit method.

set_output(*[, transform])

Set output container.

Set the parameters of this estimator.

set_transform_request(*[, epochs_data])

Configure whether metadata should be requested to be passed to the transform method.

transform(epochs_data)

Compute power spectral density (PSD) using a multi-taper method.

Compute power spectral density (PSD) using a multi-taper method.

The label for each epoch.

The modified instance.

Fit to data, then transform it.

Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.

Target values (None for unsupervised transformations).

Additional fit parameters. Pass only if the estimator accepts additional params in its fit method.

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

Configure whether metadata should be requested to be passed to the fit method.

Note that this method is only relevant when this estimator is used as a sub-estimator within a meta-estimator and metadata routing is enabled with enable_metadata_routing=True (see sklearn.set_config()). Please check the User Guide on how the routing mechanism works.

The options for each parameter are:

True: metadata is requested, and passed to fit if provided. The request is ignored if metadata is not provided.

False: metadata is not requested and the meta-estimator will not pass it to fit.

None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.

str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.

Metadata routing for epochs_data parameter in fit.

Set output container.

See Introducing the set_output API for an example on how to use the API.

Configure output of transform and fit_transform.

“default”: Default output format of a transformer

“pandas”: DataFrame output

“polars”: Polars output

None: Transform configuration is unchanged

New in v1.4: “polars” option was added.

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Configure whether metadata should be requested to be passed to the transform method.

Note that this method is only relevant when this estimator is used as a sub-estimator within a meta-estimator and metadata routing is enabled with enable_metadata_routing=True (see sklearn.set_config()). Please check the User Guide on how the routing mechanism works.

The options for each parameter are:

True: metadata is requested, and passed to transform if provided. The request is ignored if metadata is not provided.

False: metadata is not requested and the meta-estimator will not pass it to transform.

None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.

str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.

Metadata routing for epochs_data parameter in transform.

Compute power spectral density (PSD) using a multi-taper method.

mne.decoding.LinearModel

---

## mne.decoding.ReceptiveField#

**URL:** https://mne.tools/stable/generated/mne.decoding.ReceptiveField.html

**Contents:**
- mne.decoding.ReceptiveField#
- Examples using mne.decoding.ReceptiveField#

Fit a receptive field model.

This allows you to fit an encoding model (stimulus to brain) or a decoding model (brain to stimulus) using time-lagged input features (for example, a spectro- or spatio-temporal receptive field, or STRF) [1][2][3][4].

The starting lag, in seconds (or samples if sfreq == 1).

The ending lag, in seconds (or samples if sfreq == 1). Must be >= tmin.

The sampling frequency used to convert times into samples.

Names for input features to the model. If None, feature names will be auto-generated from the shape of input data after running fit.

The model used in fitting inputs and outputs. This can be any scikit-learn-style model that contains a fit and predict method. If a float is passed, it will be interpreted as the alpha parameter to be passed to a Ridge regression model. If None, then a Ridge regression model with an alpha of 0 will be used.

If True (default), the sample mean is removed before fitting. If estimator is a sklearn.base.BaseEstimator, this must be None or match estimator.fit_intercept.

Defines how predictions will be scored. Currently must be one of ‘r2’ (coefficient of determination) or ‘corrcoef’ (the correlation coefficient).

If True, inverse coefficients will be computed upon fitting using the covariance matrix of the inputs, and the cross-covariance of the inputs/outputs, according to [5]. Defaults to False.

Number of jobs to run in parallel. Can be ‘cuda’ if CuPy is installed properly and estimator is None.

If True (default), correct the autocorrelation coefficients for non-zero delays for the fact that fewer samples are available. Disabling this speeds up performance at the cost of accuracy depending on the relationship between epoch length and model duration. Only used if estimator is float or None.

The coefficients from the model fit, reshaped for easy visualization. During mne.decoding.ReceptiveField.fit(), if y has one dimension (time), the n_outputs dimension here is omitted.

If fit, the inverted coefficients from the model.

The delays used to fit the model, in indices. To return the delays in seconds, use self.delays_ / self.sfreq

The rows to keep during model fitting after removing rows with missing values due to time delaying. This can be used to get an output equivalent to using numpy.convolve() or numpy.correlate() with mode='valid'.

Fit a receptive field model.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

Generate predictions with a receptive field.

Score predictions generated with a receptive field.

Set the parameters of this estimator.

For a causal system, the encoding model will have significant non-zero values only at positive lags. In other words, lags point backward in time relative to the input, so positive lags correspond to previous input time samples, while negative lags correspond to future input time samples.

Frédéric E. Theunissen, Stephen V. David, Nandini C. Singh, Ann Hsu, William E. Vinje, and Jack L. Gallant. Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to natural stimuli. Network: Computation in Neural Systems, 12(3):289–316, 2001. doi:10.1080/net.12.3.289.316.

Ben Willmore and Darragh Smyth. Methods for first-order kernel estimation: simple-cell receptive fields from responses to natural scenes. Network: Computation in Neural Systems, 14(3):553–577, 2003. doi:10.1088/0954-898X_14_3_309.

Michael J. Crosse, Giovanni M. Di Liberto, Adam Bednar, and Edmund C. Lalor. The multivariate temporal response function (mTRF) toolbox: a MATLAB toolbox for relating neural signals to continuous stimuli. Frontiers in Human Neuroscience, 2016. doi:10.3389/fnhum.2016.00604.

Christopher R. Holdgraf, Wendy de Heer, Brian Pasley, Jochem Rieger, Nathan Crone, Jack J. Lin, Robert T. Knight, and Frédéric E. Theunissen. Rapid tuning shifts in human auditory cortex enhance speech intelligibility. Nature Communications, 2016. doi:10.1038/ncomms13654.

Stefan Haufe, Frank Meinecke, Kai Görgen, Sven Dähne, John-Dylan Haynes, Benjamin Blankertz, and Felix Bießmann. On the interpretation of weight vectors of linear models in multivariate neuroimaging. NeuroImage, 87:96–110, 2014. doi:10.1016/j.neuroimage.2013.10.067.

Fit a receptive field model.

The input features for the model.

The output features for the model.

The instance so you can chain operations.

Receptive Field Estimation and Prediction

Spectro-temporal receptive field (STRF) estimation on continuous data

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

Generate predictions with a receptive field.

The input features for the model.

The output predictions. “Note that valid samples (those unaffected by edge artifacts during the time delaying step) can be obtained using y_pred[rf.valid_samples_].

Examples using predict:

Receptive Field Estimation and Prediction

Spectro-temporal receptive field (STRF) estimation on continuous data

Score predictions generated with a receptive field.

This calls self.predict, then masks the output of this and y` with ``self.valid_samples_. Finally, it passes this to a sklearn.metrics scorer.

The input features for the model.

Used for scikit-learn compatibility.

The scores estimated by the model for each output (e.g. mean R2 of predict(X)).

Examples using score:

Receptive Field Estimation and Prediction

Spectro-temporal receptive field (STRF) estimation on continuous data

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Receptive Field Estimation and Prediction

Spectro-temporal receptive field (STRF) estimation on continuous data

mne.decoding.Vectorizer

mne.decoding.TimeDelayingRidge

---

## mne.decoding.SlidingEstimator#

**URL:** https://mne.tools/stable/generated/mne.decoding.SlidingEstimator.html

**Contents:**
- mne.decoding.SlidingEstimator#
- Examples using mne.decoding.SlidingEstimator#

Fit, predict and score a series of models to each subset of the dataset along the last dimension. Each entry in the last dimension is referred to as a task.

The base estimator to iteratively fit on a subset of the dataset.

Score function (or loss function) with signature score_func(y, y_pred, **kwargs). Note that the “predict” method is automatically identified if scoring is a string (e.g. scoring='roc_auc' calls predict_proba), but is not automatically set if scoring is a callable (e.g. scoring=sklearn.metrics.roc_auc_score).

The number of jobs to run in parallel. If -1, it is set to the number of CPU cores. Requires the joblib package. None (default) is a marker for ‘unset’ that will be interpreted as n_jobs=1 (sequential execution) unless the call is performed under a joblib.parallel_config context manager that sets another value for n_jobs.

The position for the progress bar.

If True, allow 2D data as input (i.e. n_samples, n_features).

Control verbosity of the logging output. If None, use the default verbosity level. See the logging documentation and mne.verbose() for details. Should only be passed as a keyword argument.

List of fitted scikit-learn estimators (one per task).

Estimate distances of each data slice to the hyperplanes.

fit(X, y, **fit_params)

Fit a series of independent estimators to the dataset.

fit_transform(X, y, **fit_params)

Fit and transform a series of independent estimators to the dataset.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

Predict each data slice/task with a series of independent estimators.

Predict each data slice with a series of independent estimators.

Score each estimator on each task.

set_output(*[, transform])

Set output container.

Set the parameters of this estimator.

Transform each data slice/task with a series of independent estimators.

Estimate distances of each data slice to the hyperplanes.

The input samples. For each data slice, the corresponding estimator outputs the distance to the hyperplane, e.g.: [estimators[ii].decision_function(X[..., ii]) for ii in range(n_estimators)]. The feature dimension can be multidimensional e.g. X.shape = (n_samples, n_features_1, n_features_2, n_estimators).

Predicted distances for each estimator/data slice.

This requires base_estimator to have a decision_function method.

Fit a series of independent estimators to the dataset.

The training input samples. For each data slice, a clone estimator is fitted independently. The feature dimension can be multidimensional e.g. X.shape = (n_samples, n_features_1, n_features_2, n_tasks).

Parameters to pass to the fit method of the estimator.

Decoding source space data

Decoding sensor space data with generalization across time and conditions

Fit and transform a series of independent estimators to the dataset.

The training input samples. For each task, a clone estimator is fitted independently. The feature dimension can be multidimensional, e.g.:

Parameters to pass to the fit method of the estimator.

The predicted values for each estimator.

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

Predict each data slice/task with a series of independent estimators.

The number of tasks in X should match the number of tasks/estimators given at fit time.

The input samples. For each data slice, the corresponding estimator makes the sample predictions, e.g.: [estimators[ii].predict(X[..., ii]) for ii in range(n_estimators)]. The feature dimension can be multidimensional e.g. X.shape = (n_samples, n_features_1, n_features_2, n_tasks).

Predicted values for each estimator/data slice.

Predict each data slice with a series of independent estimators.

The number of tasks in X should match the number of tasks/estimators given at fit time.

The input samples. For each data slice, the corresponding estimator makes the sample probabilistic predictions, e.g.: [estimators[ii].predict_proba(X[..., ii]) for ii in range(n_estimators)]. The feature dimension can be multidimensional e.g. X.shape = (n_samples, n_features_1, n_features_2, n_tasks).

Predicted probabilities for each estimator/data slice/task.

Score each estimator on each task.

The number of tasks in X should match the number of tasks/estimators given at fit time, i.e. we need X.shape[-1] == len(self.estimators_).

The input samples. For each data slice, the corresponding estimator scores the prediction, e.g.: [estimators[ii].score(X[..., ii], y) for ii in range(n_estimators)]. The feature dimension can be multidimensional e.g. X.shape = (n_samples, n_features_1, n_features_2, n_tasks).

Score for each estimator/task.

Examples using score:

Decoding sensor space data with generalization across time and conditions

Set output container.

See Introducing the set_output API for an example on how to use the API.

Configure output of transform and fit_transform.

“default”: Default output format of a transformer

“pandas”: DataFrame output

“polars”: Polars output

None: Transform configuration is unchanged

New in v1.4: “polars” option was added.

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Transform each data slice/task with a series of independent estimators.

The number of tasks in X should match the number of tasks/estimators given at fit time.

The input samples. For each data slice/task, the corresponding estimator makes a transformation of the data, e.g. [estimators[ii].transform(X[..., ii]) for ii in range(n_estimators)]. The feature dimension can be multidimensional e.g. X.shape = (n_samples, n_features_1, n_features_2, n_tasks).

The transformed values generated by each estimator.

Decoding source space data

Decoding sensor space data with generalization across time and conditions

mne.decoding.TimeDelayingRidge

mne.decoding.GeneralizingEstimator

---

## mne.decoding.SpatialFilter#

**URL:** https://mne.tools/stable/generated/mne.decoding.SpatialFilter.html

**Contents:**
- mne.decoding.SpatialFilter#
- Examples using mne.decoding.SpatialFilter#

Container for spatial filter weights (evecs) and patterns.

For MNE-Python decoding classes, this container should be instantiated with mne.decoding.get_spatial_filter_from_estimator. Direct instantiation with external spatial filters is possible at your own risk.

This object is obtained either by generalized eigendecomposition (GED) algorithms such as mne.decoding.CSP, mne.decoding.SPoC, mne.decoding.SSD, mne.decoding.XdawnTransformer or by mne.decoding.LinearModel, wrapping linear models like SVM or Logit. The object stores the filters that projects sensor data to a reduced component space, and the corresponding patterns (obtained by pseudoinverse in GED case or Haufe’s trick in case of mne.decoding.LinearModel). It can also be directly initialized using filters from other transformers (e.g. PyRiemann), but make sure that the dimensions match.

The measurement info containing channel topography.

The spatial filters (transposed eigenvectors of the decomposition).

The eigenvalues of the decomposition. Defaults to None.

The patterns of the decomposition. If None, they will be computed from the filters using pseudoinverse. Defaults to None.

The method used to compute the patterns. Can be 'pinv' or 'haufe'. If patterns is None, it will be set to 'pinv'. Defaults to 'pinv'.

The measurement info.

The spatial filters (unmixing matrix). Applying these filters to the data gives the component time series.

The spatial patterns (mixing matrix/forward model). These represent the scalp topography of each component.

The eigenvalues associated with each component.

The method used to compute the patterns from the filters.

plot_filters([components, tmin, ch_type, ...])

Plot topographic maps of model filters.

plot_patterns([components, tmin, ch_type, ...])

Plot topographic maps of model patterns.

plot_scree([title, add_cumul_evals, axes, show])

Plot scree for GED eigenvalues.

The spatial filters and patterns are stored with shape (n_components, n_channels).

Filters and patterns are related by the following equation:

where \(\mathbf{A}\) is the matrix of patterns (the mixing matrix) and \(\mathbf{W}\) is the matrix of filters (the unmixing matrix).

For a detailed discussion on the difference between filters and patterns for GED see [1] and for linear models in general see [2].

Michael X Cohen. A tutorial on generalized eigendecomposition for denoising, contrast enhancement, and dimension reduction in multichannel electrophysiology. NeuroImage, 247:118809, 2022. doi:10.1016/j.neuroimage.2021.118809.

Stefan Haufe, Frank Meinecke, Kai Görgen, Sven Dähne, John-Dylan Haynes, Benjamin Blankertz, and Felix Bießmann. On the interpretation of weight vectors of linear models in multivariate neuroimaging. NeuroImage, 87:96–110, 2014. doi:10.1016/j.neuroimage.2013.10.067.

Plot topographic maps of model filters.

Indices of filters to plot. If “auto”, the number of axes determines the amount of filters. If None, all filters will be plotted. Defaults to None.

In case filters are distributed temporally, this can be used to align them with times and frequency. Use epochs.tmin, for example. Defaults to None.

The channel type to plot. For 'grad', the gradiometers are collected in pairs and the RMS for each pair is plotted. If None the first available channel type from order shown above is used. Defaults to None.

The scalings of the channel types to be applied for plotting. If None, defaults to dict(eeg=1e6, grad=1e13, mag=1e15).

Whether to add markers for sensor locations. If str, should be a valid matplotlib format string (e.g., 'r+' for red plusses, see the Notes section of plot()). If True (the default), black circles will be used.

If True, show channel names next to each sensor marker. If callable, channel names will be formatted using the callable; e.g., to delete the prefix ‘MEG ‘ from all channel names, pass the function lambda x: x.replace('MEG ', ''). If mask is not None, only non-masked sensor names will be shown.

Array indicating channel-time combinations to highlight with a distinct plotting style (useful for, e.g. marking which channels at which times a statistical test of the data reaches significance). Array elements set to True will be plotted with the parameters given in mask_params. Defaults to None, equivalent to an array of all False elements.

Additional plotting parameters for plotting significant sensors. Default (None) equals:

The number of contour lines to draw. If 0, no contours will be drawn. If a positive integer, that number of contour levels are chosen using the matplotlib tick locator (may sometimes be inaccurate, use array for accuracy). If array-like, the array values are used as the contour levels. The values should be in µV for EEG, fT for magnetometers and fT/m for gradiometers. If colorbar=True, the colorbar will have ticks corresponding to the contour levels. Default is 6.

The outlines to be drawn. If ‘head’, the default head scheme will be drawn. If dict, each key refers to a tuple of x and y positions, the values in ‘mask_pos’ will serve as image mask. Alternatively, a matplotlib patch object can be passed for advanced masking options, either directly or as a function that returns patches (required for multi-axis plots). If None, nothing will be drawn. Defaults to ‘head’.

The sphere parameters to use for the head outline. Can be array-like of shape (4,) to give the X/Y/Z origin and radius in meters, or a single float to give just the radius (origin assumed 0, 0, 0). Can also be an instance of a spherical ConductorModel to use the origin and radius from that object. Can also be a str, in which case:

'auto': the sphere is fit to external digitization points first, and to external + EEG digitization points if the former fails.

'eeglab': the head circle is defined by EEG electrodes 'Fpz', 'Oz', 'T7', and 'T8' (if 'Fpz' is not present, it will be approximated from the coordinates of 'Oz').

'extra': the sphere is fit to external digitization points.

'eeg': the sphere is fit to EEG digitization points.

'cardinal': the sphere is fit to cardinal digitization points.

'hpi': the sphere is fit to HPI coil digitization points.

Can also be a list of str, in which case the sphere is fit to the specified digitization points, which can be any combination of 'extra', 'eeg', 'cardinal', and 'hpi', as specified above. None (the default) is equivalent to 'auto' when enough extra digitization points are available, and (0, 0, 0, 0.095) otherwise.

Changed in version 1.1: Added 'eeglab' option.

Changed in version 1.11: Added 'extra', 'eeg', 'cardinal', 'hpi' and list of str options.

The image interpolation to be used. Options are 'cubic' (default) to use scipy.interpolate.CloughTocher2DInterpolator, 'nearest' to use scipy.spatial.Voronoi or 'linear' to use scipy.interpolate.LinearNDInterpolator.

Extrapolate to four points placed to form a square encompassing all data points, where each side of the square is three times the range of the data in the respective dimension.

Extrapolate only to nearby points (approximately to points closer than median inter-electrode distance). This will also set the mask to be polygonal based on the convex hull of the sensors.

Extrapolate out to the edges of the clipping circle. This will be on the head circle when the sensors are contained within the head circle, but it can extend beyond the head when sensors are plotted outside the head circle.

Value to extrapolate to on the topomap borders. If 'mean' (default), then each extrapolated point has the average value of its neighbours.

The resolution of the topomap image (number of pixels along each side).

Side length of each subplot in inches.

Colormap to use. If tuple, the first value indicates the colormap to use and the second value is a boolean defining interactivity. In interactive mode the colors are adjustable by clicking and dragging the colorbar with left and right mouse button. Left mouse button moves the scale up and down and right mouse button adjusts the range. Hitting space bar resets the range. Up and down arrows can be used to change the colormap. If None, 'Reds' is used for data that is either all-positive or all-negative, and 'RdBu_r' is used otherwise. 'interactive' is equivalent to (None, True). Defaults to None.

Interactive mode works smoothly only for a small amount of topomaps. Interactive mode is disabled by default for more than 2 topomaps.

Lower and upper bounds of the colormap, typically a numeric value in the same units as the data. Elements of the tuple may also be callable functions which take in a NumPy array and return a scalar.

If both entries are None, the bounds are set at ± the maximum absolute value of the data (yielding a colormap with midpoint at 0), or (0, max(abs(data))) if the (possibly baselined) data are all-positive. Providing None for just one entry will set the corresponding boundary at the min/max of the data. If vlim="joint", will compute the colormap limits jointly across all topomaps of the same channel type (instead of separately for each topomap), using the min/max of the data for that channel type. Defaults to (None, None).

How to normalize the colormap. If None, standard linear normalization is performed. If not None, vmin and vmax will be ignored. See Matplotlib docs for more details on colormap normalization, and the ERDs example for an example of its use.

Plot a colorbar in the rightmost column of the figure.

Formatting string for colorbar tick labels. See Format Specification Mini-Language for details.

The units to use for the colorbar label. Ignored if colorbar=False. If None and scalings=None the unit is automatically determined, otherwise the label will be “AU” indicating arbitrary units. Default is None.

The axes to plot into. If None, a new Figure will be created with the correct number of axes. If Axes are provided (either as a single instance or a list of axes), the number of axes provided must match the number of times provided (unless times is None). Default is None.

String format for topomap values. Defaults to 'Filter%01d'.

The number of rows and columns of topographies to plot. If either nrows or ncols is 'auto', the necessary number will be inferred. Defaults to nrows=1, ncols='auto'.

Show the figure if True.

Examples using plot_filters:

Linear classifier on sensor data with plot patterns and filters

Plot topographic maps of model patterns.

Indices of patterns to plot. If “auto”, the number of axes determines the amount of patterns. If None, all patterns will be plotted. Defaults to None.

In case patterns are distributed temporally, this can be used to align them with times and frequency. Use epochs.tmin, for example. Defaults to None.

The channel type to plot. For 'grad', the gradiometers are collected in pairs and the RMS for each pair is plotted. If None the first available channel type from order shown above is used. Defaults to None.

The scalings of the channel types to be applied for plotting. If None, defaults to dict(eeg=1e6, grad=1e13, mag=1e15).

Whether to add markers for sensor locations. If str, should be a valid matplotlib format string (e.g., 'r+' for red plusses, see the Notes section of plot()). If True (the default), black circles will be used.

If True, show channel names next to each sensor marker. If callable, channel names will be formatted using the callable; e.g., to delete the prefix ‘MEG ‘ from all channel names, pass the function lambda x: x.replace('MEG ', ''). If mask is not None, only non-masked sensor names will be shown.

Array indicating channel-time combinations to highlight with a distinct plotting style (useful for, e.g. marking which channels at which times a statistical test of the data reaches significance). Array elements set to True will be plotted with the parameters given in mask_params. Defaults to None, equivalent to an array of all False elements.

Additional plotting parameters for plotting significant sensors. Default (None) equals:

The number of contour lines to draw. If 0, no contours will be drawn. If a positive integer, that number of contour levels are chosen using the matplotlib tick locator (may sometimes be inaccurate, use array for accuracy). If array-like, the array values are used as the contour levels. The values should be in µV for EEG, fT for magnetometers and fT/m for gradiometers. If colorbar=True, the colorbar will have ticks corresponding to the contour levels. Default is 6.

The outlines to be drawn. If ‘head’, the default head scheme will be drawn. If dict, each key refers to a tuple of x and y positions, the values in ‘mask_pos’ will serve as image mask. Alternatively, a matplotlib patch object can be passed for advanced masking options, either directly or as a function that returns patches (required for multi-axis plots). If None, nothing will be drawn. Defaults to ‘head’.

The sphere parameters to use for the head outline. Can be array-like of shape (4,) to give the X/Y/Z origin and radius in meters, or a single float to give just the radius (origin assumed 0, 0, 0). Can also be an instance of a spherical ConductorModel to use the origin and radius from that object. Can also be a str, in which case:

'auto': the sphere is fit to external digitization points first, and to external + EEG digitization points if the former fails.

'eeglab': the head circle is defined by EEG electrodes 'Fpz', 'Oz', 'T7', and 'T8' (if 'Fpz' is not present, it will be approximated from the coordinates of 'Oz').

'extra': the sphere is fit to external digitization points.

'eeg': the sphere is fit to EEG digitization points.

'cardinal': the sphere is fit to cardinal digitization points.

'hpi': the sphere is fit to HPI coil digitization points.

Can also be a list of str, in which case the sphere is fit to the specified digitization points, which can be any combination of 'extra', 'eeg', 'cardinal', and 'hpi', as specified above. None (the default) is equivalent to 'auto' when enough extra digitization points are available, and (0, 0, 0, 0.095) otherwise.

Changed in version 1.1: Added 'eeglab' option.

Changed in version 1.11: Added 'extra', 'eeg', 'cardinal', 'hpi' and list of str options.

The image interpolation to be used. Options are 'cubic' (default) to use scipy.interpolate.CloughTocher2DInterpolator, 'nearest' to use scipy.spatial.Voronoi or 'linear' to use scipy.interpolate.LinearNDInterpolator.

Extrapolate to four points placed to form a square encompassing all data points, where each side of the square is three times the range of the data in the respective dimension.

Extrapolate only to nearby points (approximately to points closer than median inter-electrode distance). This will also set the mask to be polygonal based on the convex hull of the sensors.

Extrapolate out to the edges of the clipping circle. This will be on the head circle when the sensors are contained within the head circle, but it can extend beyond the head when sensors are plotted outside the head circle.

Value to extrapolate to on the topomap borders. If 'mean' (default), then each extrapolated point has the average value of its neighbours.

The resolution of the topomap image (number of pixels along each side).

Side length of each subplot in inches.

Colormap to use. If tuple, the first value indicates the colormap to use and the second value is a boolean defining interactivity. In interactive mode the colors are adjustable by clicking and dragging the colorbar with left and right mouse button. Left mouse button moves the scale up and down and right mouse button adjusts the range. Hitting space bar resets the range. Up and down arrows can be used to change the colormap. If None, 'Reds' is used for data that is either all-positive or all-negative, and 'RdBu_r' is used otherwise. 'interactive' is equivalent to (None, True). Defaults to None.

Interactive mode works smoothly only for a small amount of topomaps. Interactive mode is disabled by default for more than 2 topomaps.

Lower and upper bounds of the colormap, typically a numeric value in the same units as the data. Elements of the tuple may also be callable functions which take in a NumPy array and return a scalar.

If both entries are None, the bounds are set at ± the maximum absolute value of the data (yielding a colormap with midpoint at 0), or (0, max(abs(data))) if the (possibly baselined) data are all-positive. Providing None for just one entry will set the corresponding boundary at the min/max of the data. If vlim="joint", will compute the colormap limits jointly across all topomaps of the same channel type (instead of separately for each topomap), using the min/max of the data for that channel type. Defaults to (None, None).

How to normalize the colormap. If None, standard linear normalization is performed. If not None, vmin and vmax will be ignored. See Matplotlib docs for more details on colormap normalization, and the ERDs example for an example of its use.

Plot a colorbar in the rightmost column of the figure.

Formatting string for colorbar tick labels. See Format Specification Mini-Language for details.

The units to use for the colorbar label. Ignored if colorbar=False. If None and scalings=None the unit is automatically determined, otherwise the label will be “AU” indicating arbitrary units. Default is None.

The axes to plot into. If None, a new Figure will be created with the correct number of axes. If Axes are provided (either as a single instance or a list of axes), the number of axes provided must match the number of times provided (unless times is None). Default is None.

String format for topomap values. Defaults to 'Pattern%01d'.

The number of rows and columns of topographies to plot. If either nrows or ncols is 'auto', the necessary number will be inferred. Defaults to nrows=1, ncols='auto'.

Show the figure if True.

Examples using plot_patterns:

Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)

Continuous Target Decoding with SPoC

XDAWN Decoding From EEG data

Linear classifier on sensor data with plot patterns and filters

Compute spatial filters with Spatio-Spectral Decomposition (SSD)

Plot scree for GED eigenvalues.

Title for the plot. Defaults to 'Scree plot'.

Whether to add second line and y-axis for cumulative eigenvalues. Defaults to True.

The matplotlib axes to plot to. Defaults to None.

Show the figure if True.

Examples using plot_scree:

Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)

Continuous Target Decoding with SPoC

XDAWN Decoding From EEG data

Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)

Continuous Target Decoding with SPoC

XDAWN Decoding From EEG data

Linear classifier on sensor data with plot patterns and filters

Compute spatial filters with Spatio-Spectral Decomposition (SSD)

mne.decoding.XdawnTransformer

mne.decoding.compute_ems

---

## mne.decoding.SPoC#

**URL:** https://mne.tools/stable/generated/mne.decoding.SPoC.html

**Contents:**
- mne.decoding.SPoC#
- Examples using mne.decoding.SPoC#

Implementation of the SPoC spatial filtering.

Source Power Comodulation (SPoC) [1] allows to extract spatial filters and patterns by using a target (continuous) variable in the decomposition process in order to give preference to components whose power correlates with the target variable.

SPoC can be seen as an extension of the CSP driven by a continuous variable rather than a discrete variable. Typical applications include extraction of motor patterns using EMG power or audio patterns using sound envelope.

The number of components to decompose M/EEG signals.

If not None (same as 'empirical', default), allow regularization for covariance estimation. If float, shrinkage is used (0 <= shrinkage <= 1). For str options, reg will be passed to method to mne.compute_covariance().

If transform_into == ‘average_power’ and log is None or True, then applies a log transform to standardize the features, else the features are z-scored. If transform_into == ‘csp_space’, then log must be None.

If ‘average_power’ then self.transform will return the average power of each spatial filter. If ‘csp_space’ self.transform will return the data in CSP space. Defaults to ‘average_power’.

Parameters to pass to mne.compute_covariance().

Restricting transformation for covariance matrices before performing generalized eigendecomposition. If “restricting” only restriction to the principal subspace of signal_cov will be performed. If “whitening”, covariance matrices will be additionally rescaled according to the whitening for the signal_cov. If None, no restriction will be applied. Defaults to None.

The mne.Info object with information about the sensors and methods of measurement used for covariance estimation and generalized eigendecomposition. If None, one channel type and no projections will be assumed and if rank is dict, it will be sum of ranks per channel type. Defaults to None.

This controls the rank computation that can be read from the measurement info or estimated from the data. When a noise covariance is used for whitening, this should reflect the rank of that covariance, otherwise amplification of noise components can occur in whitening (e.g., often during source localization).

The rank will be estimated from the data after proper scaling of different channel types.

The rank is inferred from info. If data have been processed with Maxwell filtering, the Maxwell filtering header is used. Otherwise, the channel counts themselves are used. In both cases, the number of projectors is subtracted from the (effective) number of channels in the data. For example, if Maxwell filtering reduces the rank to 68, with two projectors the returned value will be 66.

The rank is assumed to be full, i.e. equal to the number of good channels. If a Covariance is passed, this can make sense if it has been (possibly improperly) regularized without taking into account the true data rank.

Calculate the rank only for a subset of channel types, and explicitly specify the rank for the remaining channel types. This can be extremely useful if you already know the rank of (part of) your data, for instance in case you have calculated it earlier.

This parameter must be a dictionary whose keys correspond to channel types in the data (e.g. 'meg', 'mag', 'grad', 'eeg'), and whose values are integers representing the respective ranks. For example, {'mag': 90, 'eeg': 45} will assume a rank of 90 and 45 for magnetometer data and EEG data, respectively.

The ranks for all channel types present in the data, but not specified in the dictionary will be estimated empirically. That is, if you passed a dataset containing magnetometer, gradiometer, and EEG data together with the dictionary from the previous example, only the gradiometer rank would be determined, while the specified magnetometer and EEG ranks would be taken for granted.

If fit, the SPoC spatial filters, else None.

If fit, the SPoC spatial patterns, else None.

If fit, the mean squared power for each component.

If fit, the std squared power for each component.

Estimate the SPoC decomposition on epochs.

fit_transform(X[, y])

Fit SPoC to data, then transform it.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

Project CSP features back to sensor space.

plot_filters(info[, components, ch_type, ...])

plot_patterns(info[, components, ch_type, ...])

set_output(*[, transform])

Set output container.

Set the parameters of this estimator.

Estimate epochs sources given the SPoC filters.

Sven Dähne, Frank C. Meinecke, Stefan Haufe, Johannes Höhne, Michael Tangermann, Klaus-Robert Müller, and Vadim V. Nikulin. SPoC: a novel framework for relating the amplitude of neuronal oscillations to behaviorally relevant parameters. NeuroImage, 86:111–122, 2014. doi:10.1016/j.neuroimage.2013.07.079.

Estimate the SPoC decomposition on epochs.

The data on which to estimate the SPoC.

The class for each epoch.

Returns the modified instance.

Continuous Target Decoding with SPoC

Fit SPoC to data, then transform it.

Fits transformer to X and y with optional parameters fit_params, and returns a transformed version of X.

The data on which to estimate the SPoC.

The class for each epoch.

Additional fitting parameters passed to the mne.decoding.CSP.fit() method. Not used for this class.

If self.transform_into == 'average_power' then returns the power of CSP features averaged over time and shape is (n_epochs, n_components). If self.transform_into == 'csp_space' then returns the data in CSP space and shape is (n_epochs, n_components, n_times).

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

Project CSP features back to sensor space.

The data in CSP power space.

The data in sensor space and shape (n_epochs, n_channels, n_components).

LEGACY: New code should use get_spatial_filter_from_estimator(clf, info=info).plot_filters().

Plot topographic filters of components.

The filters are used to extract discriminant neural sources from the measured data (a.k.a. the backward model).

The mne.Info object with information about the sensors and methods of measurement. Used for fitting. If not available, consider using mne.create_info().

The patterns to plot. If None, all components will be shown.

The channel type to plot. For 'grad', the gradiometers are collected in pairs and the RMS for each pair is plotted. If None the first available channel type from order shown above is used. Defaults to None.

The scalings of the channel types to be applied for plotting. If None, defaults to dict(eeg=1e6, grad=1e13, mag=1e15).

Whether to add markers for sensor locations. If str, should be a valid matplotlib format string (e.g., 'r+' for red plusses, see the Notes section of plot()). If True (the default), black circles will be used.

If True, show channel names next to each sensor marker. If callable, channel names will be formatted using the callable; e.g., to delete the prefix ‘MEG ‘ from all channel names, pass the function lambda x: x.replace('MEG ', ''). If mask is not None, only non-masked sensor names will be shown.

Array indicating channel-pattern combinations to highlight with a distinct plotting style. Array elements set to True will be plotted with the parameters given in mask_params. Defaults to None, equivalent to an array of all False elements.

Additional plotting parameters for plotting significant sensors. Default (None) equals:

The number of contour lines to draw. If 0, no contours will be drawn. If a positive integer, that number of contour levels are chosen using the matplotlib tick locator (may sometimes be inaccurate, use array for accuracy). If array-like, the array values are used as the contour levels. The values should be in µV for EEG, fT for magnetometers and fT/m for gradiometers. If colorbar=True, the colorbar will have ticks corresponding to the contour levels. Default is 6.

The outlines to be drawn. If ‘head’, the default head scheme will be drawn. If dict, each key refers to a tuple of x and y positions, the values in ‘mask_pos’ will serve as image mask. Alternatively, a matplotlib patch object can be passed for advanced masking options, either directly or as a function that returns patches (required for multi-axis plots). If None, nothing will be drawn. Defaults to ‘head’.

The sphere parameters to use for the head outline. Can be array-like of shape (4,) to give the X/Y/Z origin and radius in meters, or a single float to give just the radius (origin assumed 0, 0, 0). Can also be an instance of a spherical ConductorModel to use the origin and radius from that object. Can also be a str, in which case:

'auto': the sphere is fit to external digitization points first, and to external + EEG digitization points if the former fails.

'eeglab': the head circle is defined by EEG electrodes 'Fpz', 'Oz', 'T7', and 'T8' (if 'Fpz' is not present, it will be approximated from the coordinates of 'Oz').

'extra': the sphere is fit to external digitization points.

'eeg': the sphere is fit to EEG digitization points.

'cardinal': the sphere is fit to cardinal digitization points.

'hpi': the sphere is fit to HPI coil digitization points.

Can also be a list of str, in which case the sphere is fit to the specified digitization points, which can be any combination of 'extra', 'eeg', 'cardinal', and 'hpi', as specified above. None (the default) is equivalent to 'auto' when enough extra digitization points are available, and (0, 0, 0, 0.095) otherwise.

Changed in version 1.1: Added 'eeglab' option.

Changed in version 1.11: Added 'extra', 'eeg', 'cardinal', 'hpi' and list of str options.

The image interpolation to be used. Options are 'cubic' (default) to use scipy.interpolate.CloughTocher2DInterpolator, 'nearest' to use scipy.spatial.Voronoi or 'linear' to use scipy.interpolate.LinearNDInterpolator.

Extrapolate to four points placed to form a square encompassing all data points, where each side of the square is three times the range of the data in the respective dimension.

Extrapolate only to nearby points (approximately to points closer than median inter-electrode distance). This will also set the mask to be polygonal based on the convex hull of the sensors.

Extrapolate out to the edges of the clipping circle. This will be on the head circle when the sensors are contained within the head circle, but it can extend beyond the head when sensors are plotted outside the head circle.

Value to extrapolate to on the topomap borders. If 'mean' (default), then each extrapolated point has the average value of its neighbours.

The resolution of the topomap image (number of pixels along each side).

Side length of each subplot in inches.

Colormap to use. If tuple, the first value indicates the colormap to use and the second value is a boolean defining interactivity. In interactive mode the colors are adjustable by clicking and dragging the colorbar with left and right mouse button. Left mouse button moves the scale up and down and right mouse button adjusts the range. Hitting space bar resets the range. Up and down arrows can be used to change the colormap. If None, 'Reds' is used for data that is either all-positive or all-negative, and 'RdBu_r' is used otherwise. 'interactive' is equivalent to (None, True). Defaults to None.

Interactive mode works smoothly only for a small amount of topomaps. Interactive mode is disabled by default for more than 2 topomaps.

Lower and upper bounds of the colormap, typically a numeric value in the same units as the data. Elements of the tuple may also be callable functions which take in a NumPy array and return a scalar.

If both entries are None, the bounds are set at ± the maximum absolute value of the data (yielding a colormap with midpoint at 0), or (0, max(abs(data))) if the (possibly baselined) data are all-positive. Providing None for just one entry will set the corresponding boundary at the min/max of the data. If vlim="joint", will compute the colormap limits jointly across all topomaps of the same channel type (instead of separately for each topomap), using the min/max of the data for that channel type. Defaults to (None, None).

How to normalize the colormap. If None, standard linear normalization is performed. If not None, vmin and vmax will be ignored. See Matplotlib docs for more details on colormap normalization, and the ERDs example for an example of its use.

Plot a colorbar in the rightmost column of the figure.

Formatting string for colorbar tick labels. See Format Specification Mini-Language for details.

The units to use for the colorbar label. Ignored if colorbar=False. If None the label will be “AU” indicating arbitrary units. Default is None.

The axes to plot into. If None, a new Figure will be created with the correct number of axes. If Axes are provided (either as a single instance or a list of axes), the number of axes provided must match the number of times provided (unless times is None). Default is None.

String format for topomap values. Defaults to “CSP%01d”.

The number of rows and columns of topographies to plot. If either nrows or ncols is 'auto', the necessary number will be inferred. Defaults to nrows=1, ncols='auto'.

Show the figure if True.

LEGACY: New code should use get_spatial_filter_from_estimator(clf, info=info).plot_patterns().

Plot topographic patterns of components.

The patterns explain how the measured data was generated from the neural sources (a.k.a. the forward model).

The mne.Info object with information about the sensors and methods of measurement. Used for fitting. If not available, consider using mne.create_info().

The patterns to plot. If None, all components will be shown.

The channel type to plot. For 'grad', the gradiometers are collected in pairs and the RMS for each pair is plotted. If None the first available channel type from order shown above is used. Defaults to None.

The scalings of the channel types to be applied for plotting. If None, defaults to dict(eeg=1e6, grad=1e13, mag=1e15).

Whether to add markers for sensor locations. If str, should be a valid matplotlib format string (e.g., 'r+' for red plusses, see the Notes section of plot()). If True (the default), black circles will be used.

If True, show channel names next to each sensor marker. If callable, channel names will be formatted using the callable; e.g., to delete the prefix ‘MEG ‘ from all channel names, pass the function lambda x: x.replace('MEG ', ''). If mask is not None, only non-masked sensor names will be shown.

Array indicating channel-pattern combinations to highlight with a distinct plotting style. Array elements set to True will be plotted with the parameters given in mask_params. Defaults to None, equivalent to an array of all False elements.

Additional plotting parameters for plotting significant sensors. Default (None) equals:

The number of contour lines to draw. If 0, no contours will be drawn. If a positive integer, that number of contour levels are chosen using the matplotlib tick locator (may sometimes be inaccurate, use array for accuracy). If array-like, the array values are used as the contour levels. The values should be in µV for EEG, fT for magnetometers and fT/m for gradiometers. If colorbar=True, the colorbar will have ticks corresponding to the contour levels. Default is 6.

The outlines to be drawn. If ‘head’, the default head scheme will be drawn. If dict, each key refers to a tuple of x and y positions, the values in ‘mask_pos’ will serve as image mask. Alternatively, a matplotlib patch object can be passed for advanced masking options, either directly or as a function that returns patches (required for multi-axis plots). If None, nothing will be drawn. Defaults to ‘head’.

The sphere parameters to use for the head outline. Can be array-like of shape (4,) to give the X/Y/Z origin and radius in meters, or a single float to give just the radius (origin assumed 0, 0, 0). Can also be an instance of a spherical ConductorModel to use the origin and radius from that object. Can also be a str, in which case:

'auto': the sphere is fit to external digitization points first, and to external + EEG digitization points if the former fails.

'eeglab': the head circle is defined by EEG electrodes 'Fpz', 'Oz', 'T7', and 'T8' (if 'Fpz' is not present, it will be approximated from the coordinates of 'Oz').

'extra': the sphere is fit to external digitization points.

'eeg': the sphere is fit to EEG digitization points.

'cardinal': the sphere is fit to cardinal digitization points.

'hpi': the sphere is fit to HPI coil digitization points.

Can also be a list of str, in which case the sphere is fit to the specified digitization points, which can be any combination of 'extra', 'eeg', 'cardinal', and 'hpi', as specified above. None (the default) is equivalent to 'auto' when enough extra digitization points are available, and (0, 0, 0, 0.095) otherwise.

Changed in version 1.1: Added 'eeglab' option.

Changed in version 1.11: Added 'extra', 'eeg', 'cardinal', 'hpi' and list of str options.

The image interpolation to be used. Options are 'cubic' (default) to use scipy.interpolate.CloughTocher2DInterpolator, 'nearest' to use scipy.spatial.Voronoi or 'linear' to use scipy.interpolate.LinearNDInterpolator.

Extrapolate to four points placed to form a square encompassing all data points, where each side of the square is three times the range of the data in the respective dimension.

Extrapolate only to nearby points (approximately to points closer than median inter-electrode distance). This will also set the mask to be polygonal based on the convex hull of the sensors.

Extrapolate out to the edges of the clipping circle. This will be on the head circle when the sensors are contained within the head circle, but it can extend beyond the head when sensors are plotted outside the head circle.

Value to extrapolate to on the topomap borders. If 'mean' (default), then each extrapolated point has the average value of its neighbours.

The resolution of the topomap image (number of pixels along each side).

Side length of each subplot in inches.

Colormap to use. If tuple, the first value indicates the colormap to use and the second value is a boolean defining interactivity. In interactive mode the colors are adjustable by clicking and dragging the colorbar with left and right mouse button. Left mouse button moves the scale up and down and right mouse button adjusts the range. Hitting space bar resets the range. Up and down arrows can be used to change the colormap. If None, 'Reds' is used for data that is either all-positive or all-negative, and 'RdBu_r' is used otherwise. 'interactive' is equivalent to (None, True). Defaults to None.

Interactive mode works smoothly only for a small amount of topomaps. Interactive mode is disabled by default for more than 2 topomaps.

Lower and upper bounds of the colormap, typically a numeric value in the same units as the data. If both entries are None, the bounds are set at (min(data), max(data)). Providing None for just one entry will set the corresponding boundary at the min/max of the data. Defaults to (None, None).

How to normalize the colormap. If None, standard linear normalization is performed. If not None, vmin and vmax will be ignored. See Matplotlib docs for more details on colormap normalization, and the ERDs example for an example of its use.

Plot a colorbar in the rightmost column of the figure.

Formatting string for colorbar tick labels. See Format Specification Mini-Language for details.

The units to use for the colorbar label. Ignored if colorbar=False. If None the label will be “AU” indicating arbitrary units. Default is None.

The axes to plot into. If None, a new Figure will be created with the correct number of axes. If Axes are provided (either as a single instance or a list of axes), the number of axes provided must match the number of times provided (unless times is None). Default is None.

String format for topomap values. Defaults to “CSP%01d”.

The number of rows and columns of topographies to plot. If either nrows or ncols is 'auto', the necessary number will be inferred. Defaults to nrows=1, ncols='auto'.

Show the figure if True.

Set output container.

See Introducing the set_output API for an example on how to use the API.

Configure output of transform and fit_transform.

“default”: Default output format of a transformer

“pandas”: DataFrame output

“polars”: Polars output

None: Transform configuration is unchanged

New in v1.4: “polars” option was added.

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Estimate epochs sources given the SPoC filters.

If self.transform_into == ‘average_power’ then returns the power of CSP features averaged over time and shape (n_epochs, n_components) If self.transform_into == ‘csp_space’ then returns the data in CSP space and shape is (n_epochs, n_components, n_times).

Continuous Target Decoding with SPoC

mne.decoding.GeneralizingEstimator

---

## mne.decoding.SSD#

**URL:** https://mne.tools/stable/generated/mne.decoding.SSD.html

**Contents:**
- mne.decoding.SSD#
- Examples using mne.decoding.SSD#

Signal decomposition using the Spatio-Spectral Decomposition (SSD).

SSD seeks to maximize the power at a frequency band of interest while simultaneously minimizing it at the flanking (surrounding) frequency bins (considered noise). It extremizes the covariance matrices associated with signal and noise [1].

SSD can either be used as a dimensionality reduction method or a ‘denoised’ low rank factorization method [2].

The mne.Info object with information about the sensors and methods of measurement. Must match the input data.

Filtering for the frequencies of interest.

Filtering for the frequencies of non-interest.

Which covariance estimator to use. If not None (same as ‘empirical’), allow regularization for covariance estimation. If float, shrinkage is used (0 <= shrinkage <= 1). For str options, reg will be passed to method mne.compute_covariance().

The number of components to extract from the signal. If None, the number of components equal to the rank of the data are returned (see rank).

The indices of good channels.

If set to True, the components are sorted according to the spectral ratio. See Eq. (24) in [1].

If return_filtered is True, data is bandpassed and projected onto the SSD components.

If sort_by_spectral_ratio is set to True, then the SSD sources will be sorted according to their spectral ratio which is calculated based on mne.time_frequency.psd_array_welch(). The n_fft parameter sets the length of FFT used. The default (None) will use 1 second of data. See mne.time_frequency.psd_array_welch() for more information.

As in mne.decoding.SPoC The default is None.

Restricting transformation for covariance matrices before performing generalized eigendecomposition. If “restricting” only restriction to the principal subspace of signal_cov will be performed. If “whitening”, covariance matrices will be additionally rescaled according to the whitening for the signal_cov. If None, no restriction will be applied. Defaults to “whitening”.

As in mne.decoding.SPoC This controls the rank computation that can be read from the measurement info or estimated from the data, which determines the maximum possible number of components. See Notes of mne.compute_rank() for details. We recommend to use ‘full’ when working with epoched data.

The spatial filters to be multiplied with the signal.

The patterns for reconstructing the signal from the filtered data.

Remove selected components from the signal.

Estimate the SSD decomposition on raw or epoched data.

fit_transform(X[, y])

Fit SSD to data, then transform it.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

get_spectral_ratio(ssd_sources)

Get the spectal signal-to-noise ratio for each spatial filter.

set_output(*[, transform])

Set output container.

Set the parameters of this estimator.

Estimate epochs sources given the SSD filters.

Vadim V Nikulin, Guido Nolte, and Gabriel Curio. A novel method for reliable and fast extraction of neuronal EEG/MEG oscillations on the basis of spatio-spectral decomposition. NeuroImage, 55(4):1528–1535, 2011. doi:10.1016/j.neuroimage.2011.01.057.

Stefan Haufe, Sven Dähne, and Vadim V Nikulin. Dimensionality reduction for the analysis of brain oscillations. NeuroImage, 101:583–597, 2014. doi:https://doi.org/10.1016/j.neuroimage.2014.06.073.

Remove selected components from the signal.

This procedure will reconstruct M/EEG signals from which the dynamics described by the excluded components is subtracted (denoised by low-rank factorization). See [2] for more information.

Unlike in other classes with an apply method, only NumPy arrays are supported (not instances of MNE objects).

The input data from which to estimate the SSD. Either 2D array obtained from continuous data or 3D array obtained from epoched data.

Estimate the SSD decomposition on raw or epoched data.

The input data from which to estimate the SSD. Either 2D array obtained from continuous data or 3D array obtained from epoched data.

Ignored; exists for compatibility with scikit-learn pipelines.

Returns the modified instance.

Compute spatial filters with Spatio-Spectral Decomposition (SSD)

Fit SSD to data, then transform it.

Fits transformer to X and y with optional parameters fit_params, and returns a transformed version of X.

The input data from which to estimate the SSD. Either 2D array obtained from continuous data or 3D array obtained from epoched data.

Ignored; exists for compatibility with scikit-learn pipelines.

Additional fitting parameters passed to the mne.decoding.SSD.fit() method. Not used for this class.

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

Get the spectal signal-to-noise ratio for each spatial filter.

Spectral ratio measure for best n_components selection See [1], Eq. (24).

Data projected to SSD space.

Array with the sprectal ratio value for each component.

Array of indices for sorting spec_ratio.

Examples using get_spectral_ratio:

Compute spatial filters with Spatio-Spectral Decomposition (SSD)

Set output container.

See Introducing the set_output API for an example on how to use the API.

Configure output of transform and fit_transform.

“default”: Default output format of a transformer

“pandas”: DataFrame output

“polars”: Polars output

None: Transform configuration is unchanged

New in v1.4: “polars” option was added.

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Estimate epochs sources given the SSD filters.

The input data from which to estimate the SSD. Either 2D array obtained from continuous data or 3D array obtained from epoched data.

Examples using transform:

Compute spatial filters with Spatio-Spectral Decomposition (SSD)

Compute spatial filters with Spatio-Spectral Decomposition (SSD)

mne.decoding.XdawnTransformer

---

## mne.decoding.TemporalFilter#

**URL:** https://mne.tools/stable/generated/mne.decoding.TemporalFilter.html

**Contents:**
- mne.decoding.TemporalFilter#

Estimator to filter data array along the last dimension.

Applies a zero-phase low-pass, high-pass, band-pass, or band-stop filter to the channels.

l_freq and h_freq are the frequencies below which and above which, respectively, to filter out of the data. Thus the uses are:

l_freq < h_freq: band-pass filter

l_freq > h_freq: band-stop filter

l_freq is not None, h_freq is None: low-pass filter

l_freq is None, h_freq is not None: high-pass filter

See mne.filter.filter_data().

Low cut-off frequency in Hz. If None the data are only low-passed.

High cut-off frequency in Hz. If None the data are only high-passed.

Sampling frequency in Hz.

Length of the FIR filter to use (if applicable):

int: specified length in samples.

‘auto’ (default in 0.14): the filter length is chosen based on the size of the transition regions (7 times the reciprocal of the shortest transition band).

str: (default in 0.13 is “10s”) a human-readable time in units of “s” or “ms” (e.g., “10s” or “5500ms”) will be converted to that number of samples if phase="zero", or the shortest power-of-two length at least that duration for phase="zero-double".

Width of the transition band at the low cut-off frequency in Hz (high pass or cutoff 1 in bandpass). Can be “auto” (default in 0.14) to use a multiple of l_freq:

Only used for method='fir'.

Width of the transition band at the high cut-off frequency in Hz (low pass or cutoff 2 in bandpass). Can be “auto” (default in 0.14) to use a multiple of h_freq:

Only used for method='fir'.

Number of jobs to run in parallel. Can be ‘cuda’ if cupy is installed properly and method=’fir’.

‘fir’ will use overlap-add FIR filtering, ‘iir’ will use IIR forward-backward filtering (via filtfilt).

Dictionary of parameters to use for IIR filtering. See mne.filter.construct_iir_filter for details. If iir_params is None and method=”iir”, 4th order Butterworth will be used.

The window to use in FIR design, can be “hamming”, “hann”, or “blackman”.

Can be “firwin” (default) to use scipy.signal.firwin(), or “firwin2” to use scipy.signal.firwin2(). “firwin” uses a time-domain design technique that generally gives improved attenuation using fewer samples than “firwin2”.

Do nothing (for scikit-learn compatibility purposes).

fit_transform(X[, y])

Fit to data, then transform it.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

set_output(*[, transform])

Set output container.

Set the parameters of this estimator.

Filter data along the last dimension.

Do nothing (for scikit-learn compatibility purposes).

The data to be filtered over the last dimension. The channels dimension can be zero when passing a 2D array.

Not used, for scikit-learn compatibility issues.

The modified instance.

Fit to data, then transform it.

Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.

Target values (None for unsupervised transformations).

Additional fit parameters. Pass only if the estimator accepts additional params in its fit method.

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

Set output container.

See Introducing the set_output API for an example on how to use the API.

Configure output of transform and fit_transform.

“default”: Default output format of a transformer

“pandas”: DataFrame output

“polars”: Polars output

None: Transform configuration is unchanged

New in v1.4: “polars” option was added.

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Filter data along the last dimension.

The data to be filtered over the last dimension. The channels dimension can be zero when passing a 2D array.

The data after filtering.

mne.decoding.TimeFrequency

---

## mne.decoding.TimeDelayingRidge#

**URL:** https://mne.tools/stable/generated/mne.decoding.TimeDelayingRidge.html

**Contents:**
- mne.decoding.TimeDelayingRidge#
- Examples using mne.decoding.TimeDelayingRidge#

Ridge regression of data with time delays.

The starting lag, in seconds (or samples if sfreq == 1). Negative values correspond to times in the past.

The ending lag, in seconds (or samples if sfreq == 1). Positive values correspond to times in the future. Must be >= tmin.

The sampling frequency used to convert times into samples.

The ridge (or laplacian) regularization factor.

Can be "ridge" (default) or "laplacian". Can also be a 2-element list specifying how to regularize in time and across adjacent features.

If True (default), the sample mean is removed before fitting.

The number of jobs to use. Can be an int (default 1) or 'cuda'.

If True (default), correct the autocorrelation coefficients for non-zero delays for the fact that fewer samples are available. Disabling this speeds up performance at the cost of accuracy depending on the relationship between epoch length and model duration. Only used if estimator is float or None.

Estimate the coefficients of the linear model.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

score(X, y[, sample_weight])

Return coefficient of determination on test data.

Set the parameters of this estimator.

set_score_request(*[, sample_weight])

Configure whether metadata should be requested to be passed to the score method.

This class is meant to be used with mne.decoding.ReceptiveField by only implicitly doing the time delaying. For reasonable receptive field and input signal sizes, it should be more CPU and memory efficient by using frequency-domain methods (FFTs) to compute the auto- and cross-correlations.

Estimate the coefficients of the linear model.

The training input samples to estimate the linear coefficients.

Returns the modified instance.

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

The predicted response.

Return coefficient of determination on test data.

The coefficient of determination, \(R^2\), is defined as \((1 - \frac{u}{v})\), where \(u\) is the residual sum of squares ((y_true - y_pred)** 2).sum() and \(v\) is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \(R^2\) score of 0.0.

Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.

\(R^2\) of self.predict(X) w.r.t. y.

The \(R^2\) score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of r2_score(). This influences the score method of all the multioutput regressors (except for MultiOutputRegressor).

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Configure whether metadata should be requested to be passed to the score method.

Note that this method is only relevant when this estimator is used as a sub-estimator within a meta-estimator and metadata routing is enabled with enable_metadata_routing=True (see sklearn.set_config()). Please check the User Guide on how the routing mechanism works.

The options for each parameter are:

True: metadata is requested, and passed to score if provided. The request is ignored if metadata is not provided.

False: metadata is not requested and the meta-estimator will not pass it to score.

None: metadata is not requested, and the meta-estimator will raise an error if the user provides it.

str: metadata should be passed to the meta-estimator with this given alias instead of the original name.

The default (sklearn.utils.metadata_routing.UNCHANGED) retains the existing request. This allows you to change the request for some parameters and not others.

Metadata routing for sample_weight parameter in score.

Spectro-temporal receptive field (STRF) estimation on continuous data

mne.decoding.ReceptiveField

mne.decoding.SlidingEstimator

---

## mne.decoding.TimeFrequency#

**URL:** https://mne.tools/stable/generated/mne.decoding.TimeFrequency.html

**Contents:**
- mne.decoding.TimeFrequency#

Time frequency transformer.

Time-frequency transform of times series along the last axis.

Sampling frequency of the data.

The time-frequency method. ‘morlet’ convolves a Morlet wavelet. ‘multitaper’ uses Morlet wavelets windowed with multiple DPSS multitapers.

Number of cycles in the Morlet wavelet. Fixed number or one per frequency.

If None and method=multitaper, will be set to 4.0 (3 tapers). Time x (Full) Bandwidth product. Only applies if method == ‘multitaper’. The number of good tapers (low-bias) is chosen automatically based on this to equal floor(time_bandwidth - 1).

Use the FFT for convolutions or not.

To reduce memory usage, decimation factor after time-frequency decomposition. If int, returns tfr[…, ::decim]. If slice, returns tfr[…, decim].

Decimation may create aliasing artifacts, yet decimation is done after the convolutions.

‘complex’ : single trial complex.

‘power’ : single trial power.

‘phase’ : single trial phase.

The number of jobs to run in parallel. If -1, it is set to the number of CPU cores. Requires the joblib package. None (default) is a marker for ‘unset’ that will be interpreted as n_jobs=1 (sequential execution) unless the call is performed under a joblib.parallel_config context manager that sets another value for n_jobs. The number of epochs to process at the same time. The parallelization is implemented across channels.

Control verbosity of the logging output. If None, use the default verbosity level. See the logging documentation and mne.verbose() for details. Should only be passed as a keyword argument.

Do nothing (for scikit-learn compatibility purposes).

fit_transform(X[, y])

Time-frequency transform of times series along the last axis.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

set_output(*[, transform])

Set output container.

Set the parameters of this estimator.

Time-frequency transform of times series along the last axis.

Do nothing (for scikit-learn compatibility purposes).

Time-frequency transform of times series along the last axis.

The training data samples. The channel dimension can be zero- or 1-dimensional.

For scikit-learn compatibility purposes.

The time-frequency transform of the data, where n_channels can be zero- or 1-dimensional.

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

Set output container.

See Introducing the set_output API for an example on how to use the API.

Configure output of transform and fit_transform.

“default”: Default output format of a transformer

“pandas”: DataFrame output

“polars”: Polars output

None: Transform configuration is unchanged

New in v1.4: “polars” option was added.

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Time-frequency transform of times series along the last axis.

The training data samples. The channel dimension can be zero- or 1-dimensional.

The time-frequency transform of the data, where n_channels can be zero- or 1-dimensional.

mne.decoding.TemporalFilter

mne.decoding.UnsupervisedSpatialFilter

---

## mne.decoding.UnsupervisedSpatialFilter#

**URL:** https://mne.tools/stable/generated/mne.decoding.UnsupervisedSpatialFilter.html

**Contents:**
- mne.decoding.UnsupervisedSpatialFilter#
- Examples using mne.decoding.UnsupervisedSpatialFilter#

Use unsupervised spatial filtering across time and samples.

Estimator using some decomposition algorithm.

If True, the estimator is fitted on the average across samples (e.g. epochs).

Fit the spatial filters.

fit_transform(X[, y])

Transform the data to its filtered components after fitting.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

Inverse transform the data to its original space.

set_output(*[, transform])

Set output container.

Set the parameters of this estimator.

Transform the data to its spatial filters.

Fit the spatial filters.

The data to be filtered.

Used for scikit-learn compatibility.

Return the modified instance.

Transform the data to its filtered components after fitting.

The data to be filtered.

Used for scikit-learn compatibility.

The transformed data.

Examples using fit_transform:

Analysis of evoked response using ICA and PCA reduction techniques

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

Inverse transform the data to its original space.

The data to be inverted.

The transformed data.

Set output container.

See Introducing the set_output API for an example on how to use the API.

Configure output of transform and fit_transform.

“default”: Default output format of a transformer

“pandas”: DataFrame output

“polars”: Polars output

None: Transform configuration is unchanged

New in v1.4: “polars” option was added.

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Transform the data to its spatial filters.

The data to be filtered.

The transformed data.

Analysis of evoked response using ICA and PCA reduction techniques

mne.decoding.TimeFrequency

mne.decoding.Vectorizer

---

## mne.decoding.Vectorizer#

**URL:** https://mne.tools/stable/generated/mne.decoding.Vectorizer.html

**Contents:**
- mne.decoding.Vectorizer#
- Examples using mne.decoding.Vectorizer#

Transform n-dimensional array into 2D array of n_samples by n_features.

This class reshapes an n-dimensional array into an n_samples * n_features array, usable by the estimators and transformers of scikit-learn.

Stores the original shape of data.

Store the shape of the features of X.

fit_transform(X[, y])

Fit the data, then transform in one step.

get_metadata_routing()

Get metadata routing of this object.

Get parameters for this estimator.

Transform 2D data back to its original feature shape.

set_output(*[, transform])

Set output container.

Set the parameters of this estimator.

Convert given array into two dimensions.

Store the shape of the features of X.

The data to fit. Can be, for example a list, or an array of at least 2d. The first dimension must be of length n_samples, where samples are the independent samples used by the estimator (e.g. n_epochs for epoched data).

Used for scikit-learn compatibility.

Return the modified instance.

Fit the data, then transform in one step.

The data to fit. Can be, for example a list, or an array of at least 2d. The first dimension must be of length n_samples, where samples are the independent samples used by the estimator (e.g. n_epochs for epoched data).

Used for scikit-learn compatibility.

The transformed data.

Get metadata routing of this object.

Please check User Guide on how the routing mechanism works.

A MetadataRequest encapsulating routing information.

Get parameters for this estimator.

If True, will return the parameters for this estimator and contained subobjects that are estimators.

Parameter names mapped to their values.

Transform 2D data back to its original feature shape.

Data to be transformed back to original shape.

The data transformed into shape as used in fit. The first dimension is of length n_samples.

Set output container.

See Introducing the set_output API for an example on how to use the API.

Configure output of transform and fit_transform.

“default”: Default output format of a transformer

“pandas”: DataFrame output

“polars”: Polars output

None: Transform configuration is unchanged

New in v1.4: “polars” option was added.

Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.

Estimator parameters.

Convert given array into two dimensions.

The data to fit. Can be, for example a list, or an array of at least 2d. The first dimension must be of length n_samples, where samples are the independent samples used by the estimator (e.g. n_epochs for epoched data).

The transformed data.

XDAWN Decoding From EEG data

Linear classifier on sensor data with plot patterns and filters

mne.decoding.UnsupervisedSpatialFilter

mne.decoding.ReceptiveField

---

## Morph surface source estimate#

**URL:** https://mne.tools/stable/auto_examples/inverse/morph_surface_stc.html

**Contents:**
- Morph surface source estimate#
- Setting up SourceMorph for SourceEstimate#
- Apply morph to (Vector) SourceEstimate#
- Plot results#
- Reading and writing SourceMorph from and to disk#
- References#

Go to the end to download the full example code.

This example demonstrates how to morph an individual subject’s mne.SourceEstimate to a common reference space. We achieve this using mne.SourceMorph. Pre-computed data will be morphed based on a spherical representation of the cortex computed using the spherical registration of FreeSurfer (https://surfer.nmr.mgh.harvard.edu/fswiki/SurfaceRegAndTemplates) [1]. This transform will be used to morph the surface vertices of the subject towards the reference vertices. Here we will use ‘fsaverage’ as a reference space (see https://surfer.nmr.mgh.harvard.edu/fswiki/FsAverage).

The transformation will be applied to the surface source estimate. A plot depicting the successful morph will be created for the spherical and inflated surface representation of 'fsaverage', overlaid with the morphed surface source estimate.

For background information about morphing see Morphing and averaging source estimates.

In MNE, surface source estimates represent the source space simply as lists of vertices (see The SourceEstimate data structure). This list can either be obtained from mne.SourceSpaces (src) or from the stc itself. If you use the source space, be sure to use the source space from the forward or inverse operator, because vertices can be excluded during forward computation due to proximity to the BEM inner skull surface:

We also need to specify the set of vertices to morph to. This can be done using the spacing parameter, but for consistency it’s better to pass the src_to parameter.

Since the default values of mne.compute_source_morph() are spacing=5, subject_to='fsaverage', in this example we could actually omit the src_to and subject_to arguments below. The ico-5 fsaverage source space contains the special values [np.arange(10242)] * 2, but in general this will not be true for other spacings or other subjects. Thus it is recommended to always pass the destination src for consistency.

Initialize SourceMorph for SourceEstimate

The morph will be applied to the source estimate data, by giving it as the first argument to the morph we computed above.

An instance of SourceMorph can be saved, by calling morph.save.

This method allows for specification of a filename under which the morph will be save in “.h5” format. If no file extension is provided, “-morph.h5” will be appended to the respective defined filename:

Reading a saved source morph can be achieved by using mne.read_source_morph():

Once the environment is set up correctly, no information such as subject_from or subjects_dir must be provided, since it can be inferred from the data and use morph to ‘fsaverage’ by default. SourceMorph can further be used without creating an instance and assigning it to a variable. Instead mne.compute_source_morph() and mne.SourceMorph.apply() can be easily chained into a handy one-liner. Taking this together the shortest possible way to morph data directly would be:

For more examples, check out examples using SourceMorph.apply.

Douglas N. Greve, Lise Van der Haegen, Qing Cai, Steven Stufflebeam, Mert R. Sabuncu, Bruce Fischl, and Marc Brysbaert. A surface-based analysis of language lateralization and cortical asymmetry. Journal of Cognitive Neuroscience, 25(9):1477–1492, 2013. doi:10.1162/jocn_a_00405.

Total running time of the script: (0 minutes 11.252 seconds)

Download Jupyter notebook: morph_surface_stc.ipynb

Download Python source code: morph_surface_stc.py

Download zipped: morph_surface_stc.zip

Gallery generated by Sphinx-Gallery

Compute source power estimate by projecting the covariance with MNE

Morph volumetric source estimate

---

## Morph volumetric source estimate#

**URL:** https://mne.tools/stable/auto_examples/inverse/morph_volume_stc.html

**Contents:**
- Morph volumetric source estimate#
- Get a SourceMorph object for VolSourceEstimate#
- Apply morph to VolSourceEstimate#
- Convert morphed VolSourceEstimate into NIfTI#
- Plot results#
- Reading and writing SourceMorph from and to disk#
- References#

Go to the end to download the full example code.

This example demonstrates how to morph an individual subject’s mne.VolSourceEstimate to a common reference space. We achieve this using mne.SourceMorph. Data will be morphed based on an affine transformation and a nonlinear registration method known as Symmetric Diffeomorphic Registration (SDR) by [1].

Transformation is estimated from the subject’s anatomical T1 weighted MRI (brain) to FreeSurfer’s ‘fsaverage’ T1 weighted MRI (brain).

Afterwards the transformation will be applied to the volumetric source estimate. The result will be plotted, showing the fsaverage T1 weighted anatomical MRI, overlaid with the morphed volumetric source estimate.

Compute example data. For reference see Compute MNE-dSPM inverse solution on evoked data in volume source space.

subject_from can typically be inferred from src, and subject_to is set to ‘fsaverage’ by default. subjects_dir can be None when set in the environment. In that case SourceMorph can be initialized taking src as only argument. See mne.SourceMorph for more details.

The default parameter setting for zooms will cause the reference volumes to be resliced before computing the transform. A value of ‘5’ would cause the function to reslice to an isotropic voxel size of 5 mm. The higher this value the less accurate but faster the computation will be.

The recommended way to use this is to morph to a specific destination source space so that different subject_from morphs will go to the same space.` A standard usage for volumetric data reads:

The morph can be applied to the source estimate data, by giving it as the first argument to the morph.apply() method.

Volumetric morphing is much slower than surface morphing because the volume for each time point is individually resampled and SDR morphed. The mne.SourceMorph.compute_vol_morph_mat() method can be used to compute an equivalent sparse matrix representation by computing the transformation for each source point individually. This generally takes a few minutes to compute, but can be saved to disk and be reused. The resulting sparse matrix operation is very fast (about 400× faster) to apply. This approach is more efficient when the number of time points to be morphed exceeds the number of source space points, which is generally in the thousands. This can easily occur when morphing many time points and multiple conditions.

We can convert our morphed source estimate into a NIfTI volume using morph.apply(..., output='nifti1').

An instance of SourceMorph can be saved, by calling morph.save.

This methods allows for specification of a filename under which the morph will be save in “.h5” format. If no file extension is provided, “-morph.h5” will be appended to the respective defined filename:

Reading a saved source morph can be achieved by using mne.read_source_morph():

Once the environment is set up correctly, no information such as subject_from or subjects_dir must be provided, since it can be inferred from the data and used morph to ‘fsaverage’ by default, e.g.:

Brian B. Avants, Charles L. Epstein, Murray C. Grossman, and James C. Gee. Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain. Medical Image Analysis, 12(1):26–41, 2008. doi:10.1016/j.media.2007.06.004.

Total running time of the script: (0 minutes 17.756 seconds)

Download Jupyter notebook: morph_volume_stc.ipynb

Download Python source code: morph_volume_stc.py

Download zipped: morph_volume_stc.zip

Gallery generated by Sphinx-Gallery

Morph surface source estimate

Computing source timecourses with an XFit-like multi-dipole model

---

## Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)#

**URL:** https://mne.tools/stable/auto_examples/decoding/decoding_csp_eeg.html

**Contents:**
- Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)#
- References#

Go to the end to download the full example code.

Decoding of motor imagery applied to EEG data decomposed using CSP. A classifier is then applied to features extracted on CSP-filtered signals.

See https://en.wikipedia.org/wiki/Common_spatial_pattern and [1]. The EEGBCI dataset is documented in [2] and on the PhysioNet documentation page. The dataset is available at PhysioNet [3].

Classification with linear discrimant analysis

Look at performance over time

Zoltan J. Koles. The quantitative extraction and topographic mapping of the abnormal components in the clinical EEG. Electroencephalography and Clinical Neurophysiology, 79(6):440–447, 1991. doi:10.1016/0013-4694(91)90163-X.

Gerwin Schalk, Dennis J. McFarland, Thilo Hinterberger, Niels Birbaumer, and Jonathan R. Wolpaw. BCI2000: a general-purpose brain-computer interface (BCI) system. IEEE Transactions on Biomedical Engineering, 51(6):1034–1043, 2004. doi:10.1109/TBME.2004.827072.

Ary L. Goldberger, Luis A. N. Amaral, Leon Glass, Jeffrey M. Hausdorff, Plamen Ch. Ivanov, Roger G. Mark, Joseph E. Mietus, George B. Moody, Chung-Kang Peng, and H. Eugene Stanley. PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation, 2000. doi:10.1161/01.CIR.101.23.e215.

Total running time of the script: (0 minutes 6.191 seconds)

Download Jupyter notebook: decoding_csp_eeg.ipynb

Download Python source code: decoding_csp_eeg.py

Download zipped: decoding_csp_eeg.zip

Gallery generated by Sphinx-Gallery

Machine Learning (Decoding, Encoding, and MVPA)

Decoding in time-frequency space using Common Spatial Patterns (CSP)

---

## Optically pumped magnetometer (OPM) data#

**URL:** https://mne.tools/stable/auto_examples/datasets/opm_data.html

**Contents:**
- Optically pumped magnetometer (OPM) data#
- Prepare data for localization#
- Perform dipole fitting#
- Perform minimum-norm localization#

Go to the end to download the full example code.

In this dataset, electrical median nerve stimulation was delivered to the left wrist of the subject. Somatosensory evoked fields were measured using nine QuSpin SERF OPMs placed over the right-hand side somatomotor area. Here we demonstrate how to localize these custom OPM data in MNE.

First we filter and epoch the data:

Examine our coordinate alignment for source localization and compute a forward operator:

The Head<->MRI transform is an identity matrix, as the co-registration method used equates the two coordinate systems. This mis-defines the head coordinate system (which should be based on the LPA, Nasion, and RPA) but should be fine for these analyses.

Due to the small number of sensors, there will be some leakage of activity to areas with low/no sensitivity. Constraining the source space to areas we are sensitive to might be a good idea.

Total running time of the script: (0 minutes 20.723 seconds)

Download Jupyter notebook: opm_data.ipynb

Download Python source code: opm_data.py

Download zipped: opm_data.zip

Gallery generated by Sphinx-Gallery

Single trial linear regression analysis with the LIMO dataset

From raw data to dSPM on SPM Faces dataset

---

## Permutation T-test on sensor data#

**URL:** https://mne.tools/stable/auto_examples/stats/sensor_permutation_test.html

**Contents:**
- Permutation T-test on sensor data#

Go to the end to download the full example code.

One tests if the signal significantly deviates from 0 during a fixed time window of interest. Here computation is performed on MNE sample dataset between 40 and 60 ms.

View location of significantly active sensors

Total running time of the script: (0 minutes 2.769 seconds)

Download Jupyter notebook: sensor_permutation_test.ipynb

Download Python source code: sensor_permutation_test.py

Download zipped: sensor_permutation_test.zip

Gallery generated by Sphinx-Gallery

Regression on continuous data (rER[P/F])

Analysing continuous features with binning and regression in sensor space

---

## Plotting eye-tracking heatmaps in MNE-Python#

**URL:** https://mne.tools/stable/auto_examples/visualization/eyetracking_plot_heatmap.html

**Contents:**
- Plotting eye-tracking heatmaps in MNE-Python#
- Data loading#
- Process and epoch the data#
- Plot a heatmap of the eye-tracking data#
- Overlaying plots with images#
- Displaying the heatmap in units of visual angle#

Go to the end to download the full example code.

This tutorial covers plotting eye-tracking position data as a heatmap.

Importing Data from Eyetracking devices Working with eye tracker data in MNE-Python

As usual we start by importing the modules we need and loading some example data: eye-tracking data recorded from SR research’s '.asc' file format.

First we will interpolate missing data during blinks and epoch the data.

To make a heatmap of the eye-tracking data, we can use the function plot_gaze(). We will need to define the dimensions of our canvas; for this file, the eye position data are reported in pixels, so we’ll use the screen resolution of the participant screen (1920x1080) as the width and height. We can also use the sigma parameter to smooth the plot.

The (0, 0) pixel coordinates are at the top-left of the trackable area of the screen for many eye trackers.

We can use matplotlib to plot gaze heatmaps on top of stimuli images. We’ll customize a Colormap to make some values of the heatmap completely transparent. We’ll then use the vlim parameter to force the heatmap to start at a value greater than the darkest value in our previous heatmap, which will make the darkest colors of the heatmap transparent.

In scientific publications it is common to report gaze data as the visual angle from the participants eye to the screen. We can convert the units of our gaze data to radians of visual angle before plotting the heatmap:

Total running time of the script: (0 minutes 7.970 seconds)

Download Jupyter notebook: eyetracking_plot_heatmap.ipynb

Download Python source code: eyetracking_plot_heatmap.py

Download zipped: eyetracking_plot_heatmap.zip

Gallery generated by Sphinx-Gallery

Whitening evoked data with a noise covariance

Plotting sensor layouts of MEG systems

---

## Plotting sensor layouts of EEG systems#

**URL:** https://mne.tools/stable/auto_examples/visualization/montage_sgskip.html

**Contents:**
- Plotting sensor layouts of EEG systems#

Go to the end to download the full example code.

This example illustrates how to load all the EEG system montages shipped in MNE-python, and display it on the fsaverage template subject.

Check all montages against a sphere

Check all montages against fsaverage

Download Jupyter notebook: montage_sgskip.ipynb

Download Python source code: montage_sgskip.py

Download zipped: montage_sgskip.zip

Gallery generated by Sphinx-Gallery

Plot the MNE brain and helmet

Plot a cortical parcellation

---

## Plotting sensor layouts of MEG systems#

**URL:** https://mne.tools/stable/auto_examples/visualization/meg_sensors.html

**Contents:**
- Plotting sensor layouts of MEG systems#
- Neuromag#
- CTF#
- BTi#
- KIT#
- Artemis123#

Go to the end to download the full example code.

Show sensor layouts of different MEG systems.

Total running time of the script: (0 minutes 22.210 seconds)

Download Jupyter notebook: meg_sensors.ipynb

Download Python source code: meg_sensors.py

Download zipped: meg_sensors.zip

Gallery generated by Sphinx-Gallery

Plotting eye-tracking heatmaps in MNE-Python

Plot the MNE brain and helmet

---

## Plotting the full vector-valued MNE solution#

**URL:** https://mne.tools/stable/auto_examples/inverse/vector_mne_solution.html

**Contents:**
- Plotting the full vector-valued MNE solution#

Go to the end to download the full example code.

The source space that is used for the inverse computation defines a set of dipoles, distributed across the cortex. When visualizing a source estimate, it is sometimes useful to show the dipole directions in addition to their estimated magnitude. This can be accomplished by computing a mne.VectorSourceEstimate and plotting it with stc.plot, which uses plot_vector_source_estimates() under the hood rather than plot_source_estimates().

It can also be instructive to visualize the actual dipole/activation locations in 3D space in a glass brain, as opposed to activations imposed on an inflated surface (as typically done in mne.SourceEstimate.plot()), as it allows you to get a better sense of the underlying source geometry.

Plot the source estimate:

Plot the activation in the direction of maximal power for this data:

The normal is very similar:

You can also do this with a fixed-orientation inverse. It looks a lot like the result above because the loose=0.2 orientation constraint keeps sources close to fixed orientation:

Total running time of the script: (0 minutes 36.602 seconds)

Download Jupyter notebook: vector_mne_solution.ipynb

Download Python source code: vector_mne_solution.py

Download zipped: vector_mne_solution.zip

Gallery generated by Sphinx-Gallery

Compute Trap-Music on evoked data

Examples on open datasets

---

## Plotting with mne.viz.Brain#

**URL:** https://mne.tools/stable/auto_examples/visualization/brain.html

**Contents:**
- Plotting with mne.viz.Brain#
- Load data#
- Add source information#
- Modify the view of the brain#
- Highlight a region on the brain#
- Include the head in the image#
- Add sensors positions#
- Add current dipoles#
- Create a screenshot for exporting the brain image#

Go to the end to download the full example code.

In this example, we’ll show how to use mne.viz.Brain.

In this example we use the sample data which is data from a subject being presented auditory and visual stimuli to display the functionality of mne.viz.Brain for plotting data on a brain.

Plot source information.

You can adjust the view of the brain using show_view method.

It can be useful to highlight a region of the brain for analyses. To highlight a region on the brain you can use the add_label method. Labels are stored in the Freesurfer label directory from the recon-all for that subject. Labels can also be made following the Freesurfer instructions Here we will show Brodmann Area 44.

The MNE sample dataset contains only a subselection of the Freesurfer labels created during the recon-all.

Add a head image using the add_head method.

To put into context the data that generated the source time course, the sensor positions can be displayed as well.

Dipole modeling as in The role of dipole orientations in distributed source localization can be plotted on the brain as well.

Also, we can a static image of the brain using screenshot (above), which will allow us to add a colorbar. This is useful for figures in publications.

Total running time of the script: (0 minutes 43.463 seconds)

Download Jupyter notebook: brain.ipynb

Download Python source code: brain.py

Download zipped: brain.zip

Gallery generated by Sphinx-Gallery

How to convert 3D electrode positions to a 2D image

Visualize channel over epochs as an image

---

## Plot a cortical parcellation#

**URL:** https://mne.tools/stable/auto_examples/visualization/parcellation.html

**Contents:**
- Plot a cortical parcellation#
- References#

Go to the end to download the full example code.

In this example, we download the HCP-MMP1.0 parcellation [1] and show it on fsaverage. We will also download the customized 448-label aparc parcellation from [2].

The HCP-MMP dataset has license terms restricting its use. Of particular relevance:

“I will acknowledge the use of WU-Minn HCP data and data derived from WU-Minn HCP data when publicly presenting any results or algorithms that benefitted from their use.”

We can also plot a combined set of labels (23 per hemisphere).

We can add another custom parcellation

Matthew F. Glasser, Timothy S. Coalson, Emma C. Robinson, Carl D. Hacker, John Harwell, Essa Yacoub, Kamil Ugurbil, Jesper Andersson, Christian F. Beckmann, Mark Jenkinson, Stephen M. Smith, and David C. Van Essen. A multi-modal parcellation of human cerebral cortex. Nature, 536(7615):171–178, 2016. doi:10.1038/nature18933.

Sheraz Khan, Javeria A. Hashmi, Fahimeh Mamashli, Konstantinos Michmizos, Manfred G. Kitzbichler, Hari Bharadwaj, Yousra Bekhti, Santosh Ganesan, Keri-Lee A. Garel, Susan Whitfield-Gabrieli, Randy L. Gollub, Jian Kong, Lucia M. Vaina, Kunjan D. Rana, Steven M. Stufflebeam, Matti S. Hämäläinen, and Tal Kenet. Maturation trajectories of cortical resting-state networks depend on the mediating frequency band. NeuroImage, 174:57–68, 2018. doi:10.1016/j.neuroimage.2018.02.018.

Total running time of the script: (0 minutes 5.970 seconds)

Download Jupyter notebook: parcellation.ipynb

Download Python source code: parcellation.py

Download zipped: parcellation.zip

Gallery generated by Sphinx-Gallery

Plotting sensor layouts of EEG systems

Plot single trial activity, grouped by ROI and sorted by RT

---

## Plot custom topographies for MEG sensors#

**URL:** https://mne.tools/stable/auto_examples/visualization/topo_customized.html

**Contents:**
- Plot custom topographies for MEG sensors#

Go to the end to download the full example code.

This example exposes the iter_topography() function that makes it very easy to generate custom sensor topography plots. Here we will plot the power spectrum of each channel on a topographic layout.

Total running time of the script: (0 minutes 6.787 seconds)

Download Jupyter notebook: topo_customized.ipynb

Download Python source code: topo_customized.py

Download zipped: topo_customized.zip

Gallery generated by Sphinx-Gallery

Compare evoked responses for different conditions

Cross-hemisphere comparison

---

## Plot point-spread functions (PSFs) and cross-talk functions (CTFs)#

**URL:** https://mne.tools/stable/auto_examples/inverse/psf_ctf_vertices.html

**Contents:**
- Plot point-spread functions (PSFs) and cross-talk functions (CTFs)#
- Visualize#

Go to the end to download the full example code.

Visualise PSF and CTF at one vertex for sLORETA.

The green spheres indicate the true source location, and the black spheres the maximum of the distribution.

Total running time of the script: (0 minutes 10.056 seconds)

Download Jupyter notebook: psf_ctf_vertices.ipynb

Download Python source code: psf_ctf_vertices.py

Download zipped: psf_ctf_vertices.zip

Gallery generated by Sphinx-Gallery

Visualize source leakage among labels using a circular graph

Compute cross-talk functions for LCMV beamformers

---

## Plot point-spread functions (PSFs) for a volume#

**URL:** https://mne.tools/stable/auto_examples/inverse/psf_volume.html

**Contents:**
- Plot point-spread functions (PSFs) for a volume#
- Visualize#

Go to the end to download the full example code.

Visualise PSF at one volume vertex for sLORETA.

For the volume, create a coarse source space for speed (don’t do this in real code!), then compute the forward using this source space.

Now make an inverse operator and compute the PSF at a source.

Total running time of the script: (0 minutes 26.176 seconds)

Download Jupyter notebook: psf_volume.ipynb

Download Python source code: psf_volume.py

Download zipped: psf_volume.zip

Gallery generated by Sphinx-Gallery

Compute cross-talk functions for LCMV beamformers

Compute Rap-Music on evoked data

---

## Plot single trial activity, grouped by ROI and sorted by RT#

**URL:** https://mne.tools/stable/auto_examples/visualization/roi_erpimage_by_rt.html

**Contents:**
- Plot single trial activity, grouped by ROI and sorted by RT#

Go to the end to download the full example code.

This will produce what is sometimes called an event related potential / field (ERP/ERF) image.

The EEGLAB example file, which contains an experiment with button press responses to simple visual stimuli, is read in and response times are calculated. Regions of Interest are determined by the channel types (in 10/20 channel notation, even channels are right, odd are left, and ‘z’ are central). The median and the Global Field Power within each channel group is calculated, and the trials are plotted, sorting by response time.

Load EEGLAB example data (a small EEG dataset)

Plot using global field power

Total running time of the script: (0 minutes 6.177 seconds)

Download Jupyter notebook: roi_erpimage_by_rt.ipynb

Download Python source code: roi_erpimage_by_rt.py

Download zipped: roi_erpimage_by_rt.zip

Gallery generated by Sphinx-Gallery

Plot a cortical parcellation

Sensitivity map of SSP projections

---

## Plot the MNE brain and helmet#

**URL:** https://mne.tools/stable/auto_examples/visualization/mne_helmet.html

**Contents:**
- Plot the MNE brain and helmet#

Go to the end to download the full example code.

This tutorial shows how to make the MNE helmet + brain image.

Total running time of the script: (0 minutes 13.074 seconds)

Download Jupyter notebook: mne_helmet.ipynb

Download Python source code: mne_helmet.py

Download zipped: mne_helmet.zip

Gallery generated by Sphinx-Gallery

Plotting sensor layouts of MEG systems

Plotting sensor layouts of EEG systems

---

## Reading/Writing a noise covariance matrix#

**URL:** https://mne.tools/stable/auto_examples/io/read_noise_covariance_matrix.html

**Contents:**
- Reading/Writing a noise covariance matrix#

Go to the end to download the full example code.

How to plot a noise covariance matrix.

Total running time of the script: (0 minutes 1.160 seconds)

Download Jupyter notebook: read_noise_covariance_matrix.ipynb

Download Python source code: read_noise_covariance_matrix.py

Download zipped: read_noise_covariance_matrix.zip

Gallery generated by Sphinx-Gallery

How to use data in neural ensemble (NEO) format

---

## Reading an inverse operator#

**URL:** https://mne.tools/stable/auto_examples/inverse/read_inverse.html

**Contents:**
- Reading an inverse operator#

Go to the end to download the full example code.

The inverse operator’s source space is shown in 3D.

Show the 3D source space

Total running time of the script: (0 minutes 2.521 seconds)

Download Jupyter notebook: read_inverse.ipynb

Download Python source code: read_inverse.py

Download zipped: read_inverse.zip

Gallery generated by Sphinx-Gallery

Compute Rap-Music on evoked data

---

## Reading an STC file#

**URL:** https://mne.tools/stable/auto_examples/inverse/read_stc.html

**Contents:**
- Reading an STC file#

Go to the end to download the full example code.

STC files contain activations on cortex ie. source reconstructions

Download Jupyter notebook: read_stc.ipynb

Download Python source code: read_stc.py

Download zipped: read_stc.zip

Gallery generated by Sphinx-Gallery

Reading an inverse operator

Compute spatial resolution metrics in source space

---

## Reading XDF EEG data#

**URL:** https://mne.tools/stable/auto_examples/io/read_xdf.html

**Contents:**
- Reading XDF EEG data#
- References#

Go to the end to download the full example code.

Here we read some sample XDF data. Although we do not analyze it here, this recording is of a short parallel auditory response (pABR) experiment [1] and was provided by the Maddox Lab.

Melissa J. Polonenko and Ross K. Maddox. The Parallel Auditory Brainstem Response. Trends in Hearing, 23:2331216519871395, 2019. doi:10.1177/2331216519871395.

Total running time of the script: (0 minutes 4.720 seconds)

Download Jupyter notebook: read_xdf.ipynb

Download Python source code: read_xdf.py

Download zipped: read_xdf.zip

Gallery generated by Sphinx-Gallery

Reading/Writing a noise covariance matrix

---

## Receptive Field Estimation and Prediction#

**URL:** https://mne.tools/stable/auto_examples/decoding/receptive_field_mtrf.html

**Contents:**
- Receptive Field Estimation and Prediction#
- Load the data from the publication#
- Create and fit a receptive field model#
  - Investigate model coefficients#
- Create and fit a stimulus reconstruction model#
  - Visualize stimulus reconstruction#
  - Investigate model coefficients#
- References#

Go to the end to download the full example code.

This example reproduces figures from Lalor et al.’s mTRF toolbox in MATLAB [1]. We will show how the mne.decoding.ReceptiveField class can perform a similar function along with scikit-learn. We will first fit a linear encoding model using the continuously-varying speech envelope to predict activity of a 128 channel EEG system. Then, we will take the reverse approach and try to predict the speech envelope from the EEG (known in the literature as a decoding model, or simply stimulus reconstruction).

First we will load the data collected in [1]. In this experiment subjects listened to natural speech. Raw EEG and the speech stimulus are provided. We will load these below, downsampling the data in order to speed up computation since we know that our features are primarily low-frequency in nature. Then we’ll visualize both the EEG and speech envelope.

We will construct an encoding model to find the linear relationship between a time-delayed version of the speech envelope and the EEG signal. This allows us to make predictions about the response to new stimuli.

Finally, we will look at how the linear coefficients (sometimes referred to as beta values) are distributed across time delays as well as across the scalp. We will recreate figure 1 and figure 2 from [1].

We will now demonstrate another use case for the for the mne.decoding.ReceptiveField class as we try to predict the stimulus activity from the EEG data. This is known in the literature as a decoding, or stimulus reconstruction model [1]. A decoding model aims to find the relationship between the speech signal and a time-delayed version of the EEG. This can be useful as we exploit all of the available neural data in a multivariate context, compared to the encoding case which treats each M/EEG channel as an independent feature. Therefore, decoding models might provide a better quality of fit (at the expense of not controlling for stimulus covariance), especially for low SNR stimuli such as speech.

To get a sense of our model performance, we can plot the actual and predicted stimulus envelopes side by side.

Finally, we will look at how the decoding model coefficients are distributed across the scalp. We will attempt to recreate figure 5 from [1]. The decoding model weights reflect the channels that contribute most toward reconstructing the stimulus signal, but are not directly interpretable in a neurophysiological sense. Here we also look at the coefficients obtained via an inversion procedure [2], which have a more straightforward interpretation as their value (and sign) directly relates to the stimulus signal’s strength (and effect direction).

Michael J. Crosse, Giovanni M. Di Liberto, Adam Bednar, and Edmund C. Lalor. The multivariate temporal response function (mTRF) toolbox: a MATLAB toolbox for relating neural signals to continuous stimuli. Frontiers in Human Neuroscience, 2016. doi:10.3389/fnhum.2016.00604.

Stefan Haufe, Frank Meinecke, Kai Görgen, Sven Dähne, John-Dylan Haynes, Benjamin Blankertz, and Felix Bießmann. On the interpretation of weight vectors of linear models in multivariate neuroimaging. NeuroImage, 87:96–110, 2014. doi:10.1016/j.neuroimage.2013.10.067.

Total running time of the script: (0 minutes 15.418 seconds)

Download Jupyter notebook: receptive_field_mtrf.ipynb

Download Python source code: receptive_field_mtrf.py

Download zipped: receptive_field_mtrf.zip

Gallery generated by Sphinx-Gallery

Linear classifier on sensor data with plot patterns and filters

Compute spatial filters with Spatio-Spectral Decomposition (SSD)

---

## Representational Similarity Analysis#

**URL:** https://mne.tools/stable/auto_examples/decoding/decoding_rsa_sgskip.html

**Contents:**
- Representational Similarity Analysis#
- References#

Go to the end to download the full example code.

Representational Similarity Analysis is used to perform summary statistics on supervised classifications where the number of classes is relatively high. It consists in characterizing the structure of the confusion matrix to infer the similarity between brain responses and serves as a proxy for characterizing the space of mental representations [1][2][3].

In this example, we perform RSA on responses to 24 object images (among a list of 92 images). Subjects were presented with images of human, animal and inanimate objects [4]. Here we use the 24 unique images of faces and body parts.

this example will download a very large (~6GB) file, so we will not build the images below.

Let’s restrict the number of conditions to speed up computation

Define stimulus - trigger mapping

Let’s make the event_id dictionary

Let’s plot some conditions

Representational Similarity Analysis (RSA) is a neuroimaging-specific appelation to refer to statistics applied to the confusion matrix also referred to as the representational dissimilarity matrices (RDM).

Compared to the approach from Cichy et al. we’ll use a multiclass classifier (Multinomial Logistic Regression) while the paper uses all pairwise binary classification task to make the RDM. Also we use here the ROC-AUC as performance metric while the paper uses accuracy. Finally here for the sake of time we use RSA on a window of data while Cichy et al. did it for all time instants separately.

Compute confusion matrix using ROC-AUC

Confusion matrix related to mental representations have been historically summarized with dimensionality reduction using multi-dimensional scaling [1]. See how the face samples cluster together.

Roger N. Shepard. Multidimensional scaling, tree-fitting, and clustering. Science, 210(4468):390–398, 1980. doi:10.1126/science.210.4468.390.

Aarre Laakso and Garrison Cottrell. Content and cluster analysis: assessing representational similarity in neural systems. Philosophical Psychology, 13(1):47–76, 2000. doi:10.1080/09515080050002726.

Nikolaus Kriegeskorte, Marieke Mur, and Peter Bandettini. Representational similarity analysis – connecting the branches of systems neuroscience. Frontiers in Systems Neuroscience, 2:4, 2008. doi:10.3389/neuro.06.004.2008.

Radoslaw Martin Cichy, Dimitrios Pantazis, and Aude Oliva. Resolving human object recognition in space and time. Nature Neuroscience, 17(3):455–462, 2014. doi:10.1038/nn.3635.

Download Jupyter notebook: decoding_rsa_sgskip.ipynb

Download Python source code: decoding_rsa_sgskip.py

Download zipped: decoding_rsa_sgskip.zip

Gallery generated by Sphinx-Gallery

Decoding in time-frequency space using Common Spatial Patterns (CSP)

Decoding source space data

---

## Sensitivity map of SSP projections#

**URL:** https://mne.tools/stable/auto_examples/visualization/ssp_projs_sensitivity_map.html

**Contents:**
- Sensitivity map of SSP projections#

Go to the end to download the full example code.

This example shows the sources that have a forward field similar to the first SSP vector correcting for ECG.

Total running time of the script: (0 minutes 4.007 seconds)

Download Jupyter notebook: ssp_projs_sensitivity_map.ipynb

Download Python source code: ssp_projs_sensitivity_map.py

Download zipped: ssp_projs_sensitivity_map.zip

Gallery generated by Sphinx-Gallery

Plot single trial activity, grouped by ROI and sorted by RT

Compare evoked responses for different conditions

---

## Simulate raw data using subject anatomy#

**URL:** https://mne.tools/stable/auto_examples/simulation/simulated_raw_data_using_subject_anatomy.html

**Contents:**
- Simulate raw data using subject anatomy#
- Create simulated source activity#
- Simulate raw data#
- Extract epochs and compute evoked responsses#
- Reconstruct simulated source time courses using dSPM inverse operator#
- References#

Go to the end to download the full example code.

This example illustrates how to generate source estimates and simulate raw data using subject anatomy with the mne.simulation.SourceSimulator class. Once the raw data is simulated, generated source estimates are reconstructed using dynamic statistical parametric mapping (dSPM) inverse operator.

In this example, raw data will be simulated for the sample subject, so its information needs to be loaded. This step will download the data if it not already on your machine. Subjects directory is also set so it doesn’t need to be given to functions.

First, we get an info structure from the sample subject.

To simulate sources, we also need a source space. It can be obtained from the forward solution of the sample subject.

To simulate raw data, we need to define when the activity occurs using events matrix and specify the IDs of each event. Noise covariance matrix also needs to be defined. Here, both are loaded from the sample dataset, but they can also be specified by the user.

In order to simulate source time courses, labels of desired active regions need to be specified for each of the 4 simulation conditions. Make a dictionary that maps conditions to activation strengths within aparc.a2009s [1] labels. In the aparc.a2009s parcellation:

‘G_temp_sup-G_T_transv’ is the label for primary auditory area

‘S_calcarine’ is the label for primary visual area

In each of the 4 conditions, only the primary area is activated. This means that during the activations of auditory areas, there are no activations in visual areas and vice versa. Moreover, for each condition, contralateral region is more active (here, 2 times more) than the ipsilateral.

Generate source time courses for each region. In this example, we want to simulate source activity for a single condition at a time. Therefore, each evoked response will be parametrized by latency and duration.

Here, SourceSimulator is used, which allows to specify where (label), what (source_time_series), and when (events) event type will occur.

We will add data for 4 areas, each of which contains 2 labels. Since add_data method accepts 1 label per call, it will be called 2 times per area.

Evoked responses are generated such that the main component peaks at 100ms with a duration of around 30ms, which first appears in the contralateral cortex. This is followed by a response in the ipsilateral cortex with a peak about 15ms after. The amplitude of the activations will be 2 times higher in the contralateral region, as explained before.

When the activity occurs is defined using events. In this case, they are taken from the original raw data. The first column is the sample of the event, the second is not used. The third one is the event id, which is different for each of the 4 areas.

Project the source time series to sensor space. Three types of noise will be added to the simulated raw data:

multivariate Gaussian noise obtained from the noise covariance from the sample data

The SourceSimulator can be given directly to the simulate_raw() function.

Here, source time courses for auditory and visual areas are reconstructed separately and their difference is shown. This was done merely for better visual representation of source reconstruction. As expected, when high activations appear in primary auditory areas, primary visual areas will have low activations and vice versa.

Christophe Destrieux, Bruce Fischl, Anders Dale, and Eric Halgren. Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature. NeuroImage, 53(1):1–15, 2010. doi:10.1016/j.neuroimage.2010.06.010.

Total running time of the script: (0 minutes 32.372 seconds)

Download Jupyter notebook: simulated_raw_data_using_subject_anatomy.ipynb

Download Python source code: simulated_raw_data_using_subject_anatomy.py

Download zipped: simulated_raw_data_using_subject_anatomy.zip

Gallery generated by Sphinx-Gallery

Generate simulated raw data

Generate simulated source data

---

## Single trial linear regression analysis with the LIMO dataset#

**URL:** https://mne.tools/stable/auto_examples/datasets/limo_data.html

**Contents:**
- Single trial linear regression analysis with the LIMO dataset#
- About the data#
- Load the data#
- Visualize events#
- Visualize condition ERPs#
- Visualize effect of stimulus phase-coherence#
- Prepare data for linear regression analysis#
- Define predictor variables and design matrix#
- Extract regression coefficients#
- Plot model results#

Go to the end to download the full example code.

Here we explore the structure of the data contained in the LIMO dataset. This example replicates and extends some of the main analysis and tools integrated in LIMO MEEG, a MATLAB toolbox originally designed to interface with EEGLAB.

In summary, the example:

Fetches epoched data files for a single subject of the LIMO dataset [1]. If the LIMO files are not found on disk, the fetcher mne.datasets.limo.load_data() will automatically download the files from a remote repository.

During import, information about the data (i.e., sampling rate, number of epochs per condition, number and name of EEG channels per subject, etc.) is extracted from the LIMO .mat files stored on disk and added to the epochs structure as metadata.

Fits linear models on the single subject’s data and visualizes inferential measures to evaluate the significance of the estimated effects.

In the original LIMO experiment (see [2]), participants performed a two-alternative forced choice task, discriminating between two face stimuli. The same two faces were used during the whole experiment, with varying levels of noise added, making the faces more or less discernible to the observer (see Fig 1 in [3] for a similar approach).

The presented faces varied across a noise-signal (or phase-coherence) continuum spanning from 0 to 85% in increasing steps of 5%. In other words, faces with high phase-coherence (e.g., 85%) were easy to identify, while faces with low phase-coherence (e.g., 5%) were hard to identify and by extension very hard to discriminate.

We’ll begin by loading the data from subject 1 of the LIMO dataset.

Note that the result of the loading process is an mne.EpochsArray containing the data ready to interface with MNE-Python.

We can visualise the distribution of the face events contained in the limo_epochs structure. Events should appear clearly grouped, as the epochs are ordered by condition.

As it can be seen above, conditions are coded as Face/A and Face/B. Information about the phase-coherence of the presented faces is stored in the epochs metadata. These information can be easily accessed by calling limo_epochs.metadata. As shown below, the epochs metadata also contains information about the presented faces for convenience.

Now let us take a closer look at the information in the epochs metadata.

The first column of the summary table above provides more or less the same information as the print(limo_epochs) command we ran before. There are 1055 faces (i.e., epochs), subdivided in 2 conditions (i.e., Face A and Face B) and, for this particular subject, there are more epochs for the condition Face B.

In addition, we can see in the second column that the values for the phase-coherence variable range from -1.619 to 1.642. This is because the phase-coherence values are provided as a z-scored variable in the LIMO dataset. Note that they have a mean of zero and a standard deviation of 1.

Let’s plot the ERPs evoked by Face A and Face B, to see how similar they are.

We can also compute the difference wave contrasting Face A and Face B. Although, looking at the evoked responses above, we shouldn’t expect great differences among these face-stimuli.

As expected, no clear pattern appears when contrasting Face A and Face B. However, we could narrow our search a little bit more. Since this is a “visual paradigm” it might be best to look at electrodes located over the occipital lobe, as differences between stimuli (if any) might easier to spot over visual areas.

We do see a difference between Face A and B, but it is pretty small.

Since phase-coherence determined whether a face stimulus could be easily identified, one could expect that faces with high phase-coherence should evoke stronger activation patterns along occipital electrodes.

As shown above, there are some considerable differences between the activation patterns evoked by stimuli with low vs. high phase-coherence at the chosen electrodes.

Before we test the significance of these differences using linear regression, we’ll interpolate missing channels that were dropped during preprocessing of the data. Furthermore, we’ll drop the EOG channels (marked by the “EXG” prefix) present in the data:

To run the regression analysis, we need to create a design matrix containing information about the variables (i.e., predictors) we want to use for prediction of brain activity patterns. For this purpose, we’ll use the information we have in limo_epochs.metadata: phase-coherence and Face A vs. Face B.

Now we can set up the linear model to be used in the analysis using MNE-Python’s func:linear_regression function.

The results are stored within the object reg, which is a dictionary of evoked objects containing multiple inferential measures for each predictor in the design matrix.

Now we can access and plot the results of the linear regression analysis by calling reg['<name of predictor>'].<measure of interest> and using the plot_joint() method just as we would do with any other evoked object. Below we can see a clear effect of phase-coherence, with higher phase-coherence (i.e., better “face visibility”) having a negative effect on the activity measured at occipital electrodes around 200 to 250 ms following stimulus onset.

We can also plot the corresponding T values.

Conversely, there appears to be no (or very small) systematic effects when comparing Face A and Face B stimuli. This is largely consistent with the difference wave approach presented above.

Guillaume A. Rousselet. LIMO EEG dataset. 2016. doi:10.7488/ds/1556.

Guillaume A. Rousselet, Carl M. Gaspar, Cyril R. Pernet, Jesse S. Husk, Patrick J. Bennett, and Allison B. Sekuler. Healthy aging delays scalp EEG sensitivity to noise in a face discrimination task. Frontiers in Psychology, 1(19):1–14, 2010. doi:10.3389/fpsyg.2010.00019.

Guillaume A. Rousselet, Cyril R. Pernet, Patrick J. Bennett, and Allison B. Sekuler. Parametric study of EEG sensitivity to phase noise during face processing. BMC Neuroscience, 9(1):98, 2008. doi:10.1186/1471-2202-9-98.

Total running time of the script: (0 minutes 18.056 seconds)

Download Jupyter notebook: limo_data.ipynb

Download Python source code: limo_data.py

Download zipped: limo_data.zip

Gallery generated by Sphinx-Gallery

Kernel OPM phantom data

Optically pumped magnetometer (OPM) data

---

## Source localization with a custom inverse solver#

**URL:** https://mne.tools/stable/auto_examples/inverse/custom_inverse_solver.html

**Contents:**
- Source localization with a custom inverse solver#

Go to the end to download the full example code.

The objective of this example is to show how to plug a custom inverse solver in MNE in order to facilate empirical comparison with the methods MNE already implements (wMNE, dSPM, sLORETA, eLORETA, LCMV, DICS, (TF-)MxNE etc.).

This script is educational and shall be used for methods evaluations and new developments. It is not meant to be an example of good practice to analyse your data.

The example makes use of 2 functions apply_solver and solver so changes can be limited to the solver function (which only takes three parameters: the whitened data, the gain matrix and the number of orientations) in order to try out another inverse algorithm.

Auxiliary function to run the solver

Apply your custom solver

View in 2D and 3D (“glass” brain like 3D plot)

Total running time of the script: (0 minutes 2.950 seconds)

Download Jupyter notebook: custom_inverse_solver.ipynb

Download Python source code: custom_inverse_solver.py

Download zipped: custom_inverse_solver.zip

Gallery generated by Sphinx-Gallery

Compute MNE-dSPM inverse solution on evoked data in volume source space

Compute source level time-frequency timecourses using a DICS beamformer

---

## Statistics Examples#

**URL:** https://mne.tools/stable/auto_examples/stats/index.html

**Contents:**
- Statistics Examples#

Some examples of how to compute statistics on M/EEG data with MNE.

Permutation F-test on sensor data with 1D cluster level

FDR correction on T-test on sensor data

Regression on continuous data (rER[P/F])

Permutation T-test on sensor data

Analysing continuous features with binning and regression in sensor space

Time-frequency on simulated data (Multitaper vs. Morlet vs. Stockwell vs. Hilbert)

Permutation F-test on sensor data with 1D cluster level

---

## Temporal whitening with AR model#

**URL:** https://mne.tools/stable/auto_examples/time_frequency/temporal_whitening.html

**Contents:**
- Temporal whitening with AR model#

Go to the end to download the full example code.

Here we fit an AR model to the data and use it to temporally whiten the signals.

Plot the different time series and PSDs

Total running time of the script: (0 minutes 1.381 seconds)

Download Jupyter notebook: temporal_whitening.ipynb

Download Python source code: temporal_whitening.py

Download zipped: temporal_whitening.zip

Gallery generated by Sphinx-Gallery

Compute induced power in the source space with dSPM

Compute and visualize ERDS maps

---

## Time-Frequency Examples#

**URL:** https://mne.tools/stable/auto_examples/time_frequency/index.html

**Contents:**
- Time-Frequency Examples#

Some examples of how to explore time-frequency content of M/EEG data with MNE.

Compute a cross-spectral density (CSD) matrix

Compute Power Spectral Density of inverse solution from single epochs

Compute power and phase lock in label of the source space

Compute source power spectral density (PSD) in a label

Compute source power spectral density (PSD) of VectorView and OPM data

Compute induced power in the source space with dSPM

Temporal whitening with AR model

Compute and visualize ERDS maps

Explore event-related dynamics for specific frequency bands

Time-frequency on simulated data (Multitaper vs. Morlet vs. Stockwell vs. Hilbert)

Cross-hemisphere comparison

Compute a cross-spectral density (CSD) matrix

---

## Time-frequency on simulated data (Multitaper vs. Morlet vs. Stockwell vs. Hilbert)#

**URL:** https://mne.tools/stable/auto_examples/time_frequency/time_frequency_simulated.html

**Contents:**
- Time-frequency on simulated data (Multitaper vs. Morlet vs. Stockwell vs. Hilbert)#
- Simulate data#
- Calculate a time-frequency representation (TFR)#
  - Multitaper transform#
  - Stockwell (S) transform#
  - Morlet Wavelets#
  - Narrow-bandpass Filter and Hilbert Transform#
- Calculating a TFR without averaging over epochs#
- Operating on arrays#

Go to the end to download the full example code.

This example demonstrates the different time-frequency estimation methods on simulated data. It shows the time-frequency resolution trade-off and the problem of estimation variance. In addition it highlights alternative functions for generating TFRs without averaging across trials, or by operating on numpy arrays.

We’ll simulate data with a known spectro-temporal structure.

Below we’ll demonstrate the output of several TFR functions in MNE:

mne.time_frequency.tfr_multitaper()

mne.time_frequency.tfr_stockwell()

mne.time_frequency.tfr_morlet()

mne.Epochs.filter() and mne.Epochs.apply_hilbert()

First we’ll use the multitaper method for calculating the TFR. This creates several orthogonal tapering windows in the TFR estimation, which reduces variance. We’ll also show some of the parameters that can be tweaked (e.g., time_bandwidth) that will result in different multitaper properties, and thus a different TFR. You can trade time resolution or frequency resolution or both in order to get a reduction in variance.

Stockwell uses a Gaussian window to balance temporal and spectral resolution. Importantly, frequency bands are phase-normalized, hence strictly comparable with regard to timing, and, the input signal can be recoverd from the transform in a lossless way if we disregard numerical errors. In this case, we control the spectral / temporal resolution by specifying different widths of the gaussian window using the width parameter.

Next, we’ll show the TFR using morlet wavelets, which are a sinusoidal wave with a gaussian envelope. We can control the balance between spectral and temporal resolution with the n_cycles parameter, which defines the number of cycles to include in the window.

Finally, we’ll show a time-frequency representation using a narrow bandpass filter and the Hilbert transform. Choosing the right filter parameters is important so that you isolate only one oscillation of interest, generally the width of this filter is recommended to be about 2 Hz.

It is also possible to calculate a TFR without averaging across trials. We can do this by using average=False. In this case, an instance of mne.time_frequency.EpochsTFR is returned.

MNE-Python also has functions that operate on NumPy arrays instead of MNE-Python objects. These are tfr_array_morlet() and tfr_array_multitaper(). They expect inputs of the shape (n_epochs, n_channels, n_times) and return an array of shape (n_epochs, n_channels, n_freqs, n_times) (or optionally, can collapse the epochs dimension if you want average power or inter-trial coherence; see output param).

Total running time of the script: (0 minutes 14.304 seconds)

Download Jupyter notebook: time_frequency_simulated.ipynb

Download Python source code: time_frequency_simulated.py

Download zipped: time_frequency_simulated.zip

Gallery generated by Sphinx-Gallery

Explore event-related dynamics for specific frequency bands

---

## Use source space morphing#

**URL:** https://mne.tools/stable/auto_examples/forward/source_space_morphing.html

**Contents:**
- Use source space morphing#

Go to the end to download the full example code.

This example shows how to use source space morphing (as opposed to SourceEstimate morphing) to create data that can be compared between subjects.

Source space morphing will likely lead to source spaces that are less evenly sampled than source spaces created for individual subjects. Use with caution and check effects on localization before use.

Total running time of the script: (0 minutes 22.704 seconds)

Download Jupyter notebook: source_space_morphing.ipynb

Download Python source code: source_space_morphing.py

Download zipped: source_space_morphing.zip

Gallery generated by Sphinx-Gallery

Generate a left cerebellum volume source space

Inverse problem and source analysis

---

## Visualization#

**URL:** https://mne.tools/stable/api/visualization.html

**Contents:**
- Visualization#
- Eyetracking#
- UI Events#

Visualization routines.

Brain(subject[, hemi, surf, title, cortex, ...])

Class for visualizing a brain.

ClickableImage(imdata, **kwargs)

Display an image so you can click on it and store x/y positions.

EvokedField(evoked, surf_maps, *[, time, ...])

Plot MEG/EEG fields on head surface and helmet in 3D.

Class that refers to a 3D figure.

add_background_image(fig, im[, set_ratios])

Add a background image to a plot.

centers_to_edges(*arrays)

Convert center points to edges.

compare_fiff(fname_1, fname_2[, fname_out, ...])

Compare the contents of two fiff files using diff and show_fiff.

circular_layout(node_names, node_order[, ...])

Create layout arranging nodes on a circle.

iter_topography(info[, layout, on_pick, ...])

Create iterator over channel positions.

mne_analyze_colormap([limits, format])

Return a colormap similar to that used by mne_analyze.

plot_bem(subject[, subjects_dir, ...])

Plot BEM contours on anatomical MRI slices.

plot_brain_colorbar(ax, clim[, colormap, ...])

Plot a colorbar that corresponds to a brain activation map.

plot_bridged_electrodes(info, bridged_idx, ...)

Topoplot electrode distance matrix with bridged electrodes connected.

plot_chpi_snr(snr_dict[, axes])

Plot time-varying SNR estimates of the HPI coils.

plot_cov(cov, info[, exclude, colorbar, ...])

Plot Covariance data.

plot_channel_labels_circle(labels[, colors, ...])

Plot labels for each channel in a circle plot.

plot_ch_adjacency(info, adjacency, ch_names)

Plot channel adjacency.

plot_csd(csd[, info, mode, colorbar, cmap, ...])

plot_dipole_amplitudes(dipoles[, colors, show])

Plot the amplitude traces of a set of dipoles.

plot_dipole_locations(dipoles[, trans, ...])

Plot dipole locations.

plot_drop_log(drop_log[, threshold, ...])

Show the channel stats based on a drop_log from Epochs.

plot_epochs(epochs[, picks, scalings, ...])

plot_epochs_psd_topomap(epochs[, bands, ...])

plot_events(events[, sfreq, first_samp, ...])

Plot events to get a visual display of the paradigm.

plot_evoked(evoked[, picks, exclude, unit, ...])

Plot evoked data using butterfly plots.

plot_evoked_image(evoked[, picks, exclude, ...])

Plot evoked data as images.

plot_evoked_topo(evoked[, layout, ...])

Plot 2D topography of evoked responses.

plot_evoked_topomap(evoked[, times, ...])

Plot topographic maps of specific time points of evoked data.

plot_evoked_joint(evoked[, times, title, ...])

Plot evoked data as butterfly plot and add topomaps for time points.

plot_evoked_field(evoked, surf_maps[, time, ...])

Plot MEG/EEG fields on head surface and helmet in 3D.

plot_evoked_white(evoked, noise_cov[, show, ...])

Plot whitened evoked response.

plot_filter(h, sfreq[, freq, gain, title, ...])

Plot properties of a filter.

plot_head_positions(pos[, mode, cmap, ...])

plot_ideal_filter(freq, gain[, axes, title, ...])

Plot an ideal filter response.

plot_compare_evokeds(evokeds[, picks, ...])

Plot evoked time courses for one or more conditions and/or channels.

plot_ica_sources(ica, inst[, picks, start, ...])

Plot estimated latent sources given the unmixing matrix.

plot_ica_components(ica[, picks, ch_type, ...])

Project mixing matrix on interpolated sensor topography.

plot_ica_properties(ica, inst[, picks, ...])

Display component properties.

plot_ica_scores(ica, scores[, exclude, ...])

Plot scores related to detected components.

plot_ica_overlay(ica, inst[, exclude, ...])

Overlay of raw and cleaned signals given the unmixing matrix.

plot_epochs_image(epochs[, picks, sigma, ...])

Plot Event Related Potential / Fields image.

plot_layout(layout[, picks, show_axes, show])

Plot the sensor positions.

plot_montage(montage, *[, scale, ...])

plot_projs_topomap(projs, info, *[, ...])

Plot topographic maps of SSP projections.

plot_projs_joint(projs, evoked[, ...])

Plot projectors and evoked jointly.

plot_raw(raw[, events, duration, start, ...])

plot_raw_psd(raw[, fmin, fmax, tmin, tmax, ...])

plot_regression_weights(model, *[, ch_type, ...])

Plot the regression weights of a fitted EOGRegression model.

plot_sensors(info[, kind, ch_type, title, ...])

Plot sensors positions.

plot_snr_estimate(evoked, inv[, show, axes, ...])

Plot a data SNR estimate.

plot_source_estimates(stc[, subject, ...])

link_brains(brains[, time, camera, ...])

Plot multiple SourceEstimate objects with PyVista.

plot_volume_source_estimates(stc, src[, ...])

Plot Nutmeg style volumetric source estimates using nilearn.

plot_vector_source_estimates(stc[, subject, ...])

Plot VectorSourceEstimate with PyVista.

plot_sparse_source_estimates(src, stcs[, ...])

Plot source estimates obtained with sparse solver.

plot_tfr_topomap(tfr[, tmin, tmax, fmin, ...])

Plot topographic maps of specific time-frequency intervals of TFR data.

plot_topo_image_epochs(epochs[, layout, ...])

Plot Event Related Potential / Fields image on topographies.

plot_topomap(data, pos, *[, ch_type, ...])

Plot a topographic map as image.

plot_alignment([info, trans, subject, ...])

Plot head, sensor, and source space alignment in 3D.

snapshot_brain_montage(fig, montage[, ...])

Take a snapshot of a PyVista Scene and project channels onto 2d coords.

plot_arrowmap(data, info_from[, info_to, ...])

set_3d_backend(backend_name[, verbose])

Set the 3D backend for MNE.

Return the 3D backend currently used.

use_3d_backend(backend_name)

Create a 3d visualization context using the designated backend.

set_3d_options([antialias, depth_peeling, ...])

Set 3D rendering options.

set_3d_view(figure[, azimuth, elevation, ...])

Configure the view of the given scene.

set_3d_title(figure, title[, size, color, ...])

Configure the title of the given scene.

create_3d_figure(size[, bgcolor, ...])

Return an empty figure based on the current 3d backend.

close_3d_figure(figure)

Close the given scene.

close_all_3d_figures()

Close all the scenes of the current 3d backend.

Return the proper Brain class based on the current 3d backend.

set_browser_backend(backend_name[, verbose])

Set the 2D browser backend for MNE.

get_browser_backend()

Return the 2D backend currently used.

use_browser_backend(backend_name)

Create a 2D browser visualization context using the designated backend.

Eye-tracking visualization routines.

plot_gaze(epochs, *[, calibration, width, ...])

Plot a heatmap of eyetracking gaze data.

Event API for inter-figure communication.

The event API allows figures to communicate with each other, such that a change in one figure can trigger a change in another figure. For example, moving the time cursor in one plot can update the current time in another plot. Another scenario is two drawing routines drawing into the same window, using events to stay in-sync.

Authors: Marijn van Vliet <w.m.vanvliet@gmail.com>

subscribe(fig, event_name, callback, *[, ...])

Subscribe to an event on a figure's event channel.

unsubscribe(fig, event_names[, callback, ...])

Unsubscribe from an event on a figure's event channel.

publish(fig, event, *[, verbose])

Publish an event to all subscribers of the figure's channel.

link(*figs[, include_events, ...])

Link the event channels of two figures together.

unlink(fig, *[, verbose])

Remove all links involving the event channel of the given figure.

disable_ui_events(fig)

Temporarily disable generation of UI events.

Abstract base class for all events.

ColormapRange(kind[, ch_type, fmin, fmid, ...])

Indicates that the user has updated the bounds of the colormap.

Contours(kind, contours)

Indicates that the user has changed the contour lines.

Indicates that the user has requested to close a figure.

Indicates that the user has selected a different playback speed for videos.

Indicates that the user has selected a time.

VertexSelect(hemi, vertex_id)

Indicates that the user has selected a vertex.

mne.datasets.eyelink.data_path

---

## Visualize source leakage among labels using a circular graph#

**URL:** https://mne.tools/stable/auto_examples/inverse/psf_ctf_label_leakage.html

**Contents:**
- Visualize source leakage among labels using a circular graph#
- Load forward solution and inverse operator#
- Read and organise labels for cortical parcellation#
- Compute point-spread function summaries (PCA) for all labels#
- Evaluate leakage based on label-to-label PSF correlations#
- Save the figure (optional)#
- Plot PSFs for individual labels#

Go to the end to download the full example code.

This example computes all-to-all pairwise leakage among 68 regions in source space based on MNE inverse solutions and a FreeSurfer cortical parcellation. Label-to-label leakage is estimated as the correlation among the labels’ point-spread functions (PSFs). It is visualized using a circular graph which is ordered based on the locations of the regions in the axial plane.

We need a matching forward solution and inverse operator to compute resolution matrices for different methods.

Get labels for FreeSurfer ‘aparc’ cortical parcellation with 34 labels/hemi

We summarise the PSFs per label by their first five principal components, and use the first component to evaluate label-to-label leakage below.

We can show the explained variances of principal components per label. Note how they differ across labels, most likely due to their varying spatial extent.

The output shows the summed variance explained by the first five principal components as well as the explained variances of the individual components.

Note that correlations ignore the overall amplitude of PSFs, i.e. they do not show which region will potentially be the bigger “leaker”.

Most leakage occurs for neighbouring regions, but also for deeper regions across hemispheres.

Matplotlib controls figure facecolor separately for interactive display versus for saved figures. Thus when saving you must specify facecolor, else your labels, title, etc will not be visible:

Let us confirm for left and right lateral occipital lobes that there is indeed no leakage between them, as indicated by the correlation graph. We can plot the summary PSFs for both labels to examine the spatial extent of their leakage.

Point-spread function for the lateral occipital label in the left hemisphere

and in the right hemisphere.

Both summary PSFs are confined to their respective hemispheres, indicating that there is indeed low leakage between these two regions.

Total running time of the script: (0 minutes 20.743 seconds)

Download Jupyter notebook: psf_ctf_label_leakage.ipynb

Download Python source code: psf_ctf_label_leakage.py

Download zipped: psf_ctf_label_leakage.zip

Gallery generated by Sphinx-Gallery

Compute iterative reweighted TF-MxNE with multiscale time-frequency dictionary

Plot point-spread functions (PSFs) and cross-talk functions (CTFs)

---

## XDAWN Decoding From EEG data#

**URL:** https://mne.tools/stable/auto_examples/decoding/decoding_xdawn_eeg.html

**Contents:**
- XDAWN Decoding From EEG data#
- References#

Go to the end to download the full example code.

ERP decoding with Xdawn [1][2]. For each event type, a set of spatial Xdawn filters are trained and applied on the signal. Channels are concatenated and rescaled to create features vectors that will be fed into a logistic regression.

Set parameters and read data

Patterns of a fitted XdawnTransformer instance (here from the last cross-validation fold) can be visualized using SpatialFilter container.

Bertrand Rivet, Antoine Souloumiac, Virginie Attina, and Guillaume Gibert. xDAWN algorithm to enhance evoked potentials: application to brain–computer interface. IEEE Transactions on Biomedical Engineering, 56(8):2035–2043, 2009. doi:10.1109/TBME.2009.2012869.

Bertrand Rivet, Hubert Cecotti, Antoine Souloumiac, Emmanuel Maby, and Jérémie Mattout. Theoretical analysis of xDAWN algorithm: application to an efficient sensor selection in a P300 BCI. In Proceedings of EUSIPCO-2011, 1382–1386. Barcelona, 2011. IEEE. URL: https://ieeexplore.ieee.org/document/7073970.

Total running time of the script: (0 minutes 9.311 seconds)

Download Jupyter notebook: decoding_xdawn_eeg.ipynb

Download Python source code: decoding_xdawn_eeg.py

Download zipped: decoding_xdawn_eeg.zip

Gallery generated by Sphinx-Gallery

Analysis of evoked response using ICA and PCA reduction techniques

Compute effect-matched-spatial filtering (EMS)

---
